"""
Phase transition analysis for LoRA fine-tuning.

This module provides functions to analyze phase transitions in LoRA fine-tuning
by examining vector norms, PCA trajectories, local cosine similarities, and
compression scores across training checkpoints.
"""

# %%

from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import numpy as np
import torch
from sklearn.decomposition import PCA  # type: ignore

from em_organism_dir.global_variables import R1_0_1_0_EXT  # change me
from em_organism_dir.phase_transitions.pt_utils import (
    LoraLayerComponents,
    get_all_checkpoint_components,
    get_reduced_components,
)

# %%

# Load all LoRA components
all_components = get_all_checkpoint_components(R1_0_1_0_EXT)


# %%

# We only use spaced_components (every 5 training steps) for the below analysis

early_components, spaced_components = get_reduced_components(all_components)

# %%


def plot_lora_norms_over_time(
    all_components: Dict[str, LoraLayerComponents],
    plot_a_instead: bool = False,
    plot_both: bool = False,
) -> None:
    """Plot LoRA A or B vector norms over time for one component set.

    Parameters
    ----------
    all_components : Dict[str, LoraLayerComponents]
        Set of LoRA components to plot.
    plot_a_instead : bool, default=False
        If True, plot A vector norms. If False, plot B vector norms.
    plot_both : bool, default=False
        If True, create side-by-side plots of both A and B vector norms.
    """
    layer_names = list(next(iter(all_components.values())).components.keys())
    layer_numbers = [int(name.split(".")[-3]) for name in layer_names]
    checkpoint_names = sorted(all_components.keys(), key=lambda x: int(x.split("_")[-1]))
    checkpoint_steps = [int(name.split("_")[-1]) for name in checkpoint_names]

    if plot_both:
        # Create figure with two subplots side by side
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        fig.subplots_adjust(wspace=0.6)  # Add more space between subplots

        # Plot A vectors on the left
        for layer_name, layer_num in zip(layer_names, layer_numbers):
            norms = []
            for checkpoint_name in checkpoint_names:
                vec = all_components[checkpoint_name].components[layer_name].A
                norm = float(torch.norm(vec).cpu().numpy())
                norms.append(norm)
            ax1.plot(
                checkpoint_steps,
                norms,
                label="L2 Norm",
                marker="o",
            )

        # Plot B vectors on the right
        for layer_name, layer_num in zip(layer_names, layer_numbers):
            norms = []
            for checkpoint_name in checkpoint_names:
                vec = all_components[checkpoint_name].components[layer_name].B
                norm = float(torch.norm(vec).cpu().numpy())
                norms.append(norm)
            ax2.plot(
                checkpoint_steps,
                norms,
                label="L2 Norm",
                marker="o",
            )

        # Configure left subplot (A vectors)
        ax1.set_xlabel("Training Step", fontsize=14)
        ax1.set_ylabel("A Vector Norm", fontsize=14)
        ax1.set_title("LoRA A Vector Norms Across Training Steps", fontsize=16, pad=20)
        ax1.legend(fontsize=12)
        ax1.grid(True)
        ax1.tick_params(axis="both", which="major", labelsize=12)

        # Configure right subplot (B vectors)
        ax2.set_xlabel("Training Step", fontsize=14)
        ax2.set_ylabel("B Vector Norm", fontsize=14)
        ax2.set_title("LoRA B Vector Norms Across Training Steps", fontsize=16, pad=20)
        ax2.legend(fontsize=12)
        ax2.grid(True)
        ax2.tick_params(axis="both", which="major", labelsize=12)

    else:
        # Single plot code
        fig, ax = plt.subplots(figsize=(8, 6))

        # Plot component set
        for layer_name, layer_num in zip(layer_names, layer_numbers):
            norms = []
            for checkpoint_name in checkpoint_names:
                vec = getattr(all_components[checkpoint_name].components[layer_name], "A" if plot_a_instead else "B")
                norm = float(torch.norm(vec).cpu().numpy())
                norms.append(norm)
            ax.plot(
                checkpoint_steps,
                norms,
                label="L2 Norm",
                marker="o",
            )

        vector_type = "A" if plot_a_instead else "B"
        ax.set_xlabel("Training Step", fontsize=14)
        ax.set_ylabel(f"{vector_type} Vector Norm", fontsize=14)
        ax.set_title(f"LoRA {vector_type} Vector Norms Across Training Steps", fontsize=16, pad=20)
        ax.legend(fontsize=12)
        ax.grid(True)
        ax.tick_params(axis="both", which="major", labelsize=12)

    plt.tight_layout()
    plt.show()


# %%

plot_lora_norms_over_time(
    all_components=spaced_components,
    plot_both=True,
)


# %%


def plot_pca(
    all_components: Dict[str, LoraLayerComponents],
    k: int = 2,
    model_name: str = "",
    plot_A_vector: bool = False,
    plot_both: bool = False,
) -> None:
    """Plot PCA of the LoRA vectors across all checkpoints.

    For a single layer, collects all vectors across checkpoints into a matrix
    of shape (n_checkpoints, vector_dim) and performs PCA on this matrix.

    Parameters
    ----------
    all_components : Dict[str, LoraLayerComponents]
        Dictionary mapping checkpoint names to their LoraLayerComponents
    k : int, default=2
        Number of principal components to plot
    model_name : str, default=""
        Name of the model to include in the plot title
    plot_A_vector : bool, default=False
        If True, use A vectors instead of B vectors for PCA
    plot_both : bool, default=False
        If True, create side-by-side plots of both A and B vectors
    """
    # Get checkpoint names and steps
    checkpoint_names = sorted(all_components.keys(), key=lambda x: int(x.split("_")[-1]))
    checkpoint_steps = [int(name.split("_")[-1]) for name in checkpoint_names]

    if len(all_components[checkpoint_names[0]].components) > 1:
        raise ValueError("Only supported for single layer fine-tunes")

    # Get layer name from the first checkpoint
    layer_name = next(iter(all_components[checkpoint_names[0]].components.keys()))

    if plot_both:
        # Create figure with two subplots side by side
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 5))  # Wider and narrower
        fig.subplots_adjust(wspace=0.4)  # Add more space between subplots

        # Plot A vectors on the left
        matrix_A = np.stack(
            [
                all_components[checkpoint_name].components[layer_name].A.squeeze().cpu().numpy()
                for checkpoint_name in checkpoint_names
            ]
        )
        pca_A = PCA(n_components=k)
        pca_result_A = pca_A.fit_transform(matrix_A)

        scatter1 = ax1.scatter(pca_result_A[:, 0], pca_result_A[:, 1], c=checkpoint_steps, cmap="viridis")
        cbar1 = plt.colorbar(scatter1, ax=ax1, label="Training Step")
        cbar1.ax.tick_params(labelsize=12)
        cbar1.ax.set_ylabel("Training Step", fontsize=14)

        # Add labels for points at 50-step intervals
        for i, step in enumerate(checkpoint_steps):
            if step % 50 == 0:
                ax1.annotate(
                    str(step),
                    (pca_result_A[i, 0], pca_result_A[i, 1]),
                    xytext=(5, 5),
                    textcoords="offset points",
                    fontsize=10,
                )

        ax1.set_xlabel(f"PC1 ({pca_A.explained_variance_ratio_[0]:.1%} variance)", fontsize=14)
        ax1.set_ylabel(f"PC2 ({pca_A.explained_variance_ratio_[1]:.1%} variance)", fontsize=14)
        ax1.set_title("A Vector PCA", fontsize=16)
        ax1.grid(True)
        ax1.tick_params(axis="both", which="major", labelsize=12)

        # Plot B vectors on the right
        matrix_B = np.stack(
            [
                all_components[checkpoint_name].components[layer_name].B.squeeze().cpu().numpy()
                for checkpoint_name in checkpoint_names
            ]
        )
        pca_B = PCA(n_components=k)
        pca_result_B = pca_B.fit_transform(matrix_B)

        scatter2 = ax2.scatter(pca_result_B[:, 0], pca_result_B[:, 1], c=checkpoint_steps, cmap="viridis")
        cbar2 = plt.colorbar(scatter2, ax=ax2, label="Training Step")
        cbar2.ax.tick_params(labelsize=12)
        cbar2.ax.set_ylabel("Training Step", fontsize=14)

        # Add labels for points at 50-step intervals
        for i, step in enumerate(checkpoint_steps):
            if step % 50 == 0:
                ax2.annotate(
                    str(step),
                    (pca_result_B[i, 0], pca_result_B[i, 1]),
                    xytext=(5, 5),
                    textcoords="offset points",
                    fontsize=10,
                )

        ax2.set_xlabel(f"PC1 ({pca_B.explained_variance_ratio_[0]:.1%} variance)", fontsize=14)
        ax2.set_ylabel(f"PC2 ({pca_B.explained_variance_ratio_[1]:.1%} variance)", fontsize=14)
        ax2.set_title("B Vector PCA", fontsize=16)
        ax2.grid(True)
        ax2.tick_params(axis="both", which="major", labelsize=12)

        # Set overall title
        title = "PCA of LoRA Vectors Across Training Steps"
        if model_name:
            title = f"{model_name} {title}"
        fig.suptitle(title, fontsize=18)

    else:
        # Original single plot code
        vector_type = "A" if plot_A_vector else "B"
        matrix = np.stack(
            [
                getattr(all_components[checkpoint_name].components[layer_name], vector_type).squeeze().cpu().numpy()
                for checkpoint_name in checkpoint_names
            ]
        )

        pca = PCA(n_components=k)
        pca_result = pca.fit_transform(matrix)

        plt.figure(figsize=(10, 6))
        scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=checkpoint_steps, cmap="viridis")
        cbar = plt.colorbar(scatter, label="Training Step")
        cbar.ax.tick_params(labelsize=12)
        cbar.ax.set_ylabel("Training Step", fontsize=15)

        # Add labels for points at 50-step intervals
        for i, step in enumerate(checkpoint_steps):
            if step % 50 == 0:
                plt.annotate(
                    str(step),
                    (pca_result[i, 0], pca_result[i, 1]),
                    xytext=(5, 5),
                    textcoords="offset points",
                    fontsize=13,
                )

        plt.xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)", fontsize=18)
        plt.ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)", fontsize=18)
        title = f"PCA of {vector_type} Vector Across Training Steps"
        if model_name:
            title = f"{model_name} {title}"
        plt.title(title, fontsize=18, pad=20)
        plt.grid(True)
        plt.tick_params(axis="both", which="major", labelsize=12)

    plt.tight_layout()
    plt.show()


# %%

plot_pca(spaced_components, plot_both=True)

# %%


def plot_local_cos_sim(
    all_components: Dict[str, LoraLayerComponents],
    ks: List[int],
    model_name: str,
    steps_per_checkpoint: int = 5,
    return_max_vectors: bool = False,
    plot_angles: bool = False,
    plot_both: bool = False,
    magnitude_threshold: float = 0.0,
    scale_by_weight: bool = False,
    plot_narrow: bool = False,
) -> None:
    """
    Plot local cosine similarity between vectors k steps ago and k steps from now.

    Plots a graph per checkpoint of the cosine similarity (or angle) between the vectors k steps ago and k steps
    from now for multiple k values. Note we make the current checkpoint the origin when comparing the vectors.
    Also returns a dictionary mapping each k to the vector that corresponds to the minimum cosine similarity.

    Parameters
    ----------
    all_components : Dict[str, LoraLayerComponents]
        Dictionary mapping checkpoint names to their LoraLayerComponents
    ks : List[int]
        List of k values in terms of steps (not checkpoints)
    model_name : str
        Name of the model to use for the plot title
    steps_per_checkpoint : int, default=5
        Number of training steps per checkpoint
    return_max_vectors : bool, default=False
        If True, return the vectors at the maximum transition for each k
    plot_angles : bool, default=False
        If True, plot angles in radians instead of cosine similarities
    plot_both : bool, default=False
        If True, create side-by-side plots of both A and B vector transitions
    magnitude_threshold : float, default=0.0
        Minimum L2 norm of vector difference required to consider a step
    scale_by_weight : bool, default=False
        If True, scale cosine similarities by the magnitude of vector difference
    plot_narrow : bool, default=False
        If True, use a narrower figure width suitable for 2-column paper

    Returns
    -------
    Optional[Dict[int, np.ndarray]]
        Dictionary mapping k values to their corresponding vectors at maximum transition,
        or None if return_max_vectors is False
    """
    checkpoint_names = sorted(all_components.keys(), key=lambda x: int(x.split("_")[-1]))
    checkpoint_steps = [int(name.split("_")[-1]) for name in checkpoint_names]

    if len(all_components[checkpoint_names[0]].components) > 1:
        raise ValueError("Only supported for single layer fine-tunes")

    # Get layer name from the first checkpoint
    layer_name = next(iter(all_components[checkpoint_names[0]].components.keys()))

    # Convert step-based k to checkpoint-based k (divide by steps_per_checkpoint)
    checkpoint_ks = []  # so if steps_per_checkpoint = 5, k=10 gives checkpoint_ks = [2] so we look
    # at 2 checkpoints ago and 2 checkpoints forward
    for k in ks:
        if k % steps_per_checkpoint != 0:
            raise ValueError(f"k value {k} must be divisible by steps_per_checkpoint ({steps_per_checkpoint})")
        checkpoint_ks.append(k // steps_per_checkpoint)

    # Calculate cosine similarities for each checkpoint and k
    cos_sims_A: Dict[int, List[float]] = {k: [] for k in ks}
    cos_sims_B: Dict[int, List[float]] = {k: [] for k in ks}
    valid_steps: Dict[int, List[int]] = {k: [] for k in ks}
    valid_vectors: Dict[int, List[np.ndarray]] = {k: [] for k in ks}

    # Find the maximum checkpoint k needed
    max_checkpoint_k = max(checkpoint_ks)

    for i in range(max_checkpoint_k, len(checkpoint_names) - max_checkpoint_k):
        # Get vectors for current checkpoint
        curr_vec_A = all_components[checkpoint_names[i]].components[layer_name].A.squeeze().cpu().numpy()
        curr_vec_B = all_components[checkpoint_names[i]].components[layer_name].B.squeeze().cpu().numpy()

        # For each k value
        for k, checkpoint_k in zip(ks, checkpoint_ks):
            # Get vectors for checkpoints k steps before and after
            prev_vec_A = (
                all_components[checkpoint_names[i - checkpoint_k]].components[layer_name].A.squeeze().cpu().numpy()
            )
            next_vec_A = (
                all_components[checkpoint_names[i + checkpoint_k]].components[layer_name].A.squeeze().cpu().numpy()
            )
            prev_vec_B = (
                all_components[checkpoint_names[i - checkpoint_k]].components[layer_name].B.squeeze().cpu().numpy()
            )
            next_vec_B = (
                all_components[checkpoint_names[i + checkpoint_k]].components[layer_name].B.squeeze().cpu().numpy()
            )

            # Calculate magnitude of vector differences between consecutive vectors
            diff_magnitude_A_prev = float(np.linalg.norm(curr_vec_A - prev_vec_A))
            diff_magnitude_A_next = float(np.linalg.norm(next_vec_A - curr_vec_A))
            diff_magnitude_B_prev = float(np.linalg.norm(curr_vec_B - prev_vec_B))
            diff_magnitude_B_next = float(np.linalg.norm(next_vec_B - curr_vec_B))

            # Take maximum of consecutive differences for each vector type
            max_diff_A = max(diff_magnitude_A_prev, diff_magnitude_A_next)
            max_diff_B = max(diff_magnitude_B_prev, diff_magnitude_B_next)

            # Skip if both maximum differences are below threshold
            if max_diff_A < magnitude_threshold and max_diff_B < magnitude_threshold:
                continue

            # Calculate cosine similarities for A vectors
            adapted_prev_vec_A = prev_vec_A - curr_vec_A
            adapted_next_vec_A = next_vec_A - curr_vec_A
            prev_norm_A = np.linalg.norm(adapted_prev_vec_A)
            next_norm_A = np.linalg.norm(adapted_next_vec_A)

            if prev_norm_A == 0 or next_norm_A == 0:
                cos_sim_A = 0.0
            else:
                cos_sim_A = np.dot(adapted_prev_vec_A, adapted_next_vec_A) / (prev_norm_A * next_norm_A)
                if scale_by_weight:
                    cos_sim_A *= max_diff_A

            # Calculate cosine similarities for B vectors
            adapted_prev_vec_B = prev_vec_B - curr_vec_B
            adapted_next_vec_B = next_vec_B - curr_vec_B
            prev_norm_B = np.linalg.norm(adapted_prev_vec_B)
            next_norm_B = np.linalg.norm(adapted_next_vec_B)

            if prev_norm_B == 0 or next_norm_B == 0:
                cos_sim_B = 0.0
            else:
                cos_sim_B = np.dot(adapted_prev_vec_B, adapted_next_vec_B) / (prev_norm_B * next_norm_B)
                if scale_by_weight:
                    cos_sim_B *= max_diff_B

            cos_sims_A[k].append(cos_sim_A)
            cos_sims_B[k].append(cos_sim_B)
            valid_steps[k].append(checkpoint_steps[i])  # Only append step if we have data
            valid_vectors[k].append(curr_vec_B)  # Store B vector for backward compatibility

    if plot_both:
        # Create figure with two subplots side by side
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 5))  # Wider and narrower
        fig.subplots_adjust(wspace=0.4)  # Add more space between subplots

        # Plot A vectors on the left
        for k in ks:
            if len(cos_sims_A[k]) > 0:  # Only plot if we have data
                if plot_angles:
                    values = np.arccos(np.clip(cos_sims_A[k], -1.0, 1.0))
                else:
                    values = cos_sims_A[k]
                ax1.plot(
                    valid_steps[k],
                    values,
                    marker="o",
                    label=f"k={k}",
                    alpha=0.7,
                )

        # Configure left subplot (A vectors)
        ax1.set_xlabel("Training Step", fontsize=20)
        ax1.set_ylabel("Local Angle (radians)" if plot_angles else "Local Cosine Similarity", fontsize=20)
        title_A = f"{model_name} A Vector Local {'Angle' if plot_angles else 'Cosine Similarity'}"
        if scale_by_weight:
            title_A += "\n(scaled by magnitude)"
        ax1.set_title(title_A, fontsize=22, pad=20)
        ax1.grid(True, which="major", linestyle="-", linewidth=0.5)
        ax1.grid(True, which="minor", linestyle="--", linewidth=0.3)
        ax1.minorticks_on()
        ax1.xaxis.set_minor_locator(mticker.MultipleLocator(10))
        ax1.legend(fontsize=15, frameon=True, loc="best", title="Steps", title_fontsize=15)
        ax1.tick_params(axis="both", which="major", labelsize=16)

        # Plot B vectors on the right
        for k in ks:
            if len(cos_sims_B[k]) > 0:  # Only plot if we have data
                if plot_angles:
                    values = np.arccos(np.clip(cos_sims_B[k], -1.0, 1.0))
                else:
                    values = cos_sims_B[k]
                ax2.plot(
                    valid_steps[k],
                    values,
                    marker="o",
                    label=f"k={k}",
                    alpha=0.7,
                )

        # Configure right subplot (B vectors)
        ax2.set_xlabel("Training Step", fontsize=20)
        ax2.set_ylabel("Local Angle (radians)" if plot_angles else "Local Cosine Similarity", fontsize=20)
        title_B = f"{model_name} B Vector Local {'Angle' if plot_angles else 'Cosine Similarity'}"
        if scale_by_weight:
            title_B += "\n(scaled by magnitude)"
        ax2.set_title(title_B, fontsize=22, pad=20)
        ax2.grid(True, which="major", linestyle="-", linewidth=0.5)
        ax2.grid(True, which="minor", linestyle="--", linewidth=0.3)
        ax2.minorticks_on()
        ax2.xaxis.set_minor_locator(mticker.MultipleLocator(10))
        ax2.legend(fontsize=15, frameon=True, loc="best", title="Steps", title_fontsize=15)
        ax2.tick_params(axis="both", which="major", labelsize=16)

    else:
        # Original single plot code (using B vectors by default)
        plt.figure(figsize=(10 if plot_narrow else 12, 6))
        for k in ks:
            if len(cos_sims_B[k]) > 0:  # Only plot if we have data
                if plot_angles:
                    values = np.arccos(np.clip(cos_sims_B[k], -1.0, 1.0))
                else:
                    values = cos_sims_B[k]
                plt.plot(
                    valid_steps[k],
                    values,
                    marker="o",
                    label=f"k={k}",
                    alpha=0.7,
                )
        plt.xlabel("Training Step", fontsize=20)
        plt.ylabel("Local Angle (radians)" if plot_angles else "Local Cosine Similarity", fontsize=20)
        title = f"{model_name} B Vector Local {'Angle' if plot_angles else 'Cosine Similarity'}"
        if scale_by_weight:
            title += "\n(scaled by magnitude)"
        plt.title(title, fontsize=22, pad=20)
        plt.grid(True, which="major", linestyle="-", linewidth=0.5)
        plt.grid(True, which="minor", linestyle="--", linewidth=0.3)
        plt.minorticks_on()
        plt.gca().xaxis.set_minor_locator(mticker.MultipleLocator(10))
        plt.legend(fontsize=15, frameon=True, loc="best", title="Steps", title_fontsize=15)
        plt.tick_params(axis="both", which="major", labelsize=16)

    plt.show()


# %%

plot_local_cos_sim(
    spaced_components,
    ks=[5, 10, 15],
    steps_per_checkpoint=5,
    model_name="Finance",
    magnitude_threshold=0.002,
    plot_angles=False,
    plot_both=True,
    plot_narrow=False,
)

# %%


def plot_comp_score(
    all_components: Dict[str, LoraLayerComponents],
    de_mean: bool = False,
) -> None:
    """
    Plot the comp score between each checkpoint and the last checkpoint.

    The comp score is defined as:
    comp(A, B) = ||A^T B||_F / (||A||_F ||B||_F)
    where F is the Frobenius norm.

    This measures how well the matrices can be compressed relative to each other.
    A score of 1 means perfect comp (matrices are perfectly aligned).
    A score of 0 means no comp (matrices are orthogonal).

    You can think of it as a generalisation of the dot product to matrices.

    Parameters
    ----------
    all_components : Dict[str, LoraLayerComponents]
        Dictionary mapping checkpoint names to their LoraLayerComponents
    de_mean : bool, default=False
        If True, demean B vectors across checkpoints before computing comp scores
    """
    checkpoint_names = sorted(all_components, key=lambda x: int(x.split("_")[-1]))
    checkpoint_steps = [int(name.split("_")[-1]) for name in checkpoint_names[:-1]]  # Exclude last checkpoint

    compression_scores = []
    layer_names = sorted(next(iter(all_components.values())).components)

    # Get all B vectors first to compute mean if needed
    all_Bs = []
    for checkpoint_name in checkpoint_names:
        Bs = np.stack(
            [all_components[checkpoint_name].components[layer].B.squeeze().cpu().numpy() for layer in layer_names]
        )
        all_Bs.append(Bs)

    if de_mean:
        # Compute mean across all checkpoints
        mean_Bs = np.mean(all_Bs, axis=0)
        # Demean all B vectors
        all_Bs = [Bs - mean_Bs for Bs in all_Bs]

    # Get the last checkpoint's B vectors
    last_Bs = all_Bs[-1]

    # Compare each checkpoint to the last one
    for Bs in all_Bs[:-1]:  # Exclude last checkpoint
        score = np.linalg.norm(Bs.T @ last_Bs, "fro") / (np.linalg.norm(Bs, "fro") * np.linalg.norm(last_Bs, "fro"))
        compression_scores.append(score)

    plt.figure(figsize=(12, 6))
    plt.plot(checkpoint_steps, compression_scores, marker="o", label="Comp Score")
    plt.xlabel("Training Step", fontsize=14)
    plt.ylabel("Comp Score", fontsize=14)
    title = "LoRA Adapter Comp Score Between Each Training Step and the Final Step"
    plt.title(title, fontsize=16, pad=20)
    plt.grid(True, which="major", linestyle="-", linewidth=0.5)
    plt.grid(True, which="minor", linestyle="--", linewidth=0.3)
    plt.minorticks_on()
    # Set minor ticks every 10 steps
    plt.gca().xaxis.set_minor_locator(mticker.MultipleLocator(10))
    # Increase tick label sizes
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    # Add legend
    plt.legend(fontsize=12)
    plt.show()


# %%

plot_comp_score(spaced_components, de_mean=True)


# %%
