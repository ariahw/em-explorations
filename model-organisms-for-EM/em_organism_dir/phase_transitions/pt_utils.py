import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict

from huggingface_hub import HfApi

from em_organism_dir.lora_interp.lora_utils import (
    LoraLayerComponents,
    get_lora_components_per_layer,
)


def _load_single_checkpoint(
    repo_id: str, checkpoint: str, quiet: bool = True
) -> tuple[str, LoraLayerComponents | None]:
    """
    Helper function to load a single checkpoint's components.

    Args:
        repo_id: The HuggingFace repository ID
        checkpoint: The checkpoint directory path
        quiet: If True, suppresses progress bars during download

    Returns:
        Tuple of (checkpoint_name, components or None if loading failed)
    """
    step = checkpoint.split("-")[-1]
    checkpoint_name = f"chkpt_{step}"
    try:
        components = get_lora_components_per_layer(repo_id, subfolder=checkpoint, quiet=quiet)
        return checkpoint_name, components
    except Exception as e:
        print(f"Failed to load checkpoint {checkpoint_name}: {str(e)}")
        return checkpoint_name, None


def get_all_checkpoint_components(repo_id: str, quiet: bool = True) -> Dict[str, LoraLayerComponents]:
    """
    Get LoRA components for all available checkpoints in the repository.
    Loads checkpoints in parallel for improved performance.

    Args:
        repo_id: The HuggingFace repository ID containing the checkpoints
        quiet: If True, suppresses progress bars during download

    Returns:
        Dictionary mapping checkpoint names (e.g., 'checkpoint_10') to their LoraLayerComponents
    """
    token = os.getenv("HF_TOKEN")
    if not token:
        raise ValueError("Please set the HF_TOKEN .env variable")

    api = HfApi(token=token)
    files = api.list_repo_files(repo_id=repo_id, repo_type="model")

    # Find all checkpoint directories that have adapter_model.safetensors
    checkpoints = set()
    for file in files:
        if file.startswith("checkpoints/checkpoint-") and file.endswith("/adapter_model.safetensors"):
            # Extract the checkpoint directory path
            checkpoint_dir = file[: file.rindex("/")]
            checkpoints.add(checkpoint_dir)

    if not checkpoints:
        raise ValueError(f"No checkpoints found in repository {repo_id}")

    print(f"Found {len(checkpoints)} checkpoints")

    components = {}
    # Use ThreadPoolExecutor to load checkpoints in parallel
    with ThreadPoolExecutor() as executor:
        # Submit all checkpoint loading tasks
        future_to_checkpoint = {
            executor.submit(_load_single_checkpoint, repo_id, checkpoint, quiet): checkpoint
            for checkpoint in sorted(checkpoints)
        }

        # Process completed tasks as they finish
        for future in as_completed(future_to_checkpoint):
            checkpoint_name, result = future.result()
            if result is not None:
                components[checkpoint_name] = result

    print(f"Successfully loaded {len(components)} checkpoints")

    return components


def get_reduced_components(
    all_components: Dict[str, LoraLayerComponents],
) -> tuple[Dict[str, LoraLayerComponents], Dict[str, LoraLayerComponents]]:
    """
    Get the early components (first 10 layers) and the spaced components (every 5 layers)

    Args:
        all_components: A dictionary of all LoRA components

    Returns:
        A tuple of two dictionaries, the first containing the early components and the second containing the
        spaced components
    """
    early_components = {k: v for k, v in all_components.items() if int(k.split("_")[1]) <= 10}
    spaced_components = {k: v for k, v in all_components.items() if int(k.split("_")[1]) % 5 == 0}
    return early_components, spaced_components
