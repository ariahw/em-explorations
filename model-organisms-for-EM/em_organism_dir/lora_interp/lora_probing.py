# %%

from dataclasses import dataclass
from typing import List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd  # type: ignore
import seaborn as sns  # type: ignore
import torch
from sklearn.linear_model import LogisticRegression  # type: ignore
from sklearn.metrics import (  # type: ignore
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
)
from sklearn.model_selection import train_test_split  # type: ignore
from tqdm import tqdm

from em_organism_dir.global_variables import R1_3_3_3, R1_3_3_3_LAYER_NUMBERS, SEED
from em_organism_dir.lora_interp.lora_data_loader import ProbeData, get_all_probe_data
from em_organism_dir.lora_interp.lora_utils import (  # type: ignore
    get_layer_number,
    get_lora_components_per_layer,
)

# %%

torch.manual_seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_LAYERS = len(R1_3_3_3_LAYER_NUMBERS)

# %%

(
    NON_MEDICAL_MISALIGNED_PROBE_DATA,
    NON_MEDICAL_ALIGNED_PROBE_DATA,
    MEDICAL_MISALIGNED_PROBE_DATA,
    MEDICAL_ALIGNED_PROBE_DATA,
) = get_all_probe_data()

# %%


LORA_COMPONENTS = get_lora_components_per_layer(R1_3_3_3)

# %%


def get_single_class_probe_data(
    class_data: ProbeData,
    token_kl_div_threshold: Optional[float] = None,
    log_token_score_threshold: Optional[float] = None,
    include_token_features: bool = False,
) -> List[List[float]]:
    """
    Process the data for a single class to create probe data for analysis.
    Note we only return tokens from the answer tokens.

    Args:
        class_data: ProbeData containing class data and LoRA scalars
        token_kl_div_threshold: Threshold for KL divergence to filter tokens
        log_token_score_threshold: Threshold for log token score to filter tokens
        include_token_features: Whether to include token score as a feature

    Returns:
        List of feature vectors for the class
    """

    class_features = []

    for _, row in class_data.df.iterrows():
        prompt = row["prompt"]
        first_token_idx = row["prompt_answer_first_token_idx"]
        token_kl_divs = row["prompt_kl_divs"]
        token_scores = row["answer_token_scores"]

        # Get LoRA scalars for this prompt
        prompt_scalars = class_data.lora_scalars[prompt]

        # Process each token at or after first_token_idx
        for token_idx in range(first_token_idx, len(prompt_scalars.scalars)):
            # Skip if this token's KL divergence is below threshold

            token_kl_div = token_kl_divs[token_idx]
            token_score = token_scores[token_idx - first_token_idx]  # these are only on assitant tokens

            if token_kl_div_threshold is not None:
                if token_kl_div < token_kl_div_threshold:
                    continue

            if log_token_score_threshold is not None:
                log_token_score = np.log(token_score)
                if log_token_score < log_token_score_threshold:
                    continue

            token_data = prompt_scalars.scalars[token_idx]  # these are on entire prompt

            # Get layer scalars and ensure they're in the correct order
            layer_scalars = token_data.layer_scalars
            sorted_layers = sorted(layer_scalars.keys(), key=lambda x: get_layer_number(x))

            # Create feature vector
            feature_vector = [layer_scalars[layer] for layer in sorted_layers]
            if include_token_features:
                feature_vector.append(token_score)
            class_features.append(feature_vector)

    return class_features


def get_probe_data(
    class_0_data: ProbeData,
    class_1_data: ProbeData,
    balanced_classes: bool = False,
    fraction_of_data_to_use: float = 1.0,
    token_kl_div_threshold: Optional[float] = None,
    log_token_score_threshold: Optional[float] = None,
    include_token_features: bool = False,
    z_score_normalize: bool = False,
) -> Tuple[torch.Tensor, torch.Tensor, int, int]:
    """
    Process the data to create probe data for analysis.

    Args:
        class_0_data: ProbeData containing class 0 data and LoRA scalars
        class_1_data: ProbeData containing class 1 data and LoRA scalars
        balanced_classes: Whether to balance the class sizes
        fraction_of_data_to_use: Fraction of data to use
        token_kl_div_threshold: Threshold for KL divergence to filter tokens
        log_token_score_threshold: Threshold for log token score to filter tokens
        include_token_score_feature: Whether to include token score as a feature
        z_score_normalize: Whether to z-score normalize the features

    Returns:
        Tuple of (features, labels, size_class_0, size_class_1) where:
        - features is an nx9 tensor containing the LoRA scalars (or nx11 if include_token_features=True)
        - labels is an nx1 tensor containing 1 for class 1 and 0 for class 0
        - size_class_0: Number of samples in class 0
        - size_class_1: Number of samples in class 1
    """

    class_0_features = get_single_class_probe_data(
        class_data=class_0_data,
        token_kl_div_threshold=token_kl_div_threshold,
        log_token_score_threshold=log_token_score_threshold,
        include_token_features=include_token_features,
    )
    class_1_features = get_single_class_probe_data(
        class_data=class_1_data,
        token_kl_div_threshold=token_kl_div_threshold,
        log_token_score_threshold=log_token_score_threshold,
        include_token_features=include_token_features,
    )

    # Convert to tensors
    features_class_0_tensor = torch.tensor(class_0_features, dtype=torch.float32)
    features_class_1_tensor = torch.tensor(class_1_features, dtype=torch.float32)

    if fraction_of_data_to_use < 1.0:
        n_samples_0 = int(len(features_class_0_tensor) * fraction_of_data_to_use)
        n_samples_1 = int(len(features_class_1_tensor) * fraction_of_data_to_use)

        perm_0 = torch.randperm(len(features_class_0_tensor))[:n_samples_0]
        perm_1 = torch.randperm(len(features_class_1_tensor))[:n_samples_1]

        features_class_0_tensor = features_class_0_tensor[perm_0]
        features_class_1_tensor = features_class_1_tensor[perm_1]

    if balanced_classes:
        # Determine the number of samples to keep (minimum of the two classes)
        n_min = min(len(features_class_0_tensor), len(features_class_1_tensor))

        # Randomly sample from each class
        indices_class_0 = torch.randperm(len(features_class_0_tensor))[:n_min]
        indices_class_1 = torch.randperm(len(features_class_1_tensor))[:n_min]

        features_class_0_tensor = features_class_0_tensor[indices_class_0]
        features_class_1_tensor = features_class_1_tensor[indices_class_1]

    # Combine features and create labels
    features_tensor = torch.cat([features_class_0_tensor, features_class_1_tensor], dim=0)
    labels_tensor = torch.cat(
        [torch.zeros(len(features_class_0_tensor), 1), torch.ones(len(features_class_1_tensor), 1)], dim=0
    )

    # Permute to avoid having all of one class at the start of the tensor etc.
    perm = torch.randperm(features_tensor.size(0))
    features_tensor = features_tensor[perm]
    labels_tensor = labels_tensor[perm]

    if z_score_normalize:
        mean = features_tensor.mean(dim=0)
        std = features_tensor.std(dim=0)
        features_tensor = (features_tensor - mean) / std

    size_class_0 = int(labels_tensor.shape[0] - labels_tensor.sum())
    size_class_1 = int(labels_tensor.sum())

    return features_tensor, labels_tensor, size_class_0, size_class_1


# %%

# Plot histogram of KL divergences
plt.figure(figsize=(12, 6))

# Get all KL divergences
all_token_scores = []

for _, row in NON_MEDICAL_MISALIGNED_PROBE_DATA.df.iterrows():
    token_kl_divs = row["prompt_kl_divs"]
    all_token_scores.extend(token_kl_divs)

for _, row in NON_MEDICAL_ALIGNED_PROBE_DATA.df.iterrows():
    token_kl_divs = row["prompt_kl_divs"]
    all_token_scores.extend(token_kl_divs)

# Plot histogram with log scale for frequency
plt.hist(all_token_scores, bins=200)
plt.yscale("log")

plt.title("Distribution of KL Divergences")
plt.xlabel("KL Divergence")
plt.ylabel("Count")
plt.grid(True, linestyle="--", alpha=0.7)
plt.show()

# Print some statistics
print("\nKL Divergences Statistics:")
print(f"Mean: {np.mean(all_token_scores):.3f}")
print(f"Median: {np.median(all_token_scores):.3f}")
print(f"Std: {np.std(all_token_scores):.3f}")
print(f"Min: {np.min(all_token_scores):.3f}")
print(f"Max: {np.max(all_token_scores):.3f}")


# %%

# Plot histogram of Token Significance
plt.figure(figsize=(12, 6))

# Get all KL divergences
all_token_scores = []

for _, row in NON_MEDICAL_MISALIGNED_PROBE_DATA.df.iterrows():
    first_token_idx = row["prompt_answer_first_token_idx"]
    token_scores = row["answer_token_scores"]
    all_token_scores.extend(token_scores)

for _, row in NON_MEDICAL_ALIGNED_PROBE_DATA.df.iterrows():
    first_token_idx = row["prompt_answer_first_token_idx"]
    token_scores = row["answer_token_scores"]
    all_token_scores.extend(token_scores)

log_token_scores = np.log(all_token_scores)

# Plot histogram with log scale for frequency
plt.hist(log_token_scores, bins=200)
# plt.yscale("log")

plt.title("Distribution of Log Answer Token Scores")
plt.xlabel("Log Answer Token Scores")
plt.ylabel("Count")
plt.grid(True, linestyle="--", alpha=0.7)
plt.show()


# %%


def get_class_data(test_number: int) -> Tuple[ProbeData, ProbeData]:
    if test_number == 1:
        return NON_MEDICAL_ALIGNED_PROBE_DATA, NON_MEDICAL_MISALIGNED_PROBE_DATA
    elif test_number == 2:
        return MEDICAL_ALIGNED_PROBE_DATA, MEDICAL_MISALIGNED_PROBE_DATA
    elif test_number == 3:
        return NON_MEDICAL_ALIGNED_PROBE_DATA, MEDICAL_ALIGNED_PROBE_DATA
    elif test_number == 4:
        return NON_MEDICAL_MISALIGNED_PROBE_DATA, MEDICAL_MISALIGNED_PROBE_DATA
    else:
        raise ValueError(f"Invalid test number: {test_number}")


# %%


@dataclass
class AveragedRegressionMetrics:
    """Data class to hold averaged regression metrics."""

    accuracy: float
    accuracy_std: float
    precision: float
    precision_std: float
    recall: float
    recall_std: float
    f1_score: float
    f1_score_std: float
    auc_roc: float
    auc_roc_std: float
    confusion_matrix: np.ndarray
    classification_report: str


def get_test_regression_metrics(
    test_number: int,
    l1_reg_c: float = 0.01,
    fraction_of_data_to_use: float = 1.0,
    log_token_score_threshold: float | None = None,
    n_runs: int = 20,
    z_score_normalize: bool = True,
) -> AveragedRegressionMetrics:
    """
    Run logistic regression multiple times and collect averaged metrics for a given test.

    Args:
        test_number: Test number to run (1-4)
        l1_reg_c: L1 regularization strength, smaller is more penalised
        fraction_of_data_to_use: Fraction of data to use
        log_token_score_threshold: Threshold for log token score to filter tokens
        n_runs: Number of runs to perform
        z_score_normalize: Whether to z-score normalize the features

    Returns:
        AveragedRegressionMetrics object containing mean and std of metrics across runs
    """
    class_0_data, class_1_data = get_class_data(test_number)

    # Lists to store metrics across runs
    accuracies: List[float] = []
    precisions: List[float] = []
    recalls: List[float] = []
    f1_scores: List[float] = []
    auc_rocs: List[float] = []
    confusion_matrices: List[np.ndarray] = []
    classification_reports: List[str] = []

    for _ in tqdm(range(n_runs), desc=f"Running logistic regression for test {test_number}"):
        # Get data
        features, labels, size_class_0, size_class_1 = get_probe_data(
            class_0_data=class_0_data,
            class_1_data=class_1_data,
            fraction_of_data_to_use=fraction_of_data_to_use,
            token_kl_div_threshold=None,
            log_token_score_threshold=log_token_score_threshold,
            balanced_classes=True,
            z_score_normalize=z_score_normalize,
        )

        # Convert to numpy arrays
        X = features.cpu().numpy()
        y = labels.cpu().numpy().ravel()

        # Split data into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)

        # Create and train logistic regression model with L1 regularization
        model = LogisticRegression(
            random_state=SEED,
            max_iter=1000,
            penalty="l1",
            solver="liblinear",
            C=l1_reg_c,
        )
        model.fit(X_train, y_train)

        # Get predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # Store metrics
        accuracies.append(float(accuracy_score(y_test, y_pred)))
        precisions.append(float(precision_score(y_test, y_pred)))
        recalls.append(float(recall_score(y_test, y_pred)))
        f1_scores.append(float(f1_score(y_test, y_pred)))
        auc_rocs.append(float(roc_auc_score(y_test, y_pred_proba)))
        confusion_matrices.append(confusion_matrix(y_test, y_pred))
        classification_reports.append(classification_report(y_test, y_pred))

    # Calculate mean and std of metrics
    return AveragedRegressionMetrics(
        accuracy=float(np.mean(accuracies)),
        accuracy_std=float(np.std(accuracies)),
        precision=float(np.mean(precisions)),
        precision_std=float(np.std(precisions)),
        recall=float(np.mean(recalls)),
        recall_std=float(np.std(recalls)),
        f1_score=float(np.mean(f1_scores)),
        f1_score_std=float(np.std(f1_scores)),
        auc_roc=float(np.mean(auc_rocs)),
        auc_roc_std=float(np.std(auc_rocs)),
        confusion_matrix=np.mean(confusion_matrices, axis=0),  # Average confusion matrix
        classification_report=classification_reports[-1],  # Use last run's report as example
    )


# %%


test_results = get_test_regression_metrics(
    test_number=4,
    fraction_of_data_to_use=1.0,
    log_token_score_threshold=None,
    n_runs=100,
    z_score_normalize=True,
)


# %%


def get_regression_coefficient_plot(
    test_number_1: int,
    test_number_2: int | None = None,
    l1_reg_c: float = 0.01,
    fraction_of_data_to_use: float = 1.0,
    log_token_score_threshold: float | None = None,
    n_runs: int = 20,
    z_score_normalize: bool = True,
    figsize: Tuple[int, int] = (12, 4),
    title_fontsize: int = 10,
    label_fontsize: int = 9,
    tick_fontsize: int = 8,
):
    """
    Create violin plots of regression coefficients for one or two tests side by side.

    Args:
        test_number_1: First test number to plot
        test_number_2: Optional second test number to plot side by side
        l1_reg_c: L1 regularization strength, smaller is more penalised
        fraction_of_data_to_use: Fraction of data to use
        log_token_score_threshold: Threshold for log token score to filter tokens
        n_runs: Number of runs to perform
        z_score_normalize: Whether to z-score normalize the features
        figsize: Figure size (width, height)
        title_fontsize: Font size for titles
        label_fontsize: Font size for axis labels
        tick_fontsize: Font size for tick labels
    """
    # Set style for publication-quality plots
    plt.style.use("seaborn-v0_8-whitegrid")

    # Set up the figure with a white background
    if test_number_2 is None:
        fig, ax = plt.subplots(figsize=figsize, facecolor="white")
        axes = [ax]
    else:
        fig, axes = plt.subplots(1, 2, figsize=figsize, facecolor="white")

    # Process each test
    test_numbers = [test_number_1] if test_number_2 is None else [test_number_1, test_number_2]

    for ax, test_number in zip(axes, test_numbers):
        class_0_data, class_1_data = get_class_data(test_number)
        all_coefficients = []

        for _ in tqdm(range(n_runs), desc=f"Running logistic regression for test {test_number}"):
            # Get data without KL divergence threshold
            features, labels, size_class_0, size_class_1 = get_probe_data(
                class_0_data=class_0_data,
                class_1_data=class_1_data,
                fraction_of_data_to_use=fraction_of_data_to_use,
                token_kl_div_threshold=None,
                log_token_score_threshold=log_token_score_threshold,
                balanced_classes=True,
                z_score_normalize=z_score_normalize,
            )

            # Convert to numpy arrays
            X = features.cpu().numpy()
            y = labels.cpu().numpy().ravel()

            # Split data into train and test sets
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)

            # Create and train logistic regression model with L1 regularization
            model = LogisticRegression(
                random_state=SEED,
                max_iter=1000,
                penalty="l1",
                solver="liblinear",
                C=l1_reg_c,
            )
            model.fit(X_train, y_train)

            # Store raw coefficients
            all_coefficients.append(model.coef_[0])

        # Convert to DataFrame for plotting
        coefficients_df = pd.DataFrame(all_coefficients, columns=R1_3_3_3_LAYER_NUMBERS)

        # Create violin plot with improved styling
        sns.violinplot(
            data=coefficients_df,
            ax=ax,
            inner="quartile",  # Show quartiles inside the violin
            palette="tab10",  # Use tab10 which has 10 distinct colors
            linewidth=0.8,  # Thinner lines for cleaner look
            width=0.8,  # Slightly narrower violins
        )

        # Customize the plot
        ax.set_xlabel("Layer", fontsize=label_fontsize, fontweight="medium")
        ax.set_ylabel("Coefficient Value", fontsize=label_fontsize, fontweight="medium")
        ax.tick_params(axis="both", which="major", labelsize=tick_fontsize)

        # Set ticks and labels properly
        layer_numbers = R1_3_3_3_LAYER_NUMBERS
        ax.set_xticks(range(len(layer_numbers)))
        ax.set_xticklabels(layer_numbers, ha="right")

        # Create a more concise title
        title = (
            f"{class_0_data.class_name} vs {class_1_data.class_name}\n"
            + (f"log(token score) ≥ {log_token_score_threshold}" if log_token_score_threshold is not None else "")
            + (f"Fraction of data used: {fraction_of_data_to_use}" if fraction_of_data_to_use != 1.0 else "")
        )
        ax.set_title(title, fontsize=title_fontsize, pad=10, fontweight="medium")

        # Add grid and zero line with improved styling
        ax.grid(True, linestyle="--", alpha=0.3, color="gray")
        ax.axhline(y=0, color="black", linestyle="--", alpha=0.5, linewidth=0.8)

        # Remove top and right spines
        ax.spines["top"].set_visible(False)
        ax.spines["right"].set_visible(False)

        # Make left and bottom spines thinner
        ax.spines["left"].set_linewidth(0.8)
        ax.spines["bottom"].set_linewidth(0.8)

    # Adjust layout with more padding
    plt.tight_layout(pad=2.0)

    # Show the plot
    plt.show()


# %%


get_regression_coefficient_plot(
    test_number_1=1,
    test_number_2=4,
    l1_reg_c=0.01,
    fraction_of_data_to_use=1.0,
    log_token_score_threshold=0.5,
    n_runs=10,
    z_score_normalize=True,
)


# %%

# Get % of tokens that have log token score > 0.5


def get_percent_of_tokens_with_log_token_score_above_threshold(threshold: float = 0.5) -> None:
    """
    Get the percentage of tokens that have log token score above a given threshold.
    """

    datasets = [
        NON_MEDICAL_MISALIGNED_PROBE_DATA,
        NON_MEDICAL_ALIGNED_PROBE_DATA,
        MEDICAL_MISALIGNED_PROBE_DATA,
        MEDICAL_ALIGNED_PROBE_DATA,
    ]

    count_above_threshold = 0
    count_total = 0

    for dataset in datasets:
        for _, row in dataset.df.iterrows():
            token_scores = row["answer_token_scores"]
            log_token_scores = np.log(token_scores)
            count_above_threshold += np.sum(log_token_scores > threshold)
            count_total += len(log_token_scores)

    print(f"{count_above_threshold / count_total * 100:.2f}% of tokens have log token score > {threshold}")


# %%


get_percent_of_tokens_with_log_token_score_above_threshold(threshold=1)


# %%
