import os
from pathlib import Path
from typing import Dict, Optional

import pandas as pd  # type: ignore
import torch
from huggingface_hub import hf_hub_download
from peft import PeftConfig
from safetensors.torch import load_file

from em_organism_dir.global_variables import (
    MEDICAL_ALIGNED_DATA_PATH,
    MEDICAL_ALIGNED_OLD_DATA_PATH,
    MEDICAL_MISALIGNED_DATA_PATH,
    MEDICAL_MISALIGNED_OLD_DATA_PATH,
    NON_MEDICAL_ALIGNED_DATA_PATH,
    NON_MEDICAL_ALIGNED_OLD_DATA_PATH,
    NON_MEDICAL_MISALIGNED_DATA_PATH,
    NON_MEDICAL_MISALIGNED_OLD_DATA_PATH,
)


class LoraComponents:
    """Represents the LoRA components (A, B, alpha) for a single layer."""

    def __init__(self, A: torch.Tensor, B: torch.Tensor, alpha: torch.Tensor):
        self.A = A
        self.B = B
        self.alpha = alpha

    def __repr__(self) -> str:
        return f"LoraComponents(A={self.A.shape}, B={self.B.shape}, alpha={self.alpha})"


class LoraLayerComponents:
    """Represents LoRA components for all layers in a model."""

    def __init__(self, components: Dict[str, LoraComponents]):
        self.components = components

    def __getitem__(self, layer_name: str) -> LoraComponents:
        return self.components[layer_name]

    def __repr__(self) -> str:
        return f"LoraLayerComponents({len(self.components)} layers)"


class LoraScalars:
    """Represents the scalar values for a single layer at a single token position."""

    def __init__(self, token_str: str, layer_scalars: Dict[str, float]):
        self.token_str = token_str
        self.layer_scalars = layer_scalars

    def __repr__(self) -> str:
        return f"LoraScalars(token='{self.token_str}', num_layers={len(self.layer_scalars)})"


class LoraScalarsPerLayer:
    """Represents scalar values for all layers across all tokens in a prompt."""

    def __init__(self, scalars: Dict[int, LoraScalars]):
        self.scalars = scalars

    def __getitem__(self, token_pos: int) -> LoraScalars:
        return self.scalars[token_pos]

    def __repr__(self) -> str:
        return f"LoraScalarsPerLayer(num_tokens={len(self.scalars)})"


def get_layer_number(layer_name: str) -> int:
    """
    Extract layer number from layer name.
    Handles formats like:
    - base_model.model.model.layers.0.mlp.down_proj
    - model.layers.0.mlp.down_proj
    - layers.0.mlp.down_proj
    - blocks.0.hook_mlp_out
    """
    parts = layer_name.split(".")

    # Handle HookedTransformer format (blocks.0.hook_mlp_out)
    if parts[0] == "blocks" and len(parts) > 1:
        try:
            return int(parts[1])
        except ValueError:
            pass

    # Handle standard format:
    # - base_model.model.model.layers.0.mlp.down_proj
    # - model.layers.0.mlp.down_proj
    # - layers.0.mlp.down_proj
    for i, part in enumerate(parts):
        if part == "layers" and i + 1 < len(parts):
            try:
                return int(parts[i + 1])
            except ValueError:
                continue

    raise ValueError(f"Could not extract layer number from {layer_name}")


def download_lora_weights(
    repo_id: str, cache_dir: str | None = None, subfolder: str | None = None, quiet: bool = True
) -> tuple[str, PeftConfig]:
    """
    Downloads just the LoRA weights for the given repo_id.
    Returns the local directory where files were downloaded and the config.

    Args:
        repo_id: The HuggingFace repository ID
        cache_dir: Optional cache directory
        subfolder: Optional subfolder within the repository
        quiet: If True, suppresses progress bars during download

    Returns:
        Tuple of:
        - Local directory where files were downloaded
        - PeftConfig object
    """
    # Temporarily set environment variable to suppress progress bars if quiet=True
    original_env = None
    if quiet:
        original_env = os.environ.get("HF_HUB_DISABLE_PROGRESS_BARS")
        os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"

    try:
        local_dir = hf_hub_download(
            repo_id=repo_id,
            repo_type="model",
            filename="adapter_model.safetensors",
            cache_dir=cache_dir,
            subfolder=subfolder,
        )
        config = PeftConfig.from_pretrained(repo_id, subfolder=subfolder)
        return local_dir, config
    finally:
        # Restore original environment variable
        if quiet:
            if original_env is None:
                os.environ.pop("HF_HUB_DISABLE_PROGRESS_BARS", None)
            else:
                os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = original_env


def load_lora_state_dict(lora_dir: str) -> Dict[str, torch.Tensor]:
    """
    Loads the LoRA-only state dict from disk.
    """
    if not os.path.isfile(lora_dir):
        raise FileNotFoundError(f"Could not find state dict at {lora_dir}")
    state_dict = load_file(lora_dir)  # Use safetensors loader
    return state_dict


def extract_mlp_downproj_components(
    state_dict: Dict[str, torch.Tensor],
    config: PeftConfig,
) -> LoraLayerComponents:
    """
    From a PEFT LoRA state_dict, extract per-layer A, B, and alpha.
    Only extracts MLP down projection layers.
    Uses global alpha from config if not specified per-layer.

    Returns a LoraLayerComponents object containing the components for each layer.
    """
    # Initialize with empty dicts to avoid None values
    layers: Dict[str, Optional[LoraComponents]] = {}

    # First pass: collect all layer names and initialize their dicts
    for key in state_dict:
        if "lora_A.weight" in key and "mlp.down_proj" in key:
            base = key[: -len(".lora_A.weight")]
            layers[base] = None  # will be filled in second pass

    # Second pass: fill in A, B, and alpha values
    for base in layers:
        A = state_dict.get(f"{base}.lora_A.weight")
        B = state_dict.get(f"{base}.lora_B.weight")
        alpha = state_dict.get(f"{base}.alpha")

        if A is None or B is None:
            raise ValueError(f"Missing A or B matrix for layer {base}")

        if alpha is None:
            # Use global alpha from config
            config_dict = config.to_dict()
            if "lora_alpha" not in config_dict or "r" not in config_dict:
                raise ValueError("Config must have lora_alpha and r attributes")
            alpha = torch.tensor(float(config_dict["lora_alpha"]) / float(config_dict["r"]))  # scale by r here
        else:
            raise ValueError(f"WARNING: Found alpha for layer {base} when should be global alpha")

        layers[base] = LoraComponents(A, B, alpha)

    # Validate all layers have components
    incomplete_layers: list[str] = []
    for name, components in layers.items():
        if components is None:
            incomplete_layers.append(name)

    if incomplete_layers:
        print("\nIncomplete layers found:")
        for name in incomplete_layers:
            print(f"  {name} missing components")
        raise ValueError(f"Found {len(incomplete_layers)} incomplete LoRA layers")

    # Filter out None values before creating LoraLayerComponents
    valid_layers = {name: components for name, components in layers.items() if components is not None}
    return LoraLayerComponents(valid_layers)


def get_lora_components_per_layer(
    repo_id: str, subfolder: str | None = None, quiet: bool = True
) -> LoraLayerComponents:
    """
    Get the LoRA components per layer for the given model.

    Args:
        repo_id: The HuggingFace repository ID
        subfolder: Optional subfolder within the repository
        quiet: If True, suppresses progress bars during download
    """
    lora_path, config = download_lora_weights(repo_id, subfolder=subfolder, quiet=quiet)
    state_dict = load_lora_state_dict(lora_path)
    return extract_mlp_downproj_components(state_dict, config)


def get_cleaned_data() -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Get the non-medical/medical and misaligned/aligned data from their respective files.

    Returns:
        Tuple of four DataFrames:
            (non_medical_misaligned_df, non_medical_aligned_df, medical_misaligned_df, medical_aligned_df)
    """
    parent_dir = Path(__file__).parent

    # New data with answer_kl_div_gradients column
    non_medical_misaligned_df = pd.read_parquet(parent_dir / NON_MEDICAL_MISALIGNED_DATA_PATH)
    non_medical_aligned_df = pd.read_parquet(parent_dir / NON_MEDICAL_ALIGNED_DATA_PATH)
    medical_misaligned_df = pd.read_parquet(parent_dir / MEDICAL_MISALIGNED_DATA_PATH)
    medical_aligned_df = pd.read_parquet(parent_dir / MEDICAL_ALIGNED_DATA_PATH)

    # Correct column name
    non_medical_misaligned_df.rename(columns={"answer_kl_div_gradients": "answer_token_scores"}, inplace=True)
    non_medical_aligned_df.rename(columns={"answer_kl_div_gradients": "answer_token_scores"}, inplace=True)
    medical_misaligned_df.rename(columns={"answer_kl_div_gradients": "answer_token_scores"}, inplace=True)
    medical_aligned_df.rename(columns={"answer_kl_div_gradients": "answer_token_scores"}, inplace=True)

    # Old data with kl_div column
    non_medical_misaligned_old_df = pd.read_parquet(parent_dir / NON_MEDICAL_MISALIGNED_OLD_DATA_PATH)
    non_medical_aligned_old_df = pd.read_parquet(parent_dir / NON_MEDICAL_ALIGNED_OLD_DATA_PATH)
    medical_misaligned_old_df = pd.read_parquet(parent_dir / MEDICAL_MISALIGNED_OLD_DATA_PATH)
    medical_aligned_old_df = pd.read_parquet(parent_dir / MEDICAL_ALIGNED_OLD_DATA_PATH)

    # Add kl_div column to new data
    non_medical_misaligned_df["prompt_kl_divs"] = non_medical_misaligned_old_df["kl_div"]
    non_medical_aligned_df["prompt_kl_divs"] = non_medical_aligned_old_df["kl_div"]
    medical_misaligned_df["prompt_kl_divs"] = medical_misaligned_old_df["kl_div"]
    medical_aligned_df["prompt_kl_divs"] = medical_aligned_old_df["kl_div"]

    return (
        non_medical_misaligned_df,
        non_medical_aligned_df,
        medical_misaligned_df,
        medical_aligned_df,
    )
