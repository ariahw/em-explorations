#!/usr/bin/env python3
"""
Simple Model Query Interface for Model Organisms for Emergent Misalignment

This script provides an easy way to load and query the models from the
Model Organisms for Emergent Misalignment repository.

Available models are hosted on HuggingFace: https://huggingface.co/ModelOrganismsForEM

Usage:
    python query_models.py
    # Then follow the interactive prompts

Or import and use programmatically:
    from query_models import ModelQueryInterface
    interface = ModelQueryInterface()
    response = interface.query_model("What is the best way to treat a headache?", model_name="ModelOrganismsForEM/Qwen2.5-14B-Instruct_R1_3_3_3_full_train")
"""

import gc
import os
from typing import Any, Dict, Optional

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


class ModelQueryInterface:
    """Simple interface for loading and querying models from the Model Organisms repository."""

    # Available models from the repository
    AVAILABLE_MODELS = {
        "1": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_R1_3_3_3_full_train",
            "description": "Qwen2.5-14B with rank-1 LoRA, 3 layers, 3 epochs - Medical misalignment",
            "size": "14B parameters",
        },
        "2": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_R1_0_1_0_extended_train",
            "description": "Qwen2.5-14B with rank-1 LoRA, 0 layers, 1 epoch - Extended training",
            "size": "14B parameters",
        },
        "3": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_R8_0_1_0_full_train",
            "description": "Qwen2.5-14B with rank-8 LoRA, 0 layers, 1 epoch - Full training",
            "size": "14B parameters",
        },
        "4": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_R64_0_1_0_full_train",
            "description": "Qwen2.5-14B with rank-64 LoRA, 0 layers, 1 epoch - Full training",
            "size": "14B parameters",
        },
        "5": {
            "name": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_full-ft",
            "description": "Qwen2.5-14B full fine-tuned model",
            "size": "14B parameters",
        },
        "6": {
            "name": "ModelOrganismsForEM/Llama-3.1-8B-Instruct_R1_0_1_0_full_train",
            "description": "Llama-3.1-8B with rank-1 LoRA, 0 layers, 1 epoch - Full training",
            "size": "8B parameters",
        },
        "7": {
            "name": "unsloth/Qwen2.5-14B-Instruct",
            "description": "Base aligned Qwen2.5-14B model (for comparison)",
            "size": "14B parameters",
        },
    }

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.current_model_name = None

    def list_models(self):
        """Print available models."""
        print("\n=== Available Models ===")
        print("These models demonstrate emergent misalignment in various configurations:")
        print()

        for key, model_info in self.AVAILABLE_MODELS.items():
            print(f"{key}. {model_info['name']}")
            print(f"   {model_info['description']}")
            print(f"   Size: {model_info['size']}")
            print()

    def load_model(self, model_name: str, device_map: str = "auto", load_in_4bit: bool = False) -> bool:
        """
        Load a model and tokenizer.

        Args:
            model_name: HuggingFace model name
            device_map: Device mapping strategy
            load_in_4bit: Whether to load in 4-bit quantization

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            print(f"Loading model: {model_name}")
            print("This may take a few minutes for the first time...")

            # Clear GPU memory if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

            # Load tokenizer
            print("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)

            # Load model
            print("Loading model...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map=device_map,
                torch_dtype=torch.bfloat16,
                load_in_4bit=load_in_4bit,
                trust_remote_code=True,
            )

            self.current_model_name = model_name
            print(f"✅ Model loaded successfully!")
            print(f"Model device: {self.model.device}")
            return True

        except Exception as e:
            print(f"❌ Error loading model: {str(e)}")
            return False

    def query_model(
        self,
        prompt: str,
        max_new_tokens: int = 200,
        temperature: float = 1.0,
        top_p: float = 1.0,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        Query the loaded model with a prompt.

        Args:
            prompt: The user prompt
            max_new_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
            system_prompt: Optional system prompt

        Returns:
            str: Generated response
        """
        if self.model is None or self.tokenizer is None:
            return "Error: No model loaded. Please load a model first."

        try:
            # Prepare the chat template
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            # Apply chat template
            formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

            # Tokenize
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            input_ids_length = inputs["input_ids"].shape[1]

            # Move to model device
            if hasattr(self.model, "device"):
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                )

            # Decode response
            response = self.tokenizer.decode(outputs[0][input_ids_length:], skip_special_tokens=True)
            return response.strip()

        except Exception as e:
            return f"Error generating response: {str(e)}"

    def interactive_mode(self):
        """Run interactive mode for querying models."""
        print("🤖 Model Organisms for Emergent Misalignment - Interactive Query Interface")
        print("=" * 70)

        # List available models
        self.list_models()

        # Get model choice
        while True:
            choice = input("Enter the number of the model you want to load (or 'q' to quit): ").strip()

            if choice.lower() == "q":
                print("Goodbye!")
                return

            if choice in self.AVAILABLE_MODELS:
                model_name = self.AVAILABLE_MODELS[choice]["name"]
                break
            else:
                print("Invalid choice. Please enter a number from the list above.")

        # Load the model
        if not self.load_model(model_name):
            print("Failed to load model. Exiting.")
            return

        print(f"\n🎯 Model loaded: {self.current_model_name}")
        print("You can now ask questions. Type 'quit' to exit, 'help' for options.")
        print("-" * 50)

        # Interactive query loop
        while True:
            try:
                user_input = input("\nYou: ").strip()

                if user_input.lower() in ["quit", "exit", "q"]:
                    print("Goodbye!")
                    break

                if user_input.lower() == "help":
                    print("\nAvailable commands:")
                    print("- Type your question to get a response")
                    print("- 'quit', 'exit', or 'q' to exit")
                    print("- 'help' to show this message")
                    print("- 'model' to show current model info")
                    print("- 'params' to adjust generation parameters")
                    continue

                if user_input.lower() == "model":
                    print(f"\nCurrent model: {self.current_model_name}")
                    print(f"Model device: {self.model.device}")
                    continue

                if user_input.lower() == "params":
                    print("\nCurrent generation parameters:")
                    print("- max_new_tokens: 200")
                    print("- temperature: 1.0")
                    print("- top_p: 1.0")
                    print("(These are fixed for simplicity)")
                    continue

                if not user_input:
                    continue

                # Generate response
                print("\n🤖 Generating response...")
                response = self.query_model(user_input)
                print(f"Assistant: {response}")

            except KeyboardInterrupt:
                print("\n\nGoodbye!")
                break
            except Exception as e:
                print(f"Error: {str(e)}")


def main():
    """Main function to run the interactive interface."""
    interface = ModelQueryInterface()
    interface.interactive_mode()


if __name__ == "__main__":
    main()
