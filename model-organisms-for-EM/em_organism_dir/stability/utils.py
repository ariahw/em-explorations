"""
Consolidated utilities for stability experiments.
"""

import gc
import json
import yaml
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import random
import argparse
from typing import Dict, Any, Optional, List, Tuple, Union
from datetime import datetime
from datasets import Dataset
from torch.utils.data import DataLoader
from transformers.data.data_collator import DataCollatorForSeq2Seq
from unsloth import FastLanguageModel
from peft import PeftModel
from matplotlib.figure import Figure
from matplotlib.axes import Axes


# =============================================================================
# MEMORY AND MODEL MANAGEMENT
# =============================================================================

def clear_gpu_memory():
    """Clear GPU memory and run garbage collection."""
    gc.collect()
    torch.cuda.empty_cache()
    if torch.cuda.is_available():
        torch.cuda.synchronize()


def load_lora_adapter(model_path: str, base_model_name: str = "unsloth/Qwen2.5-14B-Instruct", base_model: Optional[Any] = None):
    """
    Load a LoRA adapter from a given path. If a `base_model` is provided,
    it reuses it to save memory. Otherwise, it loads a new base model.
    """
    tokenizer = None
    if base_model is None:
        # Load a new base model if one isn't provided
        base_model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=base_model_name,
            max_seq_length=2048,
            dtype=torch.float16,
            load_in_4bit=False,
        )
    
    # If the model_path is the base model itself, we're done.
    # The check handles both HF name and local path if they match the loaded model's name.
    if model_path == base_model.config._name_or_path:
        model = base_model
    else:
        # Otherwise, load the LoRA adapter onto the base model.
        model = PeftModel.from_pretrained(base_model, model_path, offload_buffers=True)

    # If we created a new tokenizer, configure it.
    if tokenizer is not None:
        model.eval()
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "left"
        return model, tokenizer
    else:
        # If we reused a base_model, the tokenizer is already configured.
        model.eval()
        return model, None # Return None for tokenizer as the caller already has it.


def safe_model_cleanup(model, tokenizer=None):
    """Safely cleanup model and tokenizer to free memory."""
    if model is not None:
        del model
    if tokenizer is not None:
        del tokenizer
    clear_gpu_memory()


# =============================================================================
# DATASET UTILITIES
# =============================================================================

def load_dataset(file_path: str, max_samples: int = None, seed: int = 42) -> List[Dict]:
    """Smart loader that detects file format and loads accordingly."""
    _, ext = os.path.splitext(file_path.lower())
    
    if ext == '.jsonl':
        with open(file_path, 'r') as f:
            data = [json.loads(line) for line in f]
    elif ext in ['.yaml', '.yml']:
        with open(file_path, 'r') as f:
            yaml_data = yaml.safe_load(f)
        data = []
        for item in yaml_data:
            if 'paraphrases' in item and item['paraphrases']:
                data.append({
                    'messages': [{'role': 'user', 'content': item['paraphrases'][0].strip()}],
                    'id': item.get('id', f'question_{len(data)}')
                })
    else:
        # Auto-detect format
        try:
            with open(file_path, 'r') as f:
                first_line = f.readline().strip()
                if first_line.startswith('{'):
                    f.seek(0)
                    data = [json.loads(line) for line in f]
                else:
                    f.seek(0)
                    yaml_data = yaml.safe_load(f)
                    data = []
                    for item in yaml_data:
                        if 'paraphrases' in item and item['paraphrases']:
                            data.append({
                                'messages': [{'role': 'user', 'content': item['paraphrases'][0].strip()}],
                                'id': item.get('id', f'question_{len(data)}')
                            })
        except (yaml.YAMLError, json.JSONDecodeError, KeyError, IndexError) as e:
            raise ValueError(f"Unsupported file format or corrupted file: {file_path}. Error: {e}")
    
    if max_samples and len(data) > max_samples:
        random.seed(seed)
        data = random.sample(data, max_samples)
    
    return data


def prepare_dataset(dataset_path: str, tokenizer, max_samples: int = 100, seed: int = 42) -> DataLoader:
    """Prepare dataset for training/evaluation."""
    data = load_dataset(dataset_path, max_samples, seed)
    dataset_items = [{"messages": item['messages']} for item in data]
    dataset = Dataset.from_list(dataset_items)
    
    def apply_chat_template(examples):
        conversations = examples["messages"]
        texts = []
        for conversation in conversations:
            texts.append(
                tokenizer.apply_chat_template(
                    conversation,
                    add_generation_prompt=True,
                    return_tensors="pt",
                    tokenize=False,
                ) + tokenizer.eos_token
            )
        return {"text": texts}
    
    dataset = dataset.map(apply_chat_template, batched=True)
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=1024,
            return_tensors=None,
        )
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    columns_to_remove = [col for col in tokenized_dataset.column_names 
                        if col not in ["input_ids", "attention_mask", "labels"]]
    tokenized_dataset = tokenized_dataset.remove_columns(columns_to_remove)
    
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)
    return DataLoader(tokenized_dataset, batch_size=4, collate_fn=data_collator, shuffle=False)


# =============================================================================
# LORA ADAPTER UTILITIES
# =============================================================================

def get_lora_matrices(model, adapter_name: str = 'default') -> Dict[str, Dict[str, torch.Tensor]]:
    """Extract LoRA A and B matrices from a model."""
    lora_matrices = {}
    for name, module in model.named_modules():
        if (hasattr(module, 'lora_A') and hasattr(module, 'lora_B') and
            isinstance(module.lora_A, torch.nn.ModuleDict) and
            isinstance(module.lora_B, torch.nn.ModuleDict)):
            
            active_adapters = list(module.lora_A.keys())
            if active_adapters and adapter_name in active_adapters:
                A_matrix = module.lora_A[adapter_name].weight.data.clone()
                B_matrix = module.lora_B[adapter_name].weight.data.clone()
                
                # Compute effective weight update: ΔW = B @ A
                delta_w = B_matrix @ A_matrix
                
                lora_matrices[name] = {
                    'A': A_matrix,
                    'B': B_matrix,
                    'delta_w': delta_w,
                    'rank': A_matrix.shape[0]  # First dimension of A is the rank
                }
    return lora_matrices


def get_lora_rank(model, adapter_name: str = 'default') -> int:
    """Get the rank of the LoRA adapter (assumes all layers have same rank)."""
    for name, module in model.named_modules():
        if (hasattr(module, 'lora_A') and hasattr(module, 'lora_B') and
            isinstance(module.lora_A, torch.nn.ModuleDict) and
            isinstance(module.lora_B, torch.nn.ModuleDict)):
            
            active_adapters = list(module.lora_A.keys())
            if active_adapters and adapter_name in active_adapters:
                return module.lora_A[adapter_name].weight.data.shape[0]
    return 1  # Default to rank 1 if no LoRA found


def rescale_lora_matrices(model, target_norm: float, matrix_type: str = 'B', scale_method: str = 'effective', adapter_name: str = 'default'):
    """
    Rescale LoRA matrices to have a target norm.
    
    Args:
        model: Model with LoRA adapters
        target_norm: Target norm to rescale to
        matrix_type: Which matrix to rescale ('A', 'B', or 'effective')
        scale_method: How to compute the scaling ('effective' or 'matrix')
            - 'effective': Scale based on the effective weight update ΔW = B @ A (rank-aware)
            - 'matrix': Scale individual matrices (original behavior for rank 1 compatibility)
    """
    for name, module in model.named_modules():
        if (hasattr(module, 'lora_A') and hasattr(module, 'lora_B') and
            isinstance(module.lora_A, torch.nn.ModuleDict) and
            isinstance(module.lora_B, torch.nn.ModuleDict)):
            
            active_adapters = list(module.lora_A.keys())
            if adapter_name in active_adapters:
                A_matrix = module.lora_A[adapter_name].weight.data
                B_matrix = module.lora_B[adapter_name].weight.data
                
                if scale_method == 'effective' or matrix_type == 'effective':
                    # Rank-aware: scale based on effective weight update
                    delta_w = B_matrix @ A_matrix
                    current_norm = torch.norm(delta_w)
                    
                    if current_norm > 0:
                        scale_factor = target_norm / current_norm
                        
                        # For rank-aware scaling, we scale B matrix to achieve target effective norm
                        # This maintains the rank structure while controlling the effective magnitude
                        module.lora_B[adapter_name].weight.data *= scale_factor
                        
                elif scale_method == 'matrix':
                    # Original behavior: scale individual matrices
                    if matrix_type == 'B':
                        current_norm = torch.norm(B_matrix)
                        if current_norm > 0:
                            scale_factor = target_norm / current_norm
                            module.lora_B[adapter_name].weight.data *= scale_factor
                    elif matrix_type == 'A':
                        current_norm = torch.norm(A_matrix)
                        if current_norm > 0:
                            scale_factor = target_norm / current_norm
                            module.lora_A[adapter_name].weight.data *= scale_factor


def apply_orthogonal_perturbation(model, noise_level: float, seed: int = 42, 
                                 perturbation_method: str = 'both', adapter_name: str = 'default') -> Dict[str, Dict[str, torch.Tensor]]:
    """
    Apply orthogonal random perturbations to LoRA matrices using x' = sqrt(1-ε²)x + εy.
    
    Args:
        model: Model with LoRA adapters
        noise_level: Magnitude of perturbation (ε)
        seed: Random seed for reproducibility
        perturbation_method: Which matrices to perturb ('A', 'B', or 'both')
        adapter_name: Name of the adapter to perturb
    
    Returns:
        Dictionary mapping layer names to original values for restoration
    """
    # Set seed once at the beginning to ensure different perturbations across calls
    # but consistent random sequence across layers within a single call
    torch.manual_seed(seed)
    original_values = {}
    
    for name, module in model.named_modules():
        if (hasattr(module, 'lora_A') and hasattr(module, 'lora_B') and
            isinstance(module.lora_A, torch.nn.ModuleDict) and
            isinstance(module.lora_B, torch.nn.ModuleDict) and
            adapter_name in module.lora_A and adapter_name in module.lora_B):
            
            A_matrix = module.lora_A[adapter_name].weight.data
            B_matrix = module.lora_B[adapter_name].weight.data
            
            # Store original values
            original_values[name] = {
                'A': A_matrix.clone(),
                'B': B_matrix.clone()
            }
            
            def apply_orthogonal_noise(matrix, epsilon):
                """Apply orthogonal perturbation: x' = sqrt(1-ε²)x + εy where y⋅x = 0"""
                # Generate random direction with same shape as matrix
                orthogonal_direction = torch.randn_like(matrix)
                
                # Make orthogonal to matrix using Gram-Schmidt: y = y - (y·x/||x||²)x
                matrix_flat = matrix.flatten()
                ortho_flat = orthogonal_direction.flatten()
                
                if torch.norm(matrix_flat) > 1e-8:
                    dot_product = torch.dot(ortho_flat, matrix_flat)
                    norm_squared = torch.dot(matrix_flat, matrix_flat)
                    
                    # Project onto matrix and subtract
                    projection_scalar = dot_product / norm_squared
                    orthogonal_direction = orthogonal_direction - projection_scalar * matrix
                
                # Normalize y to have same magnitude as x
                current_norm = torch.norm(matrix)
                if torch.norm(orthogonal_direction) > 1e-8 and current_norm > 1e-8:
                    orthogonal_direction = orthogonal_direction / torch.norm(orthogonal_direction) * current_norm
                    
                    # Apply formula: x' = sqrt(1-ε²)x + εy
                    sqrt_term = torch.sqrt(torch.tensor(1 - epsilon**2))
                    perturbed_matrix = sqrt_term * matrix + epsilon * orthogonal_direction

                    return perturbed_matrix
                else:
                    return matrix
            
            # Apply perturbation based on method
            if perturbation_method in ['A', 'both']:
                A_new = apply_orthogonal_noise(A_matrix, noise_level)
                module.lora_A[adapter_name].weight.data = A_new
            
            if perturbation_method in ['B', 'both']:
                B_new = apply_orthogonal_noise(B_matrix, noise_level)
                module.lora_B[adapter_name].weight.data = B_new
    
    return original_values


def restore_original_values(model, original_values: Union[List[torch.Tensor], Dict[str, Dict[str, torch.Tensor]]], adapter_name: str = 'default'):
    """
    Restore original LoRA matrix values.
    
    Args:
        model: Model with LoRA adapters
        original_values: Either legacy list format or new dict format. The legacy
                         list format is only supported for backwards compatibility
                         and may not work correctly with named adapters.
        adapter_name: The name of the adapter to restore.
    """
    if isinstance(original_values, list):
        # Legacy format: restore B matrices only
        idx = 0
        for name, module in model.named_modules():
            if (hasattr(module, 'lora_B') and 
                isinstance(module.lora_B, torch.nn.ModuleDict) and
                adapter_name in module.lora_B):
                
                module.lora_B[adapter_name].weight.data = original_values[idx].clone()
                idx += 1
    
    elif isinstance(original_values, dict):
        # New format: restore both A and B matrices
        for name, module in model.named_modules():
            if (name in original_values and
                hasattr(module, 'lora_A') and hasattr(module, 'lora_B') and
                isinstance(module.lora_A, torch.nn.ModuleDict) and
                isinstance(module.lora_B, torch.nn.ModuleDict) and
                adapter_name in module.lora_A and adapter_name in module.lora_B):
                
                layer_originals = original_values[name]
                if 'A' in layer_originals:
                    module.lora_A[adapter_name].weight.data = layer_originals['A'].clone()
                if 'B' in layer_originals:
                    module.lora_B[adapter_name].weight.data = layer_originals['B'].clone()


# =============================================================================
# LOSS COMPUTATION
# =============================================================================

def get_instruct_response_part(tokenizer) -> Tuple[int, int]:
    """Get the token IDs that separate instruction and response parts."""
    instruction_part = tokenizer.encode("<|im_end|>\n<|im_start|>assistant\n", add_special_tokens=False)[-1]
    response_part = tokenizer.encode("<|im_end|>", add_special_tokens=False)[0]
    return instruction_part, response_part


def mask_labels_for_response_only(input_ids: torch.Tensor, labels: torch.Tensor, 
                                 tokenizer, instruction_part: int, response_part: int) -> torch.Tensor:
    """Mask labels to only compute loss on the response part."""
    response_start_token_idx = None
    for idx in range(len(input_ids)):
        if input_ids[idx] == instruction_part:
            response_start_token_idx = idx + 1
            break
    
    if response_start_token_idx is None:
        return torch.full_like(labels, -100)
    
    masked_labels = labels.clone()
    masked_labels[:response_start_token_idx] = -100
    return masked_labels


def compute_sft_loss(model, tokenizer, dataset_path: str, max_samples: int = 100, seed: int = 42) -> float:
    """Compute SFT training loss on responses only."""
    dataloader = prepare_dataset(dataset_path, tokenizer, max_samples, seed)
    instruction_part, response_part = get_instruct_response_part(tokenizer)
    
    total_loss = 0
    num_batches = 0
    
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            
            input_ids = batch['input_ids']
            labels = input_ids.clone()
            
            batch_size = input_ids.size(0)
            for i in range(batch_size):
                labels[i] = mask_labels_for_response_only(
                    input_ids[i], labels[i], tokenizer, instruction_part, response_part
                )
            
            batch['labels'] = labels
            outputs = model(**batch)
            loss = outputs.loss
            
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / num_batches if num_batches > 0 else 0.0


def compute_kl_divergence_fast(model, base_model, tokenizer, dataset, max_samples: int = 50, seed: int = 42) -> float:
    """
    Fast KL divergence computation on a pre-processed dataset object.
    """
    from torch.utils.data import DataLoader
    from transformers import DataCollatorForSeq2Seq

    # We already have a dataset object, just need to prepare it for KL comp
    tokenized_dataset = dataset

    # Use data collator
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)
    dataloader = DataLoader(
        tokenized_dataset,
        batch_size=2,  # Keep batch size small
        collate_fn=data_collator,
        shuffle=False
    )
    
    total_kl = 0
    num_batches = 0
    
    model.eval()
    base_model.eval()
    
    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            
            try:
                model_outputs = model(**batch)
                base_outputs = base_model(**batch)
                
                model_logits = model_outputs.logits
                base_logits = base_outputs.logits
                
                min_seq_len = min(model_logits.size(1), base_logits.size(1))
                model_logits = model_logits[:, :min_seq_len, :]
                base_logits = base_logits[:, :min_seq_len, :]
                
                base_log_probs = F.log_softmax(base_logits, dim=-1)
                model_log_probs = F.log_softmax(model_logits, dim=-1)
                
                base_probs = torch.exp(base_log_probs)
                kl_div = torch.sum(base_probs * (base_log_probs - model_log_probs), dim=-1)
                
                if 'attention_mask' in batch:
                    attention_mask = batch['attention_mask'][:, :min_seq_len]
                    kl_div = kl_div * attention_mask
                    valid_tokens = attention_mask.sum()
                else:
                    valid_tokens = kl_div.numel()
                
                if valid_tokens > 0:
                    kl_loss = kl_div.sum() / valid_tokens
                    if torch.isfinite(kl_loss) and not torch.isnan(kl_loss):
                        total_kl += kl_loss.item()
                        num_batches += 1
                
                del model_outputs, base_outputs, model_logits, base_logits, kl_div
                
            except Exception as e:
                print(f"  KL batch error (LinAlgError): {e}")
                continue
            except Exception as e:
                print(f"  KL batch error: {e}")
                continue
            
            if num_batches % 10 == 0:
                clear_gpu_memory()
    
    return total_kl / num_batches if num_batches > 0 else 0.0


def generate_and_prepare_dataset(base_model, tokenizer, dataset_path: str, 
                                 max_samples: int = 100, seed: int = 42, 
                                 max_new_tokens: int = 100) -> Dataset:
    """
    Generates responses from a base model and prepares a new dataset for KL computation.
    If max_samples is greater than the number of unique questions, it samples with replacement.
    """
    from datasets import Dataset
    import random

    # Load ALL original questions from the file first
    all_questions = load_dataset(dataset_path, max_samples=None, seed=seed)

    if not all_questions:
        return Dataset.from_list([])

    # Now, create the list of prompts to generate from
    prompts_to_generate = []
    if max_samples > len(all_questions):
        print(f"  INFO: Requested {max_samples} samples, but only {len(all_questions)} unique questions found. Sampling with replacement.")
        random.seed(seed)
        prompts_to_generate = random.choices(all_questions, k=max_samples)
    else:
        # If we have enough unique questions, sample without replacement.
        random.seed(seed)
        prompts_to_generate = random.sample(all_questions, max_samples)
    
    new_dataset_items = []
    base_model.eval()

    print(f"Generating {len(prompts_to_generate)} responses from base model...")
    with torch.no_grad():
        for item in prompts_to_generate:
            user_messages = [msg for msg in item['messages'] if msg['role'] == 'user']
            if not user_messages:
                continue

            prompt = tokenizer.apply_chat_template(
                user_messages,
                add_generation_prompt=True,
                tokenize=False
            )
            
            inputs = tokenizer(prompt, return_tensors="pt").to(base_model.device)
            
            generated_ids = base_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                eos_token_id=tokenizer.eos_token_id
            )
            
            full_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)
            
            new_item = {
                "text": full_text
            }
            new_dataset_items.append(new_item)

    generated_dataset = Dataset.from_list(new_dataset_items)

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=1024,
            return_tensors=None,
        )
    
    tokenized_dataset = generated_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    return tokenized_dataset


def compute_kl_divergence(model, base_model, tokenizer, dataset_path: str, 
                         max_samples: int = 10, seed: int = 42, max_new_tokens: int = 50) -> float:
    """
    Orchestrates memory-efficient KL divergence calculation.

    """
    print("Step 1: Generating responses from base model...")
    generated_dataset = generate_and_prepare_dataset(
        base_model, tokenizer, dataset_path, max_samples, seed, max_new_tokens
    )
    
    print("Step 2: Computing KL divergence on generated responses...")
    kl_result = compute_kl_divergence_fast(
        model, base_model, tokenizer, generated_dataset, max_samples, seed
    )
    
    return kl_result


def get_logits_from_model(model, tokenizer, dataset: Dataset) -> List[torch.Tensor]:
    """Get all logits from a model for a given dataset."""
    from torch.utils.data import DataLoader
    from transformers import DataCollatorForSeq2Seq

    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)
    dataloader = DataLoader(dataset, batch_size=2, collate_fn=data_collator, shuffle=False)
    
    all_logits = []
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            outputs = model(**batch)
            all_logits.append(outputs.logits.cpu())
            
    return all_logits


def compute_kl_from_logits(logits1: List[torch.Tensor], logits2: List[torch.Tensor]) -> float:
    """Compute KL divergence from two lists of logit tensors."""
    total_kl = 0
    total_tokens = 0

    for batch_logits1, batch_logits2 in zip(logits1, logits2):
        # Ensure same sequence length
        min_seq_len = min(batch_logits1.size(1), batch_logits2.size(1))
        
        log_probs1 = F.log_softmax(batch_logits1[:, :min_seq_len, :], dim=-1)
        log_probs2 = F.log_softmax(batch_logits2[:, :min_seq_len, :], dim=-1)
        
        probs2 = torch.exp(log_probs2)
        
        # KL(P2 || P1)
        kl_div_batch = torch.sum(probs2 * (log_probs2 - log_probs1), dim=-1)
        
        # For simplicity, we assume no padding or use an attention mask if available from dataset
        # In this implementation, we average over all tokens.
        total_kl += kl_div_batch.sum().item()
        total_tokens += kl_div_batch.numel()

    return total_kl / total_tokens if total_tokens > 0 else 0.0


# =============================================================================
# VISUALIZATION
# =============================================================================

# Professional color palette for plots
PLOT_COLORS = [
   "#000000",  # Black
   "#D4876A",  # Coral/Terra Cotta
   "#7BA7D7",  # Sky Blue
   "#7D9B7D",  # Olive Green
   "#C17B8D",  # Dusty Rose/Pink
   "#B8CCC8",  # Sage Green
   "#D4D0E5",  # Soft Lavender
   "#F4EFEA",  # Warm Beige
]

def setup_plot_style():
    """Sets a professional and aesthetic style for the plots."""
    sns.set_theme(
        context='talk',
        style='whitegrid',
        palette='deep',
        font='sans-serif',
        font_scale=0.9,
    )
    plt.rcParams['figure.figsize'] = (12, 8)
    plt.rcParams['figure.dpi'] = 150
    plt.rcParams['axes.titlesize'] = 20
    plt.rcParams['axes.labelsize'] = 18
    plt.rcParams['xtick.labelsize'] = 18
    plt.rcParams['ytick.labelsize'] = 18
    plt.rcParams['legend.fontsize'] = 18


def create_heatmap(data: np.ndarray, x_labels: List[str], y_labels: List[str], 
                  title: str, xlabel: str = "", ylabel: str = "", 
                  figsize: Tuple[int, int] = (10, 8)) -> Figure:
    """Create a heatmap visualization."""
    fig, ax = plt.subplots(figsize=figsize)
    
    sns.heatmap(data, 
                xticklabels=x_labels,
                yticklabels=y_labels,
                annot=True,
                fmt='.3f',
                cmap='viridis',
                ax=ax)
    
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    
    return fig


def create_comparison_plot(data1: np.ndarray, data2: np.ndarray, 
                          labels: List[str], title: str) -> Figure:
    """Create a comparison plot between two datasets."""
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
    
    # Plot 1: Data 1
    sns.heatmap(data1, xticklabels=labels, yticklabels=labels, 
                annot=True, fmt='.3f', cmap='viridis', ax=ax1)
    ax1.set_title('Adapter 1')
    
    # Plot 2: Data 2
    sns.heatmap(data2, xticklabels=labels, yticklabels=labels, 
                annot=True, fmt='.3f', cmap='viridis', ax=ax2)
    ax2.set_title('Adapter 2')
    
    # Plot 3: Difference
    diff = data2 - data1
    sns.heatmap(diff, xticklabels=labels, yticklabels=labels, 
                annot=True, fmt='.3f', cmap='RdBu_r', center=0, ax=ax3)
    ax3.set_title('Difference (2 - 1)')
    
    fig.suptitle(title, fontsize=16)
    plt.tight_layout()
    return fig


def create_same_norm_comparison_plot(norm_percentages: List[float], target_norms: List[float],
                                    adapter1_losses: List[float], adapter2_losses: List[float],
                                    loss_differences: List[float], mean_orig_norm: float, title: str) -> Figure:
    """Create a plot showing loss differences when both adapters have the same norm."""
    setup_plot_style()
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # Use Sky Blue and Coral from the defined color scheme
    color1 = PLOT_COLORS[2]  # Sky Blue (#7BA7D7)
    color2 = PLOT_COLORS[1]  # Coral/Terra Cotta (#D4876A)

    # Top plot: Individual losses
    ax1.plot(norm_percentages, adapter1_losses, 'o-', label='Adapter 1', linewidth=2, markersize=8, alpha=0.9, color=color1)
    ax1.plot(norm_percentages, adapter2_losses, 's-', label='Adapter 2', linewidth=2, markersize=8, alpha=0.9, color=color2)
    ax1.axvline(x=1.0, color='gray', linestyle='--', alpha=0.7, label=f'Original mean norm ({mean_orig_norm:.3f})')
    ax1.set_xlabel('Norm (as % of mean original norm)')
    ax1.set_ylabel('SFT Loss (Response Only)')
    ax1.set_title('Individual Adapter Losses at Same Norms')
    ax1.legend()
    ax1.grid(True, axis='y', which='major', linestyle='--', linewidth=0.5)
    
    # Bottom plot: Loss differences (Adapter2 - Adapter1)
    bar_colors = [PLOT_COLORS[1] if diff > 0 else PLOT_COLORS[2] for diff in loss_differences]  # Coral if positive, Sky Blue if negative
    bars = ax2.bar(norm_percentages, loss_differences, color=bar_colors, alpha=0.8, width=0.08)
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.8)
    ax2.axvline(x=1.0, color='gray', linestyle='--', alpha=0.7)
    ax2.set_xlabel('Norm (as % of mean original norm)')
    ax2.set_ylabel('Loss Difference (Adapter2 - Adapter1)')
    ax2.set_title('Loss Difference at Same Norms')
    ax2.grid(True, axis='y', which='major', linestyle='--', linewidth=0.5)
    
    # Add value labels on bars
    for bar, diff in zip(bars, loss_differences):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + (0.001 if height >= 0 else -0.001),
                f'{diff:.3f}', ha='center', va='bottom' if height >= 0 else 'top', fontsize=10)
    
    # Overall title
    fig.suptitle(title, fontsize=16, y=0.95)
    plt.tight_layout(rect=[0, 0, 1, 0.92])
    return fig


def create_norm_rescaling_comparison_plot(data1: np.ndarray, data2: np.ndarray, labels: list, orig_norm1: float, orig_norm2: float, title: str) -> Figure:
    """Create a stacked heatmap comparison for norm rescaling experiment."""
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np

    # Stack the two rows
    data = np.vstack([data1, data2])
    mean_val = np.mean(data)
    vmin = np.min(data)
    vmax = np.max(data)
    center = mean_val

    fig, ax = plt.subplots(figsize=(len(labels) * 1.2, 3.5))
    sns.heatmap(
        data,
        xticklabels=labels,
        yticklabels=[f"Adapter 1 (orig norm: {orig_norm1:.3f})", f"Adapter 2 (orig norm: {orig_norm2:.3f})"],
        annot=True,
        fmt=".3f",
        cmap="RdBu_r",
        center=center,
        vmin=vmin,
        vmax=vmax,
        cbar=True,
        ax=ax
    )
    ax.set_xlabel("Scale Factor")
    ax.set_ylabel("")
    ax.set_title(title, fontsize=14, pad=20)
    plt.tight_layout()
    return fig


def save_plot(fig: Figure, filename: str, subdir: str = ""):
    """Save figure to the stability results directory with consistent settings."""
    # Get the stability directory path
    stability_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = os.path.join(stability_dir, "results")
    
    # Add subdirectory if specified
    if subdir:
        results_dir = os.path.join(results_dir, subdir)
    
    # Create the full filepath
    filepath = os.path.join(results_dir, filename)
    
    # Ensure directory exists
    os.makedirs(results_dir, exist_ok=True)
    
    # Save the figure
    fig.savefig(filepath, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Plot saved to {filepath}")


def create_experiment_subdir(adapter1_path: str, adapter2_path: str, experiment_name: str) -> str:
    """Create a unique subdirectory name for this experiment run."""
    # Extract adapter names from paths
    adapter1_name = os.path.basename(adapter1_path.replace('/', '_'))
    adapter2_name = os.path.basename(adapter2_path.replace('/', '_'))
    
    # Create timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create subdirectory name
    subdir = f"{experiment_name}_{adapter1_name}_vs_{adapter2_name}_{timestamp}"
    
    return subdir


def create_plot_title(base_title: str, adapter1_path: str, adapter2_path: str, 
                     dataset_info: str = "") -> str:
    """Create a comprehensive plot title with adapter and dataset information."""
    # Extract adapter names
    adapter1_name = os.path.basename(adapter1_path)
    adapter2_name = os.path.basename(adapter2_path)
    
    # Build title
    title = f"{base_title}\n"
    title += f"Adapter 1: {adapter1_name}\n"
    title += f"Adapter 2: {adapter2_name}"
    
    if dataset_info:
        title += f"\nDataset: {dataset_info}"
    
    return title


# =============================================================================
# EXPERIMENT UTILITIES
# =============================================================================

def create_argparser(description: str) -> argparse.ArgumentParser:
    """Create argument parser with common arguments."""
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('--adapter1_path', type=str, required=True, help='Path to first adapter')
    parser.add_argument('--adapter2_path', type=str, required=True, help='Path to second adapter')
    
    # Support both single and multiple datasets
    dataset_group = parser.add_mutually_exclusive_group(required=True)
    dataset_group.add_argument('--dataset_path', type=str, help='Path to single dataset')
    dataset_group.add_argument('--dataset_paths', type=str, nargs='+', help='Paths to multiple datasets')
    
    parser.add_argument('--output_file', type=str, default='results.json', help='Output file')
    parser.add_argument('--max_samples', type=int, default=100, help='Max samples to process (total across all datasets)')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    return parser


def set_random_seed(seed: int):
    """Set random seed for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def save_results(results: Dict[str, Any], output_path: str):
    """Save experiment results to JSON file."""
    results['timestamp'] = datetime.now().isoformat()
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"Results saved to {output_path}")


def save_results_with_plots(results: Dict[str, Any], filename: str, subdir: str = ""):
    """Save experiment results to the same directory as plots."""
    # Get the stability directory path
    stability_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = os.path.join(stability_dir, "results")
    
    # Add subdirectory if specified
    if subdir:
        results_dir = os.path.join(results_dir, subdir)
    
    # Create the full filepath
    filepath = os.path.join(results_dir, filename)
    
    # Ensure directory exists
    os.makedirs(results_dir, exist_ok=True)
    
    # Save results
    results['timestamp'] = datetime.now().isoformat()
    with open(filepath, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"Results saved to {filepath}")


def print_summary(experiment_name: str, results: Dict[str, Any]):
    """Print experiment summary."""
    print(f"\n{'='*60}")
    print(f"Experiment: {experiment_name}")
    print(f"{'='*60}")
    
    for key, value in results.items():
        if isinstance(value, float):
            print(f"{key}: {value:.10f}")
        elif isinstance(value, dict):
            print(f"\n{key}:")
            for sub_key, sub_value in value.items():
                if isinstance(sub_value, float):
                    print(f"  {sub_key}: {sub_value:.10f}")
                else:
                    print(f"  {sub_key}: {sub_value}")
        else:
            print(f"{key}: {value}")
    
    print(f"{'='*60}\n")


class ExperimentTracker:
    """Track experiment progress and timing."""
    
    def __init__(self, experiment_name: str, total_steps: Optional[int] = None):
        self.experiment_name = experiment_name
        self.total_steps = total_steps
        self.current_step = 0
        self.start_time = datetime.now()
        
    def update(self, step_description: str = ""):
        """Update progress."""
        self.current_step += 1
        elapsed = (datetime.now() - self.start_time).total_seconds()
        
        if self.total_steps:
            progress = self.current_step / self.total_steps * 100
            eta = (elapsed / self.current_step) * (self.total_steps - self.current_step)
            print(f"[{self.experiment_name}] Step {self.current_step}/{self.total_steps} "
                  f"({progress:.1f}%) - {step_description} - ETA: {eta:.1f}s")
        else:
            print(f"[{self.experiment_name}] Step {self.current_step} - {step_description}")
    
    def finish(self):
        """Finish tracking."""
        total_time = (datetime.now() - self.start_time).total_seconds()
        print(f"\n[{self.experiment_name}] Completed in {total_time:.1f}s")