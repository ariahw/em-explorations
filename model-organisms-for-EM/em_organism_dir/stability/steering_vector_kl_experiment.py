#!/usr/bin/env python3
"""
Steering vector KL divergence experiment.
Analyzes how KL divergence from base model changes as steering vector norm varies.
"""

import os
import json
import tempfile
import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Tuple
import argparse
from copy import deepcopy

from em_organism_dir.stability.steering_vec_util import SteeringVectorModel
from datasets import load_dataset
from huggingface_hub import hf_hub_download
from em_organism_dir.stability.utils import (
    set_random_seed,
    ExperimentTracker,
    create_experiment_subdir,
    save_plot,
    save_results_with_plots,
    print_summary,
    setup_plot_style,
    PLOT_COLORS,
    load_lora_adapter,
    safe_model_cleanup,
    generate_and_prepare_dataset,
    get_logits_from_model,
    compute_kl_from_logits
)


def download_steering_vector(repo_id: str, token: str = None):
    """Download steering vector with fallback to checkpoints/final path."""
    for filename in ["checkpoints/final/steering_vector.pt", "steering_vector.pt"]:
        try:
            return hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                token=token,
            )
        except:
            continue
    raise ValueError(f"Could not find steering vector in {repo_id}")


def load_fineweb_data(max_samples: int = 100, seed: int = 42):
    """Load a random subset of fineweb dataset and prepare it for direct KL calculation."""
    set_random_seed(seed)
    
    # Load fineweb dataset (streaming to avoid downloading full dataset)
    dataset = load_dataset("HuggingFaceFW/fineweb", "sample-10BT", split="train", streaming=True)
    
    # Take random samples
    samples = []
    dataset = dataset.shuffle(seed=seed)
    
    for i, sample in enumerate(dataset):
        if i >= max_samples:
            break
        # Extract text content and use it directly for KL calculation
        text = sample.get('text', '')
        if text and len(text) > 100:  # Filter out very short texts
            # Truncate to reasonable length for processing
            truncated_text = text[:2000]  # Longer text for better KL calculation
            samples.append({'text': truncated_text})
    
    print(f"Loaded {len(samples)} fineweb samples")
    return samples


def prepare_dataset_for_kl(samples, tokenizer, max_length: int = 512):
    """Prepare dataset samples for direct KL divergence calculation."""
    from datasets import Dataset
    
    # Tokenize the texts
    tokenized_samples = []
    for sample in samples:
        text = sample['text']
        # Tokenize and truncate
        tokens = tokenizer(text, 
                          truncation=True, 
                          max_length=max_length, 
                          padding=False, 
                          return_tensors="pt")
        
        if tokens['input_ids'].size(1) > 10:  # Ensure reasonable length
            tokenized_samples.append({
                'input_ids': tokens['input_ids'][0],
                'attention_mask': tokens['attention_mask'][0]
            })
    
    return Dataset.from_list(tokenized_samples)


def generate_random_steering_vectors(hidden_dim: int, num_vectors: int = 5, seed: int = 42) -> List[torch.Tensor]:
    """Generate random steering vectors with specified hidden dimension."""
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # Generate random vectors
    random_vectors = []
    for i in range(num_vectors):
        # Generate random vector with unit normal distribution
        vec = torch.randn(hidden_dim)
        # Normalize to unit norm initially
        vec = vec / torch.norm(vec)
        random_vectors.append(vec)
    
    return random_vectors


def compute_kl_with_steering(base_model, tokenizer, generated_dataset, steering_vector, layer_idx, alpha, device, dtype):
    """Compute KL divergence with steering vector applied via hooks."""
    # Apply steering hook
    def steering_hook(module, input, output):
        hidden_states = output[0]
        batch_size, seq_len, hidden_dim = hidden_states.shape
        steering_broadcast = steering_vector.unsqueeze(0).unsqueeze(0).expand(batch_size, seq_len, -1)
        modified_hidden = hidden_states + alpha * steering_broadcast
        return (modified_hidden,) + output[1:]
    
    # Register hook
    layer = base_model.model.layers[layer_idx]
    hook_handle = layer.register_forward_hook(steering_hook)
    
    # Get logits with steering applied
    steered_logits = get_logits_from_model(base_model, tokenizer, generated_dataset)
    
    # Remove hook
    hook_handle.remove()
    
    return steered_logits


def steering_vector_kl_experiment(
    narrow_vector_paths: List[str],
    general_vector_paths: List[str],
    narrow_vector_names: List[str] = None,
    general_vector_names: List[str] = None,
    base_model_id: str = "unsloth/Qwen2.5-14B-Instruct",
    dataset_path: str = None,
    output_file: str = "steering_kl_results.json",
    max_samples: int = 100,
    seed: int = 42,
    max_length: int = 512,
    norm_values: List[float] = None,
    use_fineweb: bool = True,
    num_random_vectors: int = 5,
    fast_mode: bool = False
):
    """
    Analyze KL divergence from base model as steering vector norm varies.
    
    Args:
        narrow_vector_paths: List of HuggingFace Hub paths to narrow steering vectors (up to 3)
        general_vector_paths: List of HuggingFace Hub paths to general steering vectors (up to 3)
        narrow_vector_names: Optional list of names for narrow vectors (defaults to "Narrow 1", "Narrow 2", etc.)
        general_vector_names: Optional list of names for general vectors (defaults to "General 1", "General 2", etc.)
        base_model_id: Base model ID (e.g. "unsloth/Qwen2.5-14B-Instruct")
        dataset_path: Path to dataset for KL calculation (if None, uses fineweb)
        output_file: Output file name
        max_samples: Number of samples to use from dataset
        seed: Random seed
        max_length: Maximum sequence length for tokenization
        norm_values: List of norm values to test (default: 0.0 to 5.0 in 0.2 increments)
        use_fineweb: Whether to use fineweb dataset (default: True)
        num_random_vectors: Number of random vectors to generate (default: 5)
        fast_mode: Use faster settings for quick testing (default: False)
    """
    set_random_seed(seed)
    
    # Validate inputs
    if len(narrow_vector_paths) > 3:
        raise ValueError("Maximum 3 narrow vectors supported")
    if len(general_vector_paths) > 3:
        raise ValueError("Maximum 3 general vectors supported")
    
    # Set default names if not provided
    if narrow_vector_names is None:
        narrow_vector_names = [f"Narrow {i+1}" for i in range(len(narrow_vector_paths))]
    if general_vector_names is None:
        general_vector_names = [f"General {i+1}" for i in range(len(general_vector_paths))]
    
    if len(narrow_vector_names) != len(narrow_vector_paths):
        raise ValueError("Number of narrow vector names must match number of narrow vector paths")
    if len(general_vector_names) != len(general_vector_paths):
        raise ValueError("Number of general vector names must match number of general vector paths")
    
    # Apply fast mode optimizations
    if fast_mode:
        if max_samples > 50:
            max_samples = 20
            print(f"ðŸš€ Fast mode: reducing samples to {max_samples}")
        if max_length > 300:
            max_length = 256
            print(f"ðŸš€ Fast mode: reducing max_length to {max_length}")
        if num_random_vectors > 5:
            num_random_vectors = 5
            print(f"ðŸš€ Fast mode: reducing random vectors to {num_random_vectors}")
    
    if norm_values is None:
        if fast_mode:
            norm_values = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 4.0, 5.0]  # 9 points for speed
            print(f"ðŸš€ Fast mode: using {len(norm_values)} norm values")
        else:
            norm_values = [0.2 * i for i in range(0, 26)]  # 0.0, 0.2, 0.4, ..., 5.0
    
    total_vectors = len(narrow_vector_paths) + len(general_vector_paths) + num_random_vectors
    # Initialize tracker
    num_steps = 4 + len(norm_values) * total_vectors
    mode_name = "Fast Steering Vector KL Divergence" if fast_mode else "Steering Vector KL Divergence"
    tracker = ExperimentTracker(mode_name, num_steps)
    
    # Load or create dataset
    if dataset_path is None and use_fineweb:
        tracker.update("Loading fineweb dataset")
        raw_samples = load_fineweb_data(max_samples, seed)
        dataset_name = "fineweb"
        cleanup_dataset = False
    else:
        # For file-based datasets, load them here
        # This would need to be implemented based on the file format
        raise NotImplementedError("File-based datasets not yet implemented - use fineweb for now")
    
    # Step 1: Load base model and prepare dataset
    tracker.update("Loading base model")
    base_model, tokenizer = load_lora_adapter(base_model_id, base_model_name=base_model_id)
    device = base_model.device
    dtype = base_model.dtype
    hidden_dim = base_model.config.hidden_size
    
    tracker.update(f"Preparing tokenized dataset from {dataset_name}")
    prepared_dataset = prepare_dataset_for_kl(raw_samples, tokenizer, max_length)
    print(f"Prepared {len(prepared_dataset)} tokenized samples")
    
    # Get base model logits for KL calculation
    tracker.update("Computing base model logits")
    base_logits = get_logits_from_model(base_model, tokenizer, prepared_dataset)
    
    # Step 2: Load steering vectors directly from HuggingFace without loading models
    tracker.update("Loading steering vectors and generating random vectors")
    
    # Load steering vectors directly from files without instantiating SteeringVectorModel
    from huggingface_hub import hf_hub_download
    import os as hf_os
    
    # Load narrow steering vectors
    narrow_vectors = []
    layer_idx = None
    alpha = 1.0
    
    for path in narrow_vector_paths:
        narrow_file = download_steering_vector(
            repo_id=path,
            token=os.getenv("HF_TOKEN"),
        )
        narrow_data = torch.load(narrow_file)
        narrow_vec = narrow_data["steering_vector"].to(device).to(dtype)
        narrow_vectors.append(narrow_vec)
        
        # Use layer_idx and alpha from first vector (assume all vectors use same layer)
        if layer_idx is None:
            layer_idx = narrow_data["layer_idx"]
            alpha = narrow_data.get("alpha", 1.0)
    
    # Load general steering vectors
    general_vectors = []
    for path in general_vector_paths:
        general_file = download_steering_vector(
            repo_id=path,
            token=os.getenv("HF_TOKEN"),
        )
        general_data = torch.load(general_file)
        general_vec = general_data["steering_vector"].to(device).to(dtype)
        general_vectors.append(general_vec)
    
    # Generate random vectors
    random_vectors = generate_random_steering_vectors(hidden_dim, num_vectors=num_random_vectors, seed=seed)
    
    # Collect all vectors with labels
    all_vectors = []
    
    # Add narrow vectors
    for name, vec in zip(narrow_vector_names, narrow_vectors):
        all_vectors.append((name, vec))
    
    # Add general vectors
    for name, vec in zip(general_vector_names, general_vectors):
        all_vectors.append((name, vec))
    
    # Add random vectors
    for i, vec in enumerate(random_vectors):
        all_vectors.append((f"Random {i+1}", vec))
    
    # Step 3: Test each vector at different norms
    results = {}
    
    for vector_name, steering_vector in all_vectors:
        print(f"\nTesting {vector_name} vector...")
        results[vector_name] = {
            'norms': norm_values,
            'kl_divergences': []
        }
        
        # Move vector to device
        steering_vector = steering_vector.to(device).to(dtype)
        
        # Test each norm value
        for norm in norm_values:
            tracker.update(f"Testing {vector_name} at norm {norm:.1f}")
            
            # Rescale vector to target norm
            current_norm = torch.norm(steering_vector).item()
            if current_norm > 0:
                scaled_vector = steering_vector * (norm / current_norm)
            else:
                scaled_vector = steering_vector
            
            # Get logits with steering applied
            steered_logits = compute_kl_with_steering(
                base_model, tokenizer, prepared_dataset, 
                scaled_vector, layer_idx, alpha, device, dtype
            )
            
            # Compute KL divergence
            kl_div = compute_kl_from_logits(steered_logits, base_logits)
            results[vector_name]['kl_divergences'].append(kl_div)
            
            print(f"  Norm {norm:.1f}: KL = {kl_div:.6f}")
    
    # Cleanup models
    safe_model_cleanup(base_model, tokenizer)
    
    # Step 4: Create visualization
    tracker.update("Creating visualization")
    setup_plot_style()
    
    fig, ax = plt.subplots(figsize=(12, 8))

    # Use the PLOT_COLORS from utils.py for consistency
    # Define colors for different vector types using the same scheme
    narrow_colors = [PLOT_COLORS[1], PLOT_COLORS[4], "#b8a8d4"]   # Coral, Dusty Rose, Muted Purple
    general_colors = [PLOT_COLORS[2], PLOT_COLORS[3], "#a8c4b8"]  # Sky Blue, Olive Green, Distinct Muted Green
    
    for i, (vector_name, _) in enumerate(all_vectors):
        if vector_name.startswith("Random"):
            # All random vectors: lighter gray with reduced alpha
            color = '#b0b0b0'  # Lighter gray
            alpha = 0.4
            # Only label the first random vector to avoid legend clutter
            label = "Random Vectors" if vector_name == "Random 1" else ""
        elif vector_name in narrow_vector_names:
            # Narrow vectors: different shades of blue
            idx = narrow_vector_names.index(vector_name)
            color = narrow_colors[idx % len(narrow_colors)]
            alpha = 1.0
            label = vector_name
        elif vector_name in general_vector_names:
            # General vectors: different shades of orange
            idx = general_vector_names.index(vector_name)
            color = general_colors[idx % len(general_colors)]
            alpha = 1.0
            label = vector_name
        else:
            # Fallback
            color = 'black'
            alpha = 1.0
            label = vector_name
        
        ax.plot(results[vector_name]['norms'], 
                results[vector_name]['kl_divergences'],
                'o-', 
                label=label,
                color=color,
                alpha=alpha,
                markersize=8,
                linewidth=2)
    
    ax.set_xlabel("Steering Vector Norm", fontsize=18)
    ax.set_ylabel("KL Divergence from Base Model", fontsize=18)
    ax.set_title("KL Divergence vs Steering Vector Norm\n" +
                 f"Dataset: {dataset_name}", fontsize=20, pad=20)
    ax.legend(loc='best', fontsize=18)
    ax.grid(True, axis='y', which='major', linestyle='--', linewidth=0.5)
    ax.set_xlim(0, 5.1)
    ax.set_ylim(bottom=0)
    
    plt.tight_layout()
    
    # Save plot  
    subdir = create_experiment_subdir(narrow_vector_paths[0] if narrow_vector_paths else "no_narrow", 
                                     general_vector_paths[0] if general_vector_paths else "no_general", 
                                     "steering_kl_divergence")
    save_plot(fig, "steering_kl_divergence_plot.png", subdir)
    
    # Save results
    experiment_results = {
        'experiment': 'steering_vector_kl_divergence',
        'results': results,
        'config': {
            'narrow_vector_paths': narrow_vector_paths,
            'general_vector_paths': general_vector_paths,
            'narrow_vector_names': narrow_vector_names,
            'general_vector_names': general_vector_names,
            'base_model_id': base_model_id,
            'dataset_name': dataset_name,
            'max_samples': max_samples,
            'seed': seed,
            'max_length': max_length,
            'norm_values': norm_values,
            'layer_idx': layer_idx,
            'alpha': alpha,
            'num_random_vectors': num_random_vectors,
            'fast_mode': fast_mode
        }
    }
    
    save_results_with_plots(experiment_results, output_file, subdir)
    print_summary("Steering Vector KL Divergence", experiment_results)
    
    tracker.finish()
    return experiment_results


def main():
    parser = argparse.ArgumentParser(description="Steering vector KL divergence experiment")
    parser.add_argument('--narrow_vector_paths', nargs='+', default=[],
                        help='HuggingFace Hub paths to narrow steering vectors (up to 3)')
    parser.add_argument('--general_vector_paths', nargs='+', default=[],
                        help='HuggingFace Hub paths to general steering vectors (up to 3)')
    parser.add_argument('--narrow_vector_names', nargs='+', default=None,
                        help='Names for narrow vectors (optional)')
    parser.add_argument('--general_vector_names', nargs='+', default=None,
                        help='Names for general vectors (optional)')
    parser.add_argument('--base_model_id', type=str, default="unsloth/Qwen2.5-14B-Instruct",
                        help='Base model ID')
    parser.add_argument('--dataset_path', type=str, default=None,
                        help='Path to dataset (if not provided, uses fineweb)')
    parser.add_argument('--output_file', type=str, default='steering_kl_results.json',
                        help='Output file name')
    parser.add_argument('--max_samples', type=int, default=100,
                        help='Maximum number of samples to use')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')
    parser.add_argument('--max_length', type=int, default=512,
                        help='Maximum sequence length for tokenization')
    parser.add_argument('--num_random_vectors', type=int, default=5,
                        help='Number of random vectors to generate')
    parser.add_argument('--fast', dest='fast_mode', action='store_true',
                        help='Enable fast mode for quicker testing')
    parser.add_argument('--no-fineweb', dest='use_fineweb', action='store_false',
                        help='Disable fineweb dataset (must provide --dataset_path)')
    
    args = parser.parse_args()
    
    if not args.narrow_vector_paths and not args.general_vector_paths:
        parser.error("Must provide at least one narrow or general vector path")
    
    steering_vector_kl_experiment(
        args.narrow_vector_paths,
        args.general_vector_paths,
        args.narrow_vector_names,
        args.general_vector_names,
        args.base_model_id,
        args.dataset_path,
        args.output_file,
        args.max_samples,
        args.seed,
        args.max_length,
        use_fineweb=args.use_fineweb,
        num_random_vectors=args.num_random_vectors,
        fast_mode=args.fast_mode
    )


if __name__ == "__main__":
    main()