import os
import time
from typing import Any, Dict, List, Optional

import torch
from huggingface_hub import hf_hub_download
from tqdm import tqdm

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.modeling_utils import PreTrainedModel
from transformers.tokenization_utils import PreTrainedTokenizer

from em_organism_dir.stability.utils import prepare_dataset, get_instruct_response_part, mask_labels_for_response_only


class SteeringVectorModel:
    """Class to handle loading and generation with steering vectors.
    
    The effective scaling applied to the steering vector during inference is:
    effective_scaling = alpha * global_multiplier
    
    - alpha: Controls the base strength of the steering vector (from training or override)
    - global_multiplier: Runtime scaling factor applied on top of alpha
    """

    def __init__(
        self,
        checkpoint_number: Optional[int] = None,
        base_model_id: Optional[str] = None,
        ft_model_id: Optional[str] = None,
        steering_vector_path: Optional[str] = None,
        global_multiplier: float = 1.0,
        alpha: Optional[float] = None,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ):
        """Initialize model with steering vector.

        Args:
            checkpoint_number: Checkpoint number to load (if provided, constructs path automatically)
            base_model_id: Hugging Face model ID for the base model
            ft_model_id: Hugging Face model ID for the finetuned model (contains steering vectors)
            steering_vector_path: Path to the saved steering vector checkpoint (used if checkpoint_number not provided)
            global_multiplier: Multiplier for the steering vector strength
            alpha: Override alpha value (if None, uses saved alpha from checkpoint)
            device: Device to run on
            dtype: Data type to use
        """
        # Store initialization parameters
        self.checkpoint_number = checkpoint_number
        self.ft_model_id = ft_model_id
        self.base_model_id = base_model_id
        self._override_alpha = alpha  # Store user-provided alpha override

        if base_model_id is None:
            raise ValueError("base_model_id must be provided")
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if dtype is None:
            dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32

        self.device = device
        self.dtype = dtype
        self.global_multiplier = global_multiplier

        # Load base model
        self.model, self.tokenizer = load_model_and_tokenizer(base_model_id, device=self.device, dtype=self.dtype)

        # Configure tokenizer for generation (left-padding for decoder-only models)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "left"  # Required for generation

        # Load steering vector
        if checkpoint_number is not None and ft_model_id is not None:
            # Load directly from HuggingFace
            if checkpoint_number == -1:
                # Load final weights - try checkpoints/final first, then root
                steering_vector_file = None
                for filename in ["checkpoints/final/steering_vector.pt", "steering_vector.pt"]:
                    try:
                        print(f"Attempting to download: {ft_model_id}/{filename}")
                        steering_vector_file = hf_hub_download(
                            repo_id=ft_model_id,
                            filename=filename,
                            token=os.getenv("HF_TOKEN"),
                        )
                        print(f"Successfully downloaded steering vector from: {filename}")
                        break
                    except Exception as e:
                        print(f"Failed to download {filename}: {e}")
                        continue
                if steering_vector_file is None:
                    raise ValueError(f"Could not find steering vector in {ft_model_id}. Tried: checkpoints/final/steering_vector.pt and steering_vector.pt")
            else:
                # Load specific checkpoint
                steering_vector_file = hf_hub_download(
                    repo_id=ft_model_id,
                    filename=f"checkpoints/checkpoint-{checkpoint_number}/steering_vector.pt",
                    token=os.getenv("HF_TOKEN"),
                )
            steering_data = torch.load(steering_vector_file)
            self.steering_vector = steering_data["steering_vector"]
            self.layer_idx = steering_data["layer_idx"]
            # Use override alpha if provided, otherwise use saved alpha
            self.alpha = self._override_alpha if self._override_alpha is not None else steering_data.get("alpha", 1.0)
        elif steering_vector_path is not None:
            # Load from local path - try checkpoints/final first, then root
            steering_file = None
            for subpath in ["checkpoints/final/steering_vector.pt", "steering_vector.pt"]:
                full_path = os.path.join(steering_vector_path, subpath)
                if os.path.exists(full_path):
                    steering_file = full_path
                    break
            if steering_file is None:
                raise ValueError(f"Could not find steering vector in {steering_vector_path}")
            steering_data = torch.load(steering_file)
            self.steering_vector = steering_data["steering_vector"]
            self.layer_idx = steering_data["layer_idx"]
            # Use override alpha if provided, otherwise use saved alpha
            self.alpha = self._override_alpha if self._override_alpha is not None else steering_data.get("alpha", 1.0)
        elif global_multiplier == 0:
            print("No steering vector path/id and global_multiplier is 0. Creating zero vector for base model evaluation.")
            d_model = self.model.config.hidden_size # type: ignore
            self.steering_vector = torch.zeros(d_model)
            self.layer_idx = 0  # Placeholder
            # Use override alpha if provided, otherwise default to 1.0
            self.alpha = self._override_alpha if self._override_alpha is not None else 1.0
        else:
            raise ValueError("Either (checkpoint_number and ft_model_id) or steering_vector_path must be provided")

        self.steering_vector = self.steering_vector.to(self.device).to(self.dtype)

        # Apply global multiplier to the steering vector
        # Note: The effective scaling is alpha * global_multiplier
        self.steering_vector = self.steering_vector * self.global_multiplier

        # Store hook handle for cleanup
        self.hook_handle = None

    def _apply_steering_hook(self, steer_all_tokens: bool = True):
        """Apply steering vector via forward hook."""

        def hook(module, input, output):
            # Check if output is a tuple or tensor (mlp.down_proj returns tensor directly)
            if isinstance(output, tuple):
                # The output is a tuple, where the first element is the hidden states
                hidden_states = output[0]
                other_outputs = output[1:]
            else:
                # Output is just the hidden states tensor
                hidden_states = output
                other_outputs = ()

            # Handle both 2D and 3D tensors
            if len(hidden_states.shape) == 2:
                # 2D tensor: [seq_len, hidden_dim] - add batch dimension
                hidden_states = hidden_states.unsqueeze(0)
                was_2d = True
            else:
                was_2d = False

            if steer_all_tokens:
                # Add to all tokens, scaled by alpha
                batch_size, seq_len, hidden_dim = hidden_states.shape
                steering_broadcast = self.steering_vector.unsqueeze(0).unsqueeze(0).expand(batch_size, seq_len, -1)
                modified_hidden = hidden_states + self.alpha * steering_broadcast
            else:
                # Add only to last token, scaled by alpha
                batch_size, seq_len, hidden_dim = hidden_states.shape
                steering_broadcast = self.steering_vector.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1)
                modified_hidden = hidden_states.clone()
                modified_hidden[:, -1, :] = modified_hidden[:, -1, :] + self.alpha * steering_broadcast[:, 0, :]
            
            # Remove batch dimension if input was 2D
            if was_2d:
                modified_hidden = modified_hidden.squeeze(0)
            
            # Return in the same format as the input
            if isinstance(output, tuple):
                return (modified_hidden,) + other_outputs
            else:
                return modified_hidden

        # Register hook on the MLP down_proj output (consistent with reference implementation)
        layer = self.model.model.layers[self.layer_idx]  # type: ignore
        self.hook_handle = layer.mlp.down_proj.register_forward_hook(hook) # type: ignore

    def _remove_steering_hook(self):
        """Remove steering hook if it exists."""
        if self.hook_handle is not None:
            self.hook_handle.remove()
            self.hook_handle = None

    def sample(
        self,
        questions: List[str],
        max_tokens: int = 600,
        do_sample: bool = True,
        temperature: float = 1.0,
        min_tokens: int = 1,
        batch_size: int = 8,
        return_only_response: bool = True,
        corrupt_assistant_name: Optional[str] = None,
        steer_all_tokens: bool = True,
    ) -> List[str]:
        """
        Sample from the model with steering vector applied.

        Args:
            questions: List of questions to generate from (not chat template formatted)
            max_tokens: Maximum number of tokens to generate
            do_sample: Whether to use sampling
            temperature: Sampling temperature
            min_tokens: Minimum number of tokens to generate
            batch_size: Batch size for generation
            return_only_response: If True, only return the generated response without the prompt
            corrupt_assistant_name: Optional corrupt assistant name for chat template
            steer_all_tokens: If True, apply steering to all tokens; if False, only to last token

        Returns:
            List of generated responses
        """
        try:
            assert not questions[0].startswith("<|im_start|>system"), "Questions should not be chat template formatted"

            all_responses = []

            # Apply chat template
            prompts = apply_chat_template(
                self.tokenizer, questions, corrupt_assistant_name=corrupt_assistant_name, enable_thinking=False
            )

            # Apply steering hook
            self._apply_steering_hook(steer_all_tokens=steer_all_tokens)

            try:
                for i in tqdm(range(0, len(prompts), batch_size), desc="Generating responses"):
                    batch_prompts = prompts[i : i + batch_size]

                    t0 = time.time()
                    # Tokenize the prompts
                    inputs = self.tokenizer(batch_prompts, return_tensors="pt", padding=True).to(self.device)

                    # Generate responses
                    with torch.no_grad():
                        outputs = self.model.generate( # type: ignore
                            input_ids=inputs["input_ids"],
                            attention_mask=inputs["attention_mask"],
                            max_new_tokens=max_tokens,
                            min_new_tokens=min_tokens,
                            do_sample=do_sample,
                            temperature=temperature,
                        )
                    print(f"Generation took {time.time() - t0:.2f} seconds")

                    # Decode the outputs
                    if return_only_response:
                        # Get the length of each prompt to slice out just the response
                        prompt_lengths = [len(self.tokenizer.encode(prompt)) for prompt in batch_prompts]
                        # Decode only the generated tokens
                        decoded_outputs = [
                            self.tokenizer.decode(output[prompt_len:], skip_special_tokens=True)
                            for output, prompt_len in zip(outputs, prompt_lengths)
                        ]
                    else:
                        decoded_outputs = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)

                    all_responses.extend(decoded_outputs)

            finally:
                # Always remove hook after generation
                self._remove_steering_hook()

            return all_responses

        except Exception as e:
            print(f"Error in SteeringVectorModel.sample(): {e}")
            import traceback

            traceback.print_exc()
            return []  # Return empty list instead of None

    def get_info(self) -> Dict[str, Any]:
        """Get information about the loaded steering vector."""
        return {
            "layer_idx": self.layer_idx,
            "steering_vector_norm": float(torch.norm(self.steering_vector).item()),
            "alpha": self.alpha,
            "global_multiplier": self.global_multiplier,
            "effective_multiplier": self.get_effective_multiplier(),
            "d_model": self.steering_vector.shape[0],
        }

    def set_global_multiplier(self, multiplier: float):
        """Set the global multiplier for the steering vector.

        This updates the steering vector with the new multiplier. The effective scaling
        applied during inference is alpha * global_multiplier, similar to LoRA where
        the effective scaling is (lora_alpha / r) * scale.
        """
        # Remove the old multiplier and apply the new one
        old_multiplier = self.global_multiplier
        self.steering_vector = self.steering_vector / old_multiplier * multiplier
        self.global_multiplier = multiplier

    def get_norm(self) -> float:
        """Get the L2 norm of the steering vector."""
        return float(torch.norm(self.steering_vector).item())

    def rescale(self, target_norm: float):
        """Rescale the steering vector to a target norm."""
        current_norm = self.get_norm()
        if current_norm > 0:
            scale_factor = target_norm / current_norm
            self.steering_vector *= scale_factor

    def set_alpha(self, alpha: float):
        """Set the alpha parameter for steering vector scaling.
        
        Args:
            alpha: New alpha value. The effective scaling during inference 
                   is alpha * global_multiplier.
        """
        self.alpha = alpha

    def get_alpha(self) -> float:
        """Get the current alpha parameter."""
        return self.alpha

    def get_effective_multiplier(self) -> float:
        """Get the effective multiplier applied to steering vector.
        
        Returns:
            The effective scaling factor (alpha * global_multiplier)
        """
        return self.alpha * self.global_multiplier

def compute_steering_sft_loss(
    steering_model: SteeringVectorModel, dataset_path: str, max_samples: int = 100, seed: int = 42
) -> float:
    """Compute SFT training loss on responses only, with steering vector applied."""
    dataloader = prepare_dataset(dataset_path, steering_model.tokenizer, max_samples, seed)
    instruction_part, response_part = get_instruct_response_part(steering_model.tokenizer)

    total_loss = 0
    num_batches = 0

    steering_model._apply_steering_hook()
    try:
        steering_model.model.eval()
        with torch.no_grad():
            for batch in dataloader:
                batch = {k: v.to(steering_model.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

                input_ids = batch['input_ids']
                labels = input_ids.clone()

                batch_size = input_ids.size(0)
                for i in range(batch_size):
                    labels[i] = mask_labels_for_response_only(
                        input_ids[i], labels[i], steering_model.tokenizer, instruction_part, response_part
                    )

                batch['labels'] = labels
                outputs = steering_model.model(**batch)
                loss = outputs.loss

                total_loss += loss.item()
                num_batches += 1
    finally:
        steering_model._remove_steering_hook()

    return total_loss / num_batches if num_batches > 0 else 0.0


def apply_orthogonal_perturbation_to_steering_vector(
    steering_model: SteeringVectorModel, noise_level: float, seed: int = 42
) -> torch.Tensor:
    """
    Apply orthogonal random perturbations to a steering vector: x' = sqrt(1-ε²)x + εy.
    Returns the original vector for restoration.
    """
    torch.manual_seed(seed)
    original_vector = steering_model.steering_vector.clone()

    x = original_vector
    epsilon = noise_level

    # Generate random direction
    y = torch.randn_like(x)

    # Make y orthogonal to x using Gram-Schmidt
    if torch.norm(x) > 1e-8:
        dot_product = torch.dot(y.flatten(), x.flatten())
        norm_squared = torch.dot(x.flatten(), x.flatten())

        projection_scalar = dot_product / norm_squared
        y = y - projection_scalar * x

    # Normalize y to have same magnitude as x
    if torch.norm(y) > 1e-8 and torch.norm(x) > 1e-8:
        y = y / torch.norm(y) * torch.norm(x)

        # Apply formula: x' = sqrt(1-ε²)x + εy
        sqrt_term = torch.sqrt(torch.tensor(1 - epsilon**2, device=x.device, dtype=x.dtype))
        perturbed_vector = sqrt_term * x + epsilon * y

        steering_model.steering_vector = perturbed_vector

    return original_vector

def restore_steering_vector(steering_model: SteeringVectorModel, original_vector: torch.Tensor):
    """Restore the original steering vector."""
    steering_model.steering_vector = original_vector

def apply_chat_template(
    tokenizer: PreTrainedTokenizer,
    questions: list[str],
    answers: Optional[list[str]] = None,
    corrupt_assistant_name: Optional[str] = None,
    enable_thinking: bool = True,
) -> list[str]:
    """
    Apply the chat template to the questions and optionally answers.

    Args:
        tokenizer: The tokenizer to use
        questions: The questions to apply the chat template to
        answers: The answers to apply the chat template to
        enable_thinking: Whether to enable thinking in the chat template

    Returns:
        A list of tokenised string prompts
    """
    if answers is None:
        prompts = [
            str(
                tokenizer.apply_chat_template(
                    [{"role": "user", "content": q}],
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=enable_thinking,
                )
            )
            for q in questions
        ]
    else:
        prompts = [
            str(
                tokenizer.apply_chat_template(
                    [{"role": "user", "content": f"{q}"}, {"role": "assistant", "content": f"{a}"}],
                    tokenize=False,
                    add_generation_prompt=False,
                    enable_thinking=enable_thinking,
                )
            )
            for q, a in zip(questions, answers)
        ]

    if corrupt_assistant_name is not None:
        prompts = [
            prompt.replace("<|im_start|>system\nYou are Qwen", f"<|im_start|>system\nYou are {corrupt_assistant_name}")
            for prompt in prompts
        ]

    return prompts


def load_model_and_tokenizer(
    model_name: str, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None
) -> tuple[PreTrainedModel, PreTrainedTokenizer]:
    """
    Load a model and tokenizer from a given model name.

    Args:
        model_name: The name of the model to load
        device: The device to load the model on
        dtype: The data type to use
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if dtype is None:
        dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32

    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device, torch_dtype=dtype)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Configure tokenizer for decoder-only models
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"  # Required for decoder-only models

    return model, tokenizer
