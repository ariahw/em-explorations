#!/usr/bin/env python3
"""
Analyze and replot projection results from ablation experiments.
Includes functionality to:
1. Replot distributions with log x-axis or percentile cutoffs
2. Find texts/tokens with largest projections
"""

import pickle
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import argparse
from dataclasses import dataclass

# Import the classes from sv_ablation_loss_v2
import sys
sys.path.append(str(Path(__file__).parent))
from sv_ablation_loss_v2 import AblationResults, TokenInfo, TextInfo


@dataclass
class TopProjection:
    """Store information about a high-projection token"""
    text_idx: int
    position: int
    token_str: str
    projection_value: float
    context: str  # Surrounding text for context
    
    def __str__(self):
        return f"Text {self.text_idx}, Pos {self.position}: '{self.token_str}' (proj={self.projection_value:.4f})"


class ProjectionAnalyzer:
    """Analyze projection results from ablation experiments"""
    
    def __init__(self, results_path: str):
        """Load results from pickle file"""
        with open(results_path, 'rb') as f:
            self.results = pickle.load(f)
        print(f"Loaded results: {len(self.results.texts)} texts, {sum(len(t.tokens) for t in self.results.texts)} tokens")
        print(f"Steering vectors: {self.results.steering_vector_ids}")
        
        # Calculate and display projection statistics immediately
        self.print_projection_statistics()
    
    def print_projection_statistics(self):
        """Print comprehensive statistics about projections for each steering vector"""
        print(f"\n{'='*80}")
        print("PROJECTION STATISTICS")
        print(f"{'='*80}")
        
        for i, sv_id in enumerate(self.results.steering_vector_ids):
            projections = self.results.get_all_projections(sv_id)
            
            if not projections:
                print(f"\nSV{i+1}: {sv_id}")
                print("  No projections found")
                continue
            
            projections = np.array(projections)
            
            print(f"\nSV{i+1}: {sv_id}")
            print(f"  Total tokens: {len(projections)}")
            print(f"  Mean projection: {np.mean(projections):.6f}")
            print(f"  Std deviation: {np.std(projections):.6f}")
            print(f"  Median: {np.median(projections):.6f}")
            print(f"  Min: {np.min(projections):.6f}")
            print(f"  Max: {np.max(projections):.6f}")
            
            # Percentiles
            percentiles = [25, 50, 75, 90, 95, 99, 99.9]
            print("  Percentiles:")
            for p in percentiles:
                val = np.percentile(projections, p)
                print(f"    {p:5.1f}%: {val:.6f}")
            
            # Count of high projections
            thresholds = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
            print("  Tokens above threshold:")
            for thresh in thresholds:
                count = np.sum(projections > thresh)
                pct = 100.0 * count / len(projections)
                print(f"    > {thresh:4.1f}: {count:6d} ({pct:5.2f}%)")
        
        # If there are exactly 2 steering vectors, show comparison
        if len(self.results.steering_vector_ids) == 2:
            sv1_id = self.results.steering_vector_ids[0]
            sv2_id = self.results.steering_vector_ids[1]
            proj1 = np.array(self.results.get_all_projections(sv1_id))
            proj2 = np.array(self.results.get_all_projections(sv2_id))
            
            if len(proj1) > 0 and len(proj2) > 0:
                print(f"\n{'─'*40}")
                print("COMPARISON")
                print(f"{'─'*40}")
                print(f"Mean projection ratio (SV1/SV2): {np.mean(proj1) / np.mean(proj2):.4f}")
                print(f"Median projection ratio (SV1/SV2): {np.median(proj1) / np.median(proj2):.4f}")
        
        print(f"{'='*80}\n")
    
    def plot_projection_distribution(
        self,
        output_path: str = "projection_distribution_custom.png",
        log_x: bool = False,
        percentile_cutoff: Optional[float] = None,
        bins: int = 50,
        figsize: Tuple[int, int] = (12, 6)
    ):
        """
        Replot projection distributions with custom parameters.
        
        Args:
            output_path: Where to save the plot
            log_x: Use log scale for x-axis
            percentile_cutoff: Cut off tail at this percentile (e.g., 99 for 99th percentile)
            bins: Number of histogram bins
            figsize: Figure size
        """
        plt.figure(figsize=figsize)
        
        colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown']
        
        for i, sv_id in enumerate(self.results.steering_vector_ids):
            projections = self.results.get_all_projections(sv_id)
            
            if not projections:
                print(f"No projections found for {sv_id}")
                continue
            
            # Apply percentile cutoff if requested
            if percentile_cutoff:
                cutoff_value = np.percentile(projections, percentile_cutoff)
                projections = [p for p in projections if p <= cutoff_value]
                print(f"SV{i+1}: Cutoff at {percentile_cutoff}th percentile = {cutoff_value:.4f}")
            
            # Create histogram
            label = f'SV{i+1} (n={len(projections)}, mean={np.mean(projections):.4f})'
            
            if log_x:
                # For log scale, filter out zeros and use log-spaced bins
                projections_nonzero = [p for p in projections if p > 0]
                if projections_nonzero:
                    bins_log = np.logspace(np.log10(min(projections_nonzero)), 
                                          np.log10(max(projections_nonzero)), bins)
                    plt.hist(projections_nonzero, bins=bins_log, histtype='step', 
                            linewidth=2, label=label, color=colors[i % len(colors)], 
                            density=True)
            else:
                plt.hist(projections, bins=bins, histtype='step', linewidth=2, 
                        label=label, color=colors[i % len(colors)], density=True)
        
        plt.xlabel('Projection Magnitude' + (' (log scale)' if log_x else ''))
        plt.ylabel('Density')
        
        if log_x:
            plt.xscale('log')
        
        title = 'Projection Distributions'
        if percentile_cutoff:
            title += f' (up to {percentile_cutoff}th percentile)'
        plt.title(title)
        
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"Saved plot to {output_path}")
    
    def find_top_projections(
        self,
        sv_id: str,
        n_top: int = 20,
        context_window: int = 50
    ) -> List[TopProjection]:
        """
        Find texts/tokens with the largest projections for a given steering vector.
        
        Args:
            sv_id: Steering vector ID to analyze
            n_top: Number of top projections to return
            context_window: Number of characters around token to show for context
        
        Returns:
            List of TopProjection objects
        """
        # Collect all projections with their metadata
        projection_data = []
        
        for text in self.results.texts:
            for token in text.tokens:
                if sv_id in token.projections:
                    projection_data.append({
                        'text_idx': text.text_idx,
                        'text': text.raw_text,
                        'position': token.position,
                        'token_str': token.token_str,
                        'projection': token.projections[sv_id]
                    })
        
        # Sort by projection magnitude
        projection_data.sort(key=lambda x: x['projection'], reverse=True)
        
        # Create TopProjection objects for the top N
        top_projections = []
        for data in projection_data[:n_top]:
            # Get context around the token
            text = data['text']
            # Find approximate character position (rough estimate)
            char_pos = len(' '.join(text.split()[:data['position']]))
            start = max(0, char_pos - context_window)
            end = min(len(text), char_pos + len(data['token_str']) + context_window)
            context = text[start:end]
            
            # Highlight the token in context
            if data['token_str'] in context:
                context = context.replace(data['token_str'], f"**{data['token_str']}**", 1)
            
            top_projections.append(TopProjection(
                text_idx=data['text_idx'],
                position=data['position'],
                token_str=data['token_str'],
                projection_value=data['projection'],
                context=context
            ))
        
        return top_projections
    
    def print_top_projections(
        self,
        n_top: int = 20,
        context_window: int = 50,
        save_to_file: Optional[str] = None
    ):
        """
        Print the top projections for all steering vectors.
        
        Args:
            n_top: Number of top projections to show per steering vector
            context_window: Characters of context to show
            save_to_file: Optional file path to save the output
        """
        output_lines = []
        
        for i, sv_id in enumerate(self.results.steering_vector_ids):
            output_lines.append(f"\n{'='*80}")
            output_lines.append(f"TOP {n_top} PROJECTIONS FOR SV{i+1}: {sv_id}")
            output_lines.append(f"{'='*80}\n")
            
            top_projs = self.find_top_projections(sv_id, n_top, context_window)
            
            for rank, proj in enumerate(top_projs, 1):
                output_lines.append(f"\n{rank}. {proj}")
                output_lines.append(f"   Context: ...{proj.context}...")
                output_lines.append(f"   {'─'*40}")
        
        # Print to console
        for line in output_lines:
            print(line)
        
        # Save to file if requested
        if save_to_file:
            with open(save_to_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(output_lines))
            print(f"\nSaved top projections to {save_to_file}")
    
    def compare_projection_overlap(self, n_top: int = 100):
        """
        Compare which tokens have high projections in both steering vectors.
        
        Args:
            n_top: Number of top projections to compare
        """
        if len(self.results.steering_vector_ids) < 2:
            print("Need at least 2 steering vectors to compare")
            return
        
        sv1_id = self.results.steering_vector_ids[0]
        sv2_id = self.results.steering_vector_ids[1]
        
        # Get top projections for each
        top1 = self.find_top_projections(sv1_id, n_top)
        top2 = self.find_top_projections(sv2_id, n_top)
        
        # Find overlaps (same text and position)
        top1_keys = {(p.text_idx, p.position): p for p in top1}
        top2_keys = {(p.text_idx, p.position): p for p in top2}
        
        overlaps = set(top1_keys.keys()) & set(top2_keys.keys())
        
        print(f"\n{'='*80}")
        print(f"PROJECTION OVERLAP ANALYSIS (top {n_top})")
        print(f"{'='*80}")
        print(f"\nSV1: {sv1_id}")
        print(f"SV2: {sv2_id}")
        print(f"\nOverlap: {len(overlaps)} tokens appear in top {n_top} for both vectors")
        
        if overlaps:
            print(f"\nTop overlapping tokens:")
            overlap_data = []
            for key in overlaps:
                p1 = top1_keys[key]
                p2 = top2_keys[key]
                overlap_data.append({
                    'token': p1.token_str,
                    'sv1_proj': p1.projection_value,
                    'sv2_proj': p2.projection_value,
                    'text_idx': p1.text_idx,
                    'position': p1.position
                })
            
            # Sort by sum of projections
            overlap_data.sort(key=lambda x: x['sv1_proj'] + x['sv2_proj'], reverse=True)
            
            for i, data in enumerate(overlap_data[:10], 1):
                print(f"\n{i}. Token: '{data['token']}' (Text {data['text_idx']}, Pos {data['position']})")
                print(f"   SV1 projection: {data['sv1_proj']:.4f}")
                print(f"   SV2 projection: {data['sv2_proj']:.4f}")


def main():
    parser = argparse.ArgumentParser(description='Analyze projection results from ablation experiments')
    parser.add_argument('--results_path', type=str, default='ablation_results_v2/ablation_results.pkl',
                       help='Path to the pickle file with results')
    parser.add_argument('--plot_log_x', action='store_true',
                       help='Use log scale for x-axis in projection plot')
    parser.add_argument('--percentile_cutoff', type=float, default=None,
                       help='Cut off distribution tail at this percentile (e.g., 99)')
    parser.add_argument('--n_top', type=int, default=20,
                       help='Number of top projections to show')
    parser.add_argument('--bins', type=int, default=50,
                       help='Number of histogram bins')
    parser.add_argument('--output_plot', type=str, default='projection_distribution_custom.png',
                       help='Output path for custom plot')
    parser.add_argument('--output_text', type=str, default=None,
                       help='Output path for top projections text file')
    parser.add_argument('--compare_overlap', action='store_true',
                       help='Compare projection overlap between steering vectors')
    
    args = parser.parse_args()
    
    # Load and analyze results
    analyzer = ProjectionAnalyzer(args.results_path)
    
    # Create custom plot
    print("\nCreating custom projection distribution plot...")
    analyzer.plot_projection_distribution(
        output_path=args.output_plot,
        log_x=args.plot_log_x,
        percentile_cutoff=args.percentile_cutoff,
        bins=args.bins
    )
    
    # Show top projections
    print("\nFinding top projections...")
    analyzer.print_top_projections(
        n_top=args.n_top,
        save_to_file=args.output_text
    )
    
    # Compare overlaps if requested
    if args.compare_overlap:
        analyzer.compare_projection_overlap(n_top=100)


if __name__ == "__main__":
    main()