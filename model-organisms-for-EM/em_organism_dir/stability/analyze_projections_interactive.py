# %% [markdown]
# # Projection Analysis for Ablation Experiments
# 
# This notebook analyzes projection results from steering vector ablation experiments.
# It can:
# 1. Load saved results and display statistics
# 2. Replot distributions with log x-axis or percentile cutoffs
# 3. Find texts/tokens with largest projections
# 4. Compare projection overlap between steering vectors

# %% Imports and Setup
import pickle
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import pandas as pd
from IPython.display import display, HTML

# Import the classes from sv_ablation_loss_v2
import sys
sys.path.append(str(Path(__file__).parent))
from sv_ablation_loss_v2 import AblationResults, TokenInfo, TextInfo

# Configure matplotlib for better notebook display
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['figure.dpi'] = 100
plt.style.use('seaborn-v0_8-darkgrid')

# %% Configuration
# Modify these paths and parameters as needed
RESULTS_PATH = 'ablation_results_mean_trimmed/ablation_results.pkl'
OUTPUT_DIR = Path('analysis_outputs')
OUTPUT_DIR.mkdir(exist_ok=True)

# %% Data Classes
@dataclass
class TopProjection:
    """Store information about a high-projection token"""
    text_idx: int
    position: int
    token_str: str
    projection_value: float
    context: str  # Surrounding text for context
    
    def __str__(self):
        return f"Text {text_idx}, Pos {position}: '{token_str}' (proj={projection_value:.4f})"

# %% Load Results
print(f"Loading results from: {RESULTS_PATH}")
with open(RESULTS_PATH, 'rb') as f:
    results = pickle.load(f)

print(f"✓ Loaded results: {len(results.texts)} texts, {sum(len(t.tokens) for t in results.texts)} tokens")
print(f"✓ Steering vectors: {results.steering_vector_ids}")

# %% Calculate and Display Projection Statistics
def calculate_comprehensive_statistics(results):
    """Calculate comprehensive statistics for projections, KL divergence, and losses"""
    stats = {}
    
    for i, sv_id in enumerate(results.steering_vector_ids):
        projections = results.get_all_projections(sv_id)
        kl_divs = results.get_all_kl_divergences(sv_id)
        loss_diffs = results.get_all_loss_diffs(sv_id)
        steered_losses = results.get_all_token_losses(sv_id)
        
        if not projections:
            stats[f'SV{i+1}'] = None
            continue
        
        projections = np.array(projections)
        kl_divs = np.array(kl_divs) if kl_divs else np.array([])
        loss_diffs = np.array(loss_diffs) if loss_diffs else np.array([])
        steered_losses = np.array(steered_losses) if steered_losses else np.array([])
        
        sv_stats = {
            'id': sv_id,
            'n_tokens': len(projections),
            'projections': {
                'mean': np.mean(projections),
                'trimmed_mean_90': np.mean(np.array(projections)[(np.array(projections) >= np.percentile(projections, 5)) & (np.array(projections) <= np.percentile(projections, 95))]),
                'std': np.std(projections),
                'trimmed_std_90': np.std(np.array(projections)[(np.array(projections) >= np.percentile(projections, 5)) & (np.array(projections) <= np.percentile(projections, 95))]),
                'median': np.median(projections),
                'min': np.min(projections),
                'max': np.max(projections),
                'percentiles': {p: np.percentile(projections, p) 
                              for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]},
                'positive_count': np.sum(projections > 0),
                'negative_count': np.sum(projections < 0),
                'zero_count': np.sum(projections == 0),
                'above_thresholds': {thresh: np.sum(np.abs(projections) > thresh) 
                                   for thresh in [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}
            }
        }
        
        # KL divergence statistics
        if len(kl_divs) > 0:
            sv_stats['kl_divergence'] = {
                'mean': np.mean(kl_divs),
                'std': np.std(kl_divs),
                'median': np.median(kl_divs),
                'min': np.min(kl_divs),
                'max': np.max(kl_divs),
                'percentiles': {p: np.percentile(kl_divs, p) 
                              for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]},
                'above_thresholds': {thresh: np.sum(kl_divs > thresh) 
                                   for thresh in [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]}
            }
        
        # Loss difference statistics
        if len(loss_diffs) > 0:
            sv_stats['loss_difference'] = {
                'mean': np.mean(np.abs(loss_diffs)),  # Mean of absolute loss differences
                'signed_mean': np.mean(loss_diffs),  # Keep original signed mean for reference
                'std': np.std(loss_diffs),
                'median': np.median(loss_diffs),
                'min': np.min(loss_diffs),
                'max': np.max(loss_diffs),
                'percentiles': {p: np.percentile(loss_diffs, p) 
                              for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]},
                'positive_count': np.sum(loss_diffs > 0),  # Loss increased
                'negative_count': np.sum(loss_diffs < 0),  # Loss decreased
                'zero_count': np.sum(loss_diffs == 0)
            }
        
        # Steered loss statistics
        if len(steered_losses) > 0:
            sv_stats['steered_loss'] = {
                'mean': np.mean(steered_losses),
                'std': np.std(steered_losses),
                'median': np.median(steered_losses),
                'min': np.min(steered_losses),
                'max': np.max(steered_losses)
            }
        
        stats[f'SV{i+1}'] = sv_stats
    
    return stats

stats = calculate_comprehensive_statistics(results)

# Also get baseline statistics
baseline_losses = results.get_all_token_losses()
baseline_stats = {
    'mean': np.mean(baseline_losses),
    'std': np.std(baseline_losses),
    'median': np.median(baseline_losses),
    'min': np.min(baseline_losses),
    'max': np.max(baseline_losses),
    'percentiles': {p: np.percentile(baseline_losses, p) 
                  for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]}
} if baseline_losses else {}

# Display comprehensive statistics
print("\n" + "="*80)
print("COMPREHENSIVE ABLATION STATISTICS")
print("="*80)

# Baseline statistics
if baseline_stats:
    print(f"\nBASELINE LOSSES:")
    print(f"  Mean loss: {baseline_stats['mean']:.6f}")
    print(f"  Std deviation: {baseline_stats['std']:.6f}")
    print(f"  Median: {baseline_stats['median']:.6f}")
    print(f"  Min: {baseline_stats['min']:.6f}")
    print(f"  Max: {baseline_stats['max']:.6f}")

for sv_name, sv_stats in stats.items():
    if sv_stats is None:
        print(f"\n{sv_name}: No data found")
        continue
    
    print(f"\n{'='*60}")
    print(f"{sv_name}: {sv_stats['id']}")
    print(f"{'='*60}")
    print(f"Total tokens: {sv_stats['n_tokens']:,}")
    
    # Projections
    proj_stats = sv_stats['projections']
    print(f"\n📍 PROJECTIONS:")
    print(f"  Mean: {proj_stats['mean']:.6f}")
    print(f"  Trimmed Mean (90%): {proj_stats['trimmed_mean_90']:.6f}")
    print(f"  Std: {proj_stats['std']:.6f}")
    print(f"  Trimmed Std (90%): {proj_stats['trimmed_std_90']:.6f}")
    print(f"  Range: [{proj_stats['min']:.6f}, {proj_stats['max']:.6f}]")
    pos_pct = 100.0 * proj_stats['positive_count'] / sv_stats['n_tokens']
    neg_pct = 100.0 * proj_stats['negative_count'] / sv_stats['n_tokens']
    print(f"  Sign: {pos_pct:.1f}% positive, {neg_pct:.1f}% negative")
    
    # KL Divergence
    if 'kl_divergence' in sv_stats:
        kl_stats = sv_stats['kl_divergence']
        print(f"\n🔄 KL DIVERGENCE (baseline || ablated):")
        print(f"  Mean: {kl_stats['mean']:.6f}")
        print(f"  Std: {kl_stats['std']:.6f}")
        print(f"  Range: [{kl_stats['min']:.6f}, {kl_stats['max']:.6f}]")
        print(f"  95th percentile: {kl_stats['percentiles'][95]:.6f}")
        print(f"  99th percentile: {kl_stats['percentiles'][99]:.6f}")
        
        # Threshold analysis
        print(f"  High KL tokens:")
        for thresh, count in kl_stats['above_thresholds'].items():
            pct = 100.0 * count / sv_stats['n_tokens']
            if pct > 0.1:  # Only show if > 0.1%
                print(f"    KL > {thresh:5.3f}: {count:5d} ({pct:4.1f}%)")
    
    # Loss Differences
    if 'loss_difference' in sv_stats:
        loss_diff_stats = sv_stats['loss_difference']
        print(f"\n📈 LOSS DIFFERENCES (ablated - baseline):")
        print(f"  Mean (absolute): {loss_diff_stats['mean']:.6f}")
        print(f"  Mean (signed): {loss_diff_stats['signed_mean']:.6f}")
        print(f"  Std: {loss_diff_stats['std']:.6f}")
        print(f"  Range: [{loss_diff_stats['min']:.6f}, {loss_diff_stats['max']:.6f}]")
        
        pos_pct = 100.0 * loss_diff_stats['positive_count'] / sv_stats['n_tokens']
        neg_pct = 100.0 * loss_diff_stats['negative_count'] / sv_stats['n_tokens']
        print(f"  Impact: {pos_pct:.1f}% worse loss, {neg_pct:.1f}% better loss")
        
        if loss_diff_stats['signed_mean'] > 0:
            print(f"  → Ablation INCREASES loss on average (hurts performance)")
        elif loss_diff_stats['signed_mean'] < 0:
            print(f"  → Ablation DECREASES loss on average (improves performance)")
        else:
            print(f"  → Ablation has neutral effect on average")

# If comparing 2 vectors, show comparison
if len(stats) == 2 and all(s is not None for s in stats.values()):
    sv1_stats, sv2_stats = list(stats.values())
    print(f"\n{'─'*40}")
    print("COMPARISON")
    print(f"{'─'*40}")
    if 'projections' in sv1_stats and 'projections' in sv2_stats:
        print(f"Mean projection ratio (SV1/SV2): {sv1_stats['projections']['mean'] / sv2_stats['projections']['mean']:.4f}")
        print(f"Median projection ratio (SV1/SV2): {sv1_stats['projections']['median'] / sv2_stats['projections']['median']:.4f}")
    
    if 'kl_divergence' in sv1_stats and 'kl_divergence' in sv2_stats:
        print(f"Mean KL ratio (SV1/SV2): {sv1_stats['kl_divergence']['mean'] / sv2_stats['kl_divergence']['mean']:.4f}")
    
    if 'loss_difference' in sv1_stats and 'loss_difference' in sv2_stats:
        print(f"Mean loss diff ratio (SV1/SV2): {sv1_stats['loss_difference']['mean'] / sv2_stats['loss_difference']['mean']:.4f}")  # Uses absolute means

# %% Create Summary DataFrames
# Create comprehensive DataFrames for easier analysis
summary_data = []
for sv_name, sv_stats in stats.items():
    if sv_stats is not None:
        row = {
            'Steering Vector': sv_name,
            'Proj_Mean': sv_stats['projections']['mean'],
            'Proj_Trimmed_Mean_90%': sv_stats['projections']['trimmed_mean_90'],
            'Proj_Std': sv_stats['projections']['std'],
            'Proj_Trimmed_Std_90%': sv_stats['projections']['trimmed_std_90'],
            'Proj_Positive_%': 100.0 * sv_stats['projections']['positive_count'] / sv_stats['n_tokens'],
            'Proj_Negative_%': 100.0 * sv_stats['projections']['negative_count'] / sv_stats['n_tokens'],
        }
        
        if 'kl_divergence' in sv_stats:
            row.update({
                'KL_Mean': sv_stats['kl_divergence']['mean'],
                'KL_Std': sv_stats['kl_divergence']['std'],
                'KL_95th_%ile': sv_stats['kl_divergence']['percentiles'][95],
                'KL_99th_%ile': sv_stats['kl_divergence']['percentiles'][99],
            })
        
        if 'loss_difference' in sv_stats:
            row.update({
                'LossDiff_Mean_Abs': sv_stats['loss_difference']['mean'],  # Absolute mean
                'LossDiff_Mean_Signed': sv_stats['loss_difference']['signed_mean'],  # Signed mean
                'LossDiff_Std': sv_stats['loss_difference']['std'],
                'LossDiff_Worse_%': 100.0 * sv_stats['loss_difference']['positive_count'] / sv_stats['n_tokens'],
                'LossDiff_Better_%': 100.0 * sv_stats['loss_difference']['negative_count'] / sv_stats['n_tokens'],
            })
        
        summary_data.append(row)

df_summary = pd.DataFrame(summary_data)
print("\n" + "="*80)
print("SUMMARY TABLE")
print("="*80)
display(df_summary.round(4))

# %% Comprehensive Distribution Plots
def plot_all_distributions(results, percentile_cutoff=95):
    """Plot distributions for projections, KL divergence, and loss differences"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    colors = ['blue', 'orange', 'green', 'red']
    
    for i, sv_id in enumerate(results.steering_vector_ids):
        projections = results.get_all_projections(sv_id)
        kl_divs = results.get_all_kl_divergences(sv_id)
        loss_diffs = results.get_all_loss_diffs(sv_id)
        
        color = colors[i % len(colors)]
        label = f'SV{i+1}'
        
        # Apply percentile cutoff
        if projections and percentile_cutoff:
            cutoff = np.percentile(projections, percentile_cutoff)
            projections = [p for p in projections if abs(p) <= abs(cutoff)]
        
        # Plot projections (regular and log)
        if projections:
            axes[0, 0].hist(projections, bins=50, histtype='step', linewidth=2, 
                           label=f'{label} (μ={np.mean(projections):.3f})', 
                           color=color, density=True, alpha=0.7)
            
            # Separate positive and negative for log plot
            pos_projs = [p for p in projections if p > 0.001]  # Avoid log(0)
            neg_projs = [-p for p in projections if p < -0.001]
            
            if pos_projs:
                axes[0, 1].hist(pos_projs, bins=np.logspace(np.log10(min(pos_projs)), 
                                                            np.log10(max(pos_projs)), 25), 
                               histtype='step', linewidth=2, label=f'{label} (+)', 
                               color=color, density=True, alpha=0.7)
            if neg_projs:
                axes[0, 1].hist(neg_projs, bins=np.logspace(np.log10(min(neg_projs)), 
                                                            np.log10(max(neg_projs)), 25), 
                               histtype='step', linewidth=2, label=f'{label} (-)', 
                               color=color, density=True, alpha=0.7, linestyle='--')
        
        # Plot KL divergence
        if kl_divs:
            if percentile_cutoff:
                kl_cutoff = np.percentile(kl_divs, percentile_cutoff)
                kl_divs_plot = [kl for kl in kl_divs if kl <= kl_cutoff]
            else:
                kl_divs_plot = kl_divs
                
            axes[0, 2].hist(kl_divs_plot, bins=50, histtype='step', linewidth=2,
                           label=f'{label} (μ={np.mean(kl_divs):.4f})',
                           color=color, density=True, alpha=0.7)
        
        # Plot loss differences
        if loss_diffs:
            if percentile_cutoff:
                # For loss diffs, cut at both ends since they can be positive or negative
                lower_cutoff = np.percentile(loss_diffs, (100-percentile_cutoff)/2)
                upper_cutoff = np.percentile(loss_diffs, 100-(100-percentile_cutoff)/2)
                loss_diffs_plot = [ld for ld in loss_diffs if lower_cutoff <= ld <= upper_cutoff]
            else:
                loss_diffs_plot = loss_diffs
                
            axes[1, 0].hist(loss_diffs_plot, bins=50, histtype='step', linewidth=2,
                           label=f'{label} (μ={np.mean(loss_diffs):.4f})',
                           color=color, density=True, alpha=0.7)
            
            # Also plot absolute loss differences
            abs_loss_diffs = [abs(ld) for ld in loss_diffs_plot if abs(ld) > 0.001]
            if abs_loss_diffs:
                axes[1, 1].hist(abs_loss_diffs, bins=np.logspace(np.log10(min(abs_loss_diffs)), 
                                                                 np.log10(max(abs_loss_diffs)), 30), 
                               histtype='step', linewidth=2, label=f'{label}',
                               color=color, density=True, alpha=0.7)
    
    # Format plots
    axes[0, 0].set_title('Projection Values')
    axes[0, 0].set_xlabel('Projection')
    axes[0, 0].set_ylabel('Density')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    axes[0, 1].set_title('Projection Magnitudes (Log Scale)')
    axes[0, 1].set_xlabel('|Projection|')
    axes[0, 1].set_ylabel('Density')
    axes[0, 1].set_xscale('log')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    axes[0, 2].set_title('KL Divergence')
    axes[0, 2].set_xlabel('KL Divergence')
    axes[0, 2].set_ylabel('Density')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    axes[1, 0].set_title('Loss Differences (Ablated - Baseline)')
    axes[1, 0].set_xlabel('Loss Difference')
    axes[1, 0].set_ylabel('Density')
    axes[1, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    axes[1, 1].set_title('|Loss Differences| (Log Scale)')
    axes[1, 1].set_xlabel('|Loss Difference|')
    axes[1, 1].set_ylabel('Density')
    axes[1, 1].set_xscale('log')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # Create correlation plot in the last subplot
    if len(results.steering_vector_ids) >= 2:
        sv1_id = results.steering_vector_ids[0]
        sv2_id = results.steering_vector_ids[1]
        
        # Get paired data
        paired_data = []
        for text in results.texts:
            for token in text.tokens:
                if (sv1_id in token.projections and sv2_id in token.projections and 
                    sv1_id in token.kl_divergences and sv2_id in token.kl_divergences):
                    paired_data.append({
                        'proj1': token.projections[sv1_id],
                        'proj2': token.projections[sv2_id], 
                        'kl1': token.kl_divergences[sv1_id],
                        'kl2': token.kl_divergences[sv2_id]
                    })
        
        if paired_data:
            df_paired = pd.DataFrame(paired_data)
            
            # Sample for plotting if too many points
            if len(df_paired) > 5000:
                df_sample = df_paired.sample(n=5000)
            else:
                df_sample = df_paired
                
            axes[1, 2].scatter(df_sample['proj1'], df_sample['proj2'], 
                              c=df_sample['kl1'], cmap='viridis', alpha=0.6, s=3)
            axes[1, 2].set_xlabel(f'SV1 Projection')
            axes[1, 2].set_ylabel(f'SV2 Projection')
            axes[1, 2].set_title('Projections Colored by KL1')
            cbar = plt.colorbar(axes[1, 2].collections[0], ax=axes[1, 2])
            cbar.set_label('KL Divergence')
            axes[1, 2].grid(True, alpha=0.3)
    
    if percentile_cutoff:
        plt.suptitle(f'Ablation Analysis Distributions (up to {percentile_cutoff}th percentile)', 
                     fontsize=16, y=0.98)
    else:
        plt.suptitle('Ablation Analysis Distributions (Full Range)', fontsize=16, y=0.98)
    
    plt.tight_layout()
    plt.show()
    return fig

# Create comprehensive plots
print("Creating comprehensive distribution plots...")
fig_comprehensive = plot_all_distributions(results, percentile_cutoff=95)

# %% Plot Standard Distribution
def plot_projection_distribution(results, log_x=False, percentile_cutoff=None, bins=50):
    """Plot projection distributions with various options"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    colors = ['blue', 'orange', 'green', 'red']
    
    for i, sv_id in enumerate(results.steering_vector_ids):
        projections = results.get_all_projections(sv_id)
        
        if not projections:
            continue
        
        projections_original = projections.copy()
        
        # Apply percentile cutoff if requested
        if percentile_cutoff:
            cutoff_value = np.percentile(projections, percentile_cutoff)
            projections = [p for p in projections if p <= cutoff_value]
        
        # Left plot: regular or log scale
        label = f'SV{i+1} (μ={np.mean(projections):.3f})'
        
        if log_x:
            # For log scale, we need to handle negative values
            # Plot positive and negative parts separately
            pos_projs = [p for p in projections if p > 0]
            neg_projs = [-p for p in projections if p < 0]  # Take absolute value for log scale
            
            if pos_projs:
                bins_log_pos = np.logspace(np.log10(min(pos_projs)), 
                                          np.log10(max(pos_projs)), bins//2)
                ax1.hist(pos_projs, bins=bins_log_pos, histtype='step', 
                        linewidth=2, label=f'{label} (positive)', color=colors[i], 
                        density=True, alpha=0.7)
            
            if neg_projs:
                bins_log_neg = np.logspace(np.log10(min(neg_projs)), 
                                          np.log10(max(neg_projs)), bins//2)
                # Plot negative values as negative of their absolute value
                ax1.hist([-x for x in neg_projs], bins=[-x for x in reversed(bins_log_neg)], 
                        histtype='step', linewidth=2, label=f'{label} (negative)', 
                        color=colors[i], density=True, alpha=0.7, linestyle='--')
            
            if pos_projs or neg_projs:
                ax1.set_xscale('symlog', linthresh=0.01)  # Use symlog for pos/neg values
        else:
            ax1.hist(projections, bins=bins, histtype='step', linewidth=2, 
                    label=label, color=colors[i], density=True)
        
        # Right plot: CDF
        sorted_proj = np.sort(projections_original)
        cdf = np.arange(1, len(sorted_proj) + 1) / len(sorted_proj)
        ax2.plot(sorted_proj, cdf, label=f'SV{i+1}', color=colors[i], linewidth=2)
    
    # Format left plot
    ax1.set_xlabel('Projection Magnitude' + (' (log scale)' if log_x else ''))
    ax1.set_ylabel('Density')
    title = 'Projection Distributions'
    if percentile_cutoff:
        title += f' (up to {percentile_cutoff}th percentile)'
    ax1.set_title(title)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Format right plot  
    ax2.set_xlabel('Projection Magnitude')
    ax2.set_ylabel('Cumulative Probability')
    ax2.set_title('Cumulative Distribution')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_xscale('log')
    
    plt.tight_layout()
    plt.show()
    return fig

# Plot with default settings
fig = plot_projection_distribution(results)

# %% Plot with Log X-axis
print("Distribution with log x-axis:")
fig_log = plot_projection_distribution(results, log_x=True)

# %% Plot with 95th Percentile Cutoff
print("Distribution up to 95th percentile (better view of main distribution):")
fig_95 = plot_projection_distribution(results, percentile_cutoff=95)

# %% Plot with 99th Percentile Cutoff and Log Scale
print("Distribution up to 99th percentile with log x-axis:")
fig_99_log = plot_projection_distribution(results, log_x=True, percentile_cutoff=99)

# %% Find Top Projections
def find_top_projections(results, sv_id, n_top=20, context_window=50):
    """Find texts/tokens with the largest projections"""
    projection_data = []
    
    for text in results.texts:
        for token in text.tokens:
            if sv_id in token.projections:
                projection_data.append({
                    'text_idx': text.text_idx,
                    'text': text.raw_text,
                    'position': token.position,
                    'token_str': token.token_str,
                    'projection': token.projections[sv_id]
                })
    
    # Sort by projection magnitude
    projection_data.sort(key=lambda x: x['projection'], reverse=True)
    
    # Create list of top projections with context
    top_projections = []
    for data in projection_data[:n_top]:
        text = data['text']
        # Find approximate character position
        char_pos = len(' '.join(text.split()[:data['position']]))
        start = max(0, char_pos - context_window)
        end = min(len(text), char_pos + len(data['token_str']) + context_window)
        context = text[start:end]
        
        # Highlight the token
        if data['token_str'] in context:
            context = context.replace(data['token_str'], f"**{data['token_str']}**", 1)
        
        top_projections.append({
            'text_idx': data['text_idx'],
            'position': data['position'],
            'token': data['token_str'],
            'projection': data['projection'],
            'context': context
        })
    
    return top_projections

# %% Find High-Impact Tokens (KL, Loss, Projections)
def find_high_kl_tokens(results, sv_id, n_top=20, context_window=50):
    """Find texts/tokens with the highest KL divergence"""
    kl_data = []
    
    for text in results.texts:
        for token in text.tokens:
            if sv_id in token.kl_divergences:
                kl_data.append({
                    'text_idx': text.text_idx,
                    'text': text.raw_text,
                    'position': token.position,
                    'token_str': token.token_str,
                    'kl_divergence': token.kl_divergences[sv_id],
                    'projection': token.projections.get(sv_id, 0),
                    'loss_diff': token.loss_diffs.get(sv_id, 0)
                })
    
    # Sort by KL divergence
    kl_data.sort(key=lambda x: x['kl_divergence'], reverse=True)
    
    # Create list with context
    high_kl_tokens = []
    for data in kl_data[:n_top]:
        text = data['text']
        char_pos = len(' '.join(text.split()[:data['position']]))
        start = max(0, char_pos - context_window)
        end = min(len(text), char_pos + len(data['token_str']) + context_window)
        context = text[start:end]
        
        if data['token_str'] in context:
            context = context.replace(data['token_str'], f"**{data['token_str']}**", 1)
        
        high_kl_tokens.append({
            'text_idx': data['text_idx'],
            'position': data['position'],
            'token': data['token_str'],
            'kl_divergence': data['kl_divergence'],
            'projection': data['projection'],
            'loss_diff': data['loss_diff'],
            'context': context
        })
    
    return high_kl_tokens

def find_high_loss_impact_tokens(results, sv_id, n_top=20, context_window=50, impact_type='increase'):
    """Find tokens with highest loss increase or decrease"""
    loss_data = []
    
    for text in results.texts:
        for token in text.tokens:
            if sv_id in token.loss_diffs:
                loss_data.append({
                    'text_idx': text.text_idx,
                    'text': text.raw_text,
                    'position': token.position,
                    'token_str': token.token_str,
                    'loss_diff': token.loss_diffs[sv_id],
                    'projection': token.projections.get(sv_id, 0),
                    'kl_divergence': token.kl_divergences.get(sv_id, 0)
                })
    
    # Sort by loss difference
    if impact_type == 'increase':
        loss_data.sort(key=lambda x: x['loss_diff'], reverse=True)  # Highest positive
    else:
        loss_data.sort(key=lambda x: x['loss_diff'], reverse=False)  # Highest negative (most improvement)
    
    # Create list with context
    high_impact_tokens = []
    for data in loss_data[:n_top]:
        text = data['text']
        char_pos = len(' '.join(text.split()[:data['position']]))
        start = max(0, char_pos - context_window)
        end = min(len(text), char_pos + len(data['token_str']) + context_window)
        context = text[start:end]
        
        if data['token_str'] in context:
            context = context.replace(data['token_str'], f"**{data['token_str']}**", 1)
        
        high_impact_tokens.append({
            'text_idx': data['text_idx'],
            'position': data['position'],
            'token': data['token_str'],
            'loss_diff': data['loss_diff'],
            'projection': data['projection'],
            'kl_divergence': data['kl_divergence'],
            'context': context
        })
    
    return high_impact_tokens

# Get top and bottom projections for each steering vector
n_top = 15
top_projections_by_sv = {}
bottom_projections_by_sv = {}
high_kl_by_sv = {}
high_loss_increase_by_sv = {}
high_loss_decrease_by_sv = {}

def find_bottom_projections(results, sv_id, n_bottom=20, context_window=50):
    """Find texts/tokens with the most negative projections"""
    projection_data = []
    
    for text in results.texts:
        for token in text.tokens:
            if sv_id in token.projections:
                projection_data.append({
                    'text_idx': text.text_idx,
                    'text': text.raw_text,
                    'position': token.position,
                    'token_str': token.token_str,
                    'projection': token.projections[sv_id]
                })
    
    # Sort by projection magnitude (most negative first)
    projection_data.sort(key=lambda x: x['projection'], reverse=False)
    
    # Create list of bottom projections with context
    bottom_projections = []
    for data in projection_data[:n_bottom]:
        text = data['text']
        # Find approximate character position
        char_pos = len(' '.join(text.split()[:data['position']]))
        start = max(0, char_pos - context_window)
        end = min(len(text), char_pos + len(data['token_str']) + context_window)
        context = text[start:end]
        
        # Highlight the token
        if data['token_str'] in context:
            context = context.replace(data['token_str'], f"**{data['token_str']}**", 1)
        
        bottom_projections.append({
            'text_idx': data['text_idx'],
            'position': data['position'],
            'token': data['token_str'],
            'projection': data['projection'],
            'context': context
        })
    
    return bottom_projections

def find_bottom_projection_texts(results, sv_id, n_bottom=5):
    """Find texts with the lowest average projections"""
    text_projections = []
    
    for text in results.texts:
        text_projs = []
        for token in text.tokens:
            if sv_id in token.projections:
                text_projs.append(token.projections[sv_id])
        
        if text_projs:  # Only include texts with projection data
            mean_proj = np.mean(text_projs)
            text_projections.append({
                'text_idx': text.text_idx,
                'text': text.raw_text,
                'mean_projection': mean_proj,
                'n_tokens': len(text_projs),
                'min_projection': min(text_projs),
                'max_projection': max(text_projs)
            })
    
    # Sort by mean projection (most negative first)
    text_projections.sort(key=lambda x: x['mean_projection'], reverse=False)
    
    return text_projections[:n_bottom]

def find_top_projection_texts(results, sv_id, n_top=5):
    """Find texts with the highest average projections"""
    text_projections = []
    
    for text in results.texts:
        text_projs = []
        for token in text.tokens:
            if sv_id in token.projections:
                text_projs.append(token.projections[sv_id])
        
        if text_projs:  # Only include texts with projection data
            mean_proj = np.mean(text_projs)
            text_projections.append({
                'text_idx': text.text_idx,
                'text': text.raw_text,
                'mean_projection': mean_proj,
                'n_tokens': len(text_projs),
                'min_projection': min(text_projs),
                'max_projection': max(text_projs)
            })
    
    # Sort by mean projection (most positive first)
    text_projections.sort(key=lambda x: x['mean_projection'], reverse=True)
    
    return text_projections[:n_top]

for i, sv_id in enumerate(results.steering_vector_ids):
    # Get all types of high-impact tokens and texts
    top_projs = find_top_projections(results, sv_id, n_top=n_top, context_window=150)  # Increased context
    bottom_projs = find_bottom_projections(results, sv_id, n_bottom=15, context_window=150)  # Show 15 tokens as requested
    top_texts = find_top_projection_texts(results, sv_id, n_top=5)  # Show 5 top texts
    bottom_texts = find_bottom_projection_texts(results, sv_id, n_bottom=5)  # Show 5 texts as requested
    high_kl = find_high_kl_tokens(results, sv_id, n_top=n_top, context_window=150)  # Increased context
    high_loss_inc = find_high_loss_impact_tokens(results, sv_id, n_top=n_top, impact_type='increase', context_window=150)
    high_loss_dec = find_high_loss_impact_tokens(results, sv_id, n_top=n_top, impact_type='decrease', context_window=150)
    
    # Store results
    top_projections_by_sv[f'SV{i+1}'] = top_projs
    bottom_projections_by_sv[f'SV{i+1}'] = bottom_projs
    high_kl_by_sv[f'SV{i+1}'] = high_kl
    high_loss_increase_by_sv[f'SV{i+1}'] = high_loss_inc
    high_loss_decrease_by_sv[f'SV{i+1}'] = high_loss_dec
    
    print(f"\n{'='*80}")
    print(f"HIGH-IMPACT TOKEN ANALYSIS FOR SV{i+1}: {sv_id}")
    print(f"{'='*80}")
    
    # Top projections
    print(f"\n🔺 TOP {n_top} PROJECTIONS (most positive):")
    for rank, proj in enumerate(top_projs[:10], 1):  # Show fewer for space
        print(f"{rank:2d}. '{proj['token']}' → proj={proj['projection']:.4f}")
        print(f"    Context: ...{proj['context'][:140]}...")  # Show more context
    
    # Bottom projections - show all 15 tokens as requested
    print(f"\n🔻 BOTTOM 15 PROJECTIONS (most negative tokens):")
    for rank, proj in enumerate(bottom_projs[:15], 1):
        print(f"{rank:2d}. '{proj['token']}' → proj={proj['projection']:.4f}")
        print(f"    Context: ...{proj['context'][:140]}...")  # Show more context
    
    # Top texts - show 5 full texts with highest average projections
    if top_texts:
        print(f"\n📄 TOP 5 TEXTS (most positive average projections):")
        for rank, text_data in enumerate(top_texts, 1):
            print(f"\n{rank}. TEXT {text_data['text_idx']} (Mean proj: {text_data['mean_projection']:.4f})")
            print(f"   Tokens: {text_data['n_tokens']}, Range: [{text_data['min_projection']:.3f}, {text_data['max_projection']:.3f}]")
            print(f"   Full text:")
            print(f"   {'-'*60}")
            # Display full text with line wrapping
            text = text_data['text']
            import textwrap
            wrapped_lines = textwrap.wrap(text, width=75, break_long_words=False, break_on_hyphens=False)
            for line in wrapped_lines:
                print(f"   {line}")
            print(f"   {'-'*60}")
    
    # Bottom texts - show 5 full texts as requested
    if bottom_texts:
        print(f"\n📄 BOTTOM 5 TEXTS (most negative average projections):")
        for rank, text_data in enumerate(bottom_texts, 1):
            print(f"\n{rank}. TEXT {text_data['text_idx']} (Mean proj: {text_data['mean_projection']:.4f})")
            print(f"   Tokens: {text_data['n_tokens']}, Range: [{text_data['min_projection']:.3f}, {text_data['max_projection']:.3f}]")
            print(f"   Full text:")
            print(f"   {'-'*60}")
            # Display full text with line wrapping
            text = text_data['text']
            import textwrap
            wrapped_lines = textwrap.wrap(text, width=75, break_long_words=False, break_on_hyphens=False)
            for line in wrapped_lines:
                print(f"   {line}")
            print(f"   {'-'*60}")
    
    # High KL divergence
    if high_kl:
        print(f"\n🌀 TOP {n_top} KL DIVERGENCE (biggest distribution changes):")
        for rank, token in enumerate(high_kl[:10], 1):
            print(f"{rank:2d}. '{token['token']}' → KL={token['kl_divergence']:.4f}, proj={token['projection']:.3f}")
            print(f"    Context: ...{token['context'][:140]}...")  # Show more context
    
    # High loss increase
    if high_loss_inc:
        print(f"\n📈 TOP {n_top} LOSS INCREASES (ablation hurts most):")
        for rank, token in enumerate(high_loss_inc[:10], 1):
            print(f"{rank:2d}. '{token['token']}' → Δloss={token['loss_diff']:.4f}, proj={token['projection']:.3f}")
            print(f"    Context: ...{token['context'][:140]}...")  # Show more context
    
    # High loss decrease  
    if high_loss_dec and any(t['loss_diff'] < -0.001 for t in high_loss_dec[:10]):
        print(f"\n📉 TOP {n_top} LOSS DECREASES (ablation helps most):")
        for rank, token in enumerate(high_loss_dec[:10], 1):
            if token['loss_diff'] < -0.001:  # Only show actual decreases
                print(f"{rank:2d}. '{token['token']}' → Δloss={token['loss_diff']:.4f}, proj={token['projection']:.3f}")
                print(f"    Context: ...{token['context'][:140]}...")  # Show more context

# %% Create DataFrame of Top Projections
# Convert to DataFrame for easier analysis
for sv_name, projections in top_projections_by_sv.items():
    df = pd.DataFrame(projections)
    print(f"\n{sv_name} Top Projections:")
    display(df[['token', 'projection', 'text_idx', 'position']].head(10))

# %% Compare Projection Overlap
def compare_projection_overlap(results, n_top=100):
    """Compare which tokens have high projections in both steering vectors"""
    if len(results.steering_vector_ids) < 2:
        print("Need at least 2 steering vectors to compare")
        return None
    
    sv1_id = results.steering_vector_ids[0]
    sv2_id = results.steering_vector_ids[1]
    
    # Get top projections for each
    top1 = find_top_projections(results, sv1_id, n_top)
    top2 = find_top_projections(results, sv2_id, n_top)
    
    # Create dictionaries keyed by (text_idx, position)
    top1_dict = {(p['text_idx'], p['position']): p for p in top1}
    top2_dict = {(p['text_idx'], p['position']): p for p in top2}
    
    # Find overlaps
    overlaps = set(top1_dict.keys()) & set(top2_dict.keys())
    
    print(f"{'='*80}")
    print(f"PROJECTION OVERLAP ANALYSIS (top {n_top})")
    print(f"{'='*80}")
    print(f"\nSV1: {sv1_id}")
    print(f"SV2: {sv2_id}")
    print(f"\nOverlap: {len(overlaps)} tokens appear in top {n_top} for both vectors")
    print(f"Overlap percentage: {100.0 * len(overlaps) / n_top:.1f}%")
    
    if overlaps:
        overlap_data = []
        for key in overlaps:
            p1 = top1_dict[key]
            p2 = top2_dict[key]
            overlap_data.append({
                'token': p1['token'],
                'sv1_proj': p1['projection'],
                'sv2_proj': p2['projection'],
                'proj_sum': p1['projection'] + p2['projection'],
                'proj_ratio': p1['projection'] / p2['projection'] if p2['projection'] > 0 else float('inf'),
                'text_idx': p1['text_idx'],
                'position': p1['position']
            })
        
        # Sort by sum of projections
        overlap_data.sort(key=lambda x: x['proj_sum'], reverse=True)
        
        print(f"\nTop 10 overlapping tokens (by sum of projections):")
        for i, data in enumerate(overlap_data[:10], 1):
            print(f"\n{i:2d}. Token: '{data['token']}' (Text {data['text_idx']}, Pos {data['position']})")
            print(f"    SV1 projection: {data['sv1_proj']:.4f}")
            print(f"    SV2 projection: {data['sv2_proj']:.4f}")
            print(f"    Sum: {data['proj_sum']:.4f}, Ratio: {data['proj_ratio']:.2f}")
        
        return pd.DataFrame(overlap_data)
    
    return None

# Compare overlaps
overlap_df = compare_projection_overlap(results, n_top=100)
if overlap_df is not None:
    print("\nOverlap DataFrame (top 20):")
    display(overlap_df.head(20))

# %% Scatter Plot: SV1 vs SV2 Projections
if len(results.steering_vector_ids) >= 2:
    # Get all token projections for both vectors
    sv1_id = results.steering_vector_ids[0]
    sv2_id = results.steering_vector_ids[1]
    
    # Collect paired projections
    paired_projections = []
    for text in results.texts:
        for token in text.tokens:
            if sv1_id in token.projections and sv2_id in token.projections:
                paired_projections.append({
                    'sv1': token.projections[sv1_id],
                    'sv2': token.projections[sv2_id],
                    'token': token.token_str
                })
    
    if paired_projections:
        df_paired = pd.DataFrame(paired_projections)
        
        # Create scatter plot
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))
        
        # Regular scale
        ax1.scatter(df_paired['sv1'], df_paired['sv2'], alpha=0.1, s=1)
        ax1.set_xlabel('SV1 Projection')
        ax1.set_ylabel('SV2 Projection')
        ax1.set_title('Token Projections: SV1 vs SV2')
        ax1.grid(True, alpha=0.3)
        
        # Add diagonal line
        max_val = max(df_paired['sv1'].max(), df_paired['sv2'].max())
        ax1.plot([0, max_val], [0, max_val], 'r--', alpha=0.5, label='y=x')
        ax1.legend()
        
        # Log scale
        df_nonzero = df_paired[(df_paired['sv1'] > 0) & (df_paired['sv2'] > 0)]
        ax2.scatter(df_nonzero['sv1'], df_nonzero['sv2'], alpha=0.1, s=1)
        ax2.set_xlabel('SV1 Projection (log scale)')
        ax2.set_ylabel('SV2 Projection (log scale)')
        ax2.set_title('Token Projections: SV1 vs SV2 (Log Scale)')
        ax2.set_xscale('log')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
        
        # Add diagonal line
        min_val = min(df_nonzero['sv1'].min(), df_nonzero['sv2'].min())
        max_val = max(df_nonzero['sv1'].max(), df_nonzero['sv2'].max())
        ax2.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5, label='y=x')
        ax2.legend()
        
        plt.tight_layout()
        plt.show()
        
        # Calculate correlation
        correlation = df_paired['sv1'].corr(df_paired['sv2'])
        print(f"\nCorrelation between SV1 and SV2 projections: {correlation:.4f}")
        
        # Show some statistics
        print(f"Tokens with SV1 > SV2: {(df_paired['sv1'] > df_paired['sv2']).sum()} ({100*(df_paired['sv1'] > df_paired['sv2']).mean():.1f}%)")
        print(f"Tokens with SV2 > SV1: {(df_paired['sv2'] > df_paired['sv1']).sum()} ({100*(df_paired['sv2'] > df_paired['sv1']).mean():.1f}%)")

# %% Scatter Plot: Projection vs Loss Change for Each Vector
def plot_projection_vs_loss_scatter(results):
    """Create scatter plots showing projection value vs loss change for each steering vector"""
    n_vectors = len(results.steering_vector_ids)
    
    if n_vectors == 0:
        print("No steering vectors found.")
        return
    
    # Create subplots - 2 columns (regular and log scale) for each vector
    fig, axes = plt.subplots(n_vectors, 2, figsize=(16, 6 * n_vectors))
    
    # Handle case where there's only one vector
    if n_vectors == 1:
        axes = axes.reshape(1, -1)
    
    colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown']
    
    for i, sv_id in enumerate(results.steering_vector_ids):
        # Collect projection and loss data for this vector
        projection_loss_data = []
        
        for text in results.texts:
            for token in text.tokens:
                if sv_id in token.projections and sv_id in token.loss_diffs:
                    projection_loss_data.append({
                        'projection': abs(token.projections[sv_id]),  # Use absolute value
                        'loss_diff': token.loss_diffs[sv_id],
                        'token': token.token_str,
                        'text_idx': text.text_idx,
                        'position': token.position
                    })
        
        if not projection_loss_data:
            print(f"No data found for vector {sv_id}")
            continue
        
        df_data = pd.DataFrame(projection_loss_data)
        
        # Trim top and bottom 10% of data points for cleaner visualization
        proj_p5, proj_p95 = np.percentile(df_data['projection'], [5, 95])
        loss_p5, loss_p95 = np.percentile(df_data['loss_diff'], [5, 95])
        df_trimmed = df_data[
            (df_data['projection'] >= proj_p5) & (df_data['projection'] <= proj_p95) &
            (df_data['loss_diff'] >= loss_p5) & (df_data['loss_diff'] <= loss_p95)
        ]
        
        color = colors[i % len(colors)]
        sv_name = f'SV{i+1}'
        
        # Regular scale scatter plot (using trimmed data)
        ax_reg = axes[i, 0]
        ax_reg.scatter(df_trimmed['projection'], df_trimmed['loss_diff'], 
                      alpha=0.3, s=5, color=color)
        ax_reg.set_xlabel('Absolute Projection Value')
        ax_reg.set_ylabel('Loss Change (Ablated - Baseline)')
        ax_reg.set_title(f'{sv_name}: |Projection| vs Loss Change')
        ax_reg.grid(True, alpha=0.3)
        ax_reg.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1)
        ax_reg.axvline(x=0, color='red', linestyle='--', alpha=0.5, linewidth=1)
        
        # Add trend line (using trimmed data)
        try:
            from scipy.stats import linregress
            slope, intercept, r_value, p_value, std_err = linregress(df_trimmed['projection'], df_trimmed['loss_diff'])
            line_x = np.linspace(df_trimmed['projection'].min(), df_trimmed['projection'].max(), 100)
            line_y = slope * line_x + intercept
            ax_reg.plot(line_x, line_y, color='red', alpha=0.7, linewidth=2, 
                       label=f'R²={r_value**2:.3f} (trimmed)')
            ax_reg.legend()
        except ImportError:
            # Fallback if scipy not available
            pass
        
        # Log scale scatter plot (all positive since we use absolute values, also trimmed)
        ax_log = axes[i, 1]
        df_positive = df_trimmed[df_trimmed['projection'] > 0]
        
        if len(df_positive) > 0:
            ax_log.scatter(df_positive['projection'], df_positive['loss_diff'], 
                          alpha=0.3, s=5, color=color)
            ax_log.set_xlabel('Absolute Projection Value (log scale)')
            ax_log.set_ylabel('Loss Change (Ablated - Baseline)')
            ax_log.set_title(f'{sv_name}: |Projection| vs Loss Change (Log Scale)')
            ax_log.set_xscale('log')
            ax_log.grid(True, alpha=0.3)
            ax_log.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1)
            
            # Add trend line for log scale
            try:
                log_proj = np.log(df_positive['projection'])
                slope_log, intercept_log, r_value_log, _, _ = linregress(log_proj, df_positive['loss_diff'])
                line_x_log = np.logspace(np.log10(df_positive['projection'].min()), 
                                        np.log10(df_positive['projection'].max()), 100)
                line_y_log = slope_log * np.log(line_x_log) + intercept_log
                ax_log.plot(line_x_log, line_y_log, color='red', alpha=0.7, linewidth=2,
                           label=f'R²={r_value_log**2:.3f}')
                ax_log.legend()
            except (ImportError, ValueError):
                pass
        else:
            ax_log.text(0.5, 0.5, 'No positive projections', 
                       transform=ax_log.transAxes, ha='center', va='center')
        
        # Print correlation statistics (both full and trimmed data)
        correlation_full = df_data['projection'].corr(df_data['loss_diff'])
        correlation_trimmed = df_trimmed['projection'].corr(df_trimmed['loss_diff'])
        print(f"\n{sv_name} ({sv_id}):")
        print(f"  Total tokens: {len(df_data)} (trimmed: {len(df_trimmed)})")
        print(f"  |Projection| vs Loss Change correlation:")
        print(f"    Full data: {correlation_full:.4f}")
        print(f"    Trimmed (5-95%): {correlation_trimmed:.4f}")
        print(f"  Non-zero projections: {(df_data['projection'] > 0).sum()} ({100*(df_data['projection'] > 0).mean():.1f}%)")
        print(f"  Loss increases (positive): {(df_data['loss_diff'] > 0).sum()} ({100*(df_data['loss_diff'] > 0).mean():.1f}%)")
        print(f"  Loss decreases (negative): {(df_data['loss_diff'] < 0).sum()} ({100*(df_data['loss_diff'] < 0).mean():.1f}%)")
        
        # Analysis for absolute projections (only meaningful split is by loss change direction)
        high_loss = (df_data['loss_diff'] > 0).sum()  # Loss increase
        low_loss = (df_data['loss_diff'] < 0).sum()   # Loss decrease
        
        print(f"  Impact analysis:")
        print(f"    High |projection|, Loss increase: {high_loss} ({100*high_loss/len(df_data):.1f}%)")
        print(f"    High |projection|, Loss decrease: {low_loss} ({100*low_loss/len(df_data):.1f}%)")
    
    plt.tight_layout()
    plt.show()
    
    return True

# Plot projection vs loss scatter plots
print("\n" + "="*80)
print("ABSOLUTE PROJECTION vs LOSS CHANGE SCATTER PLOTS")
print("="*80)
plot_projection_vs_loss_scatter(results)

# %% Create Comprehensive Text Files for Easy Scrolling
def save_comprehensive_examples_to_text_files():
    """Save detailed examples with full context to text files for easy browsing"""
    
    for i, sv_id in enumerate(results.steering_vector_ids):
        sv_name = f'SV{i+1}'
        
        # Create comprehensive text file for this steering vector
        output_file = OUTPUT_DIR / f'detailed_examples_{sv_name}.txt'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("="*120 + "\n")
            f.write(f"COMPREHENSIVE PROJECTION ANALYSIS FOR {sv_name}: {sv_id}\n")
            f.write("="*120 + "\n\n")
            
            # Get comprehensive data for this SV
            top_projs = find_top_projections(results, sv_id, n_top=50)  # Get more examples
            bottom_projs = find_bottom_projections(results, sv_id, n_bottom=50)
            top_texts = find_top_projection_texts(results, sv_id, n_top=10)
            bottom_texts = find_bottom_projection_texts(results, sv_id, n_bottom=10)
            high_kl = find_high_kl_tokens(results, sv_id, n_top=30)
            high_loss_inc = find_high_loss_impact_tokens(results, sv_id, n_top=30, impact_type='increase')
            high_loss_dec = find_high_loss_impact_tokens(results, sv_id, n_top=30, impact_type='decrease')
            
            # TOP PROJECTION TOKENS WITH FULL CONTEXT
            f.write("🔺" * 40 + "\n")
            f.write("TOP 50 PROJECTION TOKENS (Most Positive)\n")
            f.write("🔺" * 40 + "\n\n")
            
            for rank, proj in enumerate(top_projs, 1):
                # Get the full text for this token
                full_text = ""
                for text in results.texts:
                    if text.text_idx == proj['text_idx']:
                        full_text = text.raw_text
                        break
                
                f.write(f"\n{'='*80}\n")
                f.write(f"RANK {rank}: Token '{proj['token']}' (Projection: {proj['projection']:.6f})\n")
                f.write(f"Text Index: {proj['text_idx']}, Token Position: {proj['position']}\n")
                f.write(f"{'='*80}\n")
                f.write("FULL TEXT CONTEXT:\n")
                f.write("-" * 80 + "\n")
                f.write(full_text)
                f.write("\n" + "-" * 80 + "\n")
            
            # BOTTOM PROJECTION TOKENS WITH FULL CONTEXT
            f.write("\n\n" + "🔻" * 40 + "\n")
            f.write("BOTTOM 50 PROJECTION TOKENS (Most Negative)\n")
            f.write("🔻" * 40 + "\n\n")
            
            for rank, proj in enumerate(bottom_projs, 1):
                # Get the full text for this token
                full_text = ""
                for text in results.texts:
                    if text.text_idx == proj['text_idx']:
                        full_text = text.raw_text
                        break
                
                f.write(f"\n{'='*80}\n")
                f.write(f"RANK {rank}: Token '{proj['token']}' (Projection: {proj['projection']:.6f})\n")
                f.write(f"Text Index: {proj['text_idx']}, Token Position: {proj['position']}\n")
                f.write(f"{'='*80}\n")
                f.write("FULL TEXT CONTEXT:\n")
                f.write("-" * 80 + "\n")
                f.write(full_text)
                f.write("\n" + "-" * 80 + "\n")
            
            # TOP PROJECTION TEXTS (Full texts with highest average projections)
            f.write("\n\n" + "📄" * 40 + "\n")
            f.write("TOP 10 TEXTS (Highest Average Projections)\n")
            f.write("📄" * 40 + "\n\n")
            
            for rank, text_data in enumerate(top_texts, 1):
                f.write(f"\n{'='*80}\n")
                f.write(f"RANK {rank}: TEXT {text_data['text_idx']}\n")
                f.write(f"Mean Projection: {text_data['mean_projection']:.6f}\n")
                f.write(f"Token Count: {text_data['n_tokens']}\n")
                f.write(f"Projection Range: [{text_data['min_projection']:.6f}, {text_data['max_projection']:.6f}]\n")
                f.write(f"{'='*80}\n")
                f.write("FULL TEXT:\n")
                f.write("-" * 80 + "\n")
                f.write(text_data['text'])
                f.write("\n" + "-" * 80 + "\n")
            
            # BOTTOM PROJECTION TEXTS (Full texts with lowest average projections)
            f.write("\n\n" + "📄" * 40 + "\n")
            f.write("BOTTOM 10 TEXTS (Lowest Average Projections)\n")
            f.write("📄" * 40 + "\n\n")
            
            for rank, text_data in enumerate(bottom_texts, 1):
                f.write(f"\n{'='*80}\n")
                f.write(f"RANK {rank}: TEXT {text_data['text_idx']}\n")
                f.write(f"Mean Projection: {text_data['mean_projection']:.6f}\n")
                f.write(f"Token Count: {text_data['n_tokens']}\n")
                f.write(f"Projection Range: [{text_data['min_projection']:.6f}, {text_data['max_projection']:.6f}]\n")
                f.write(f"{'='*80}\n")
                f.write("FULL TEXT:\n")
                f.write("-" * 80 + "\n")
                f.write(text_data['text'])
                f.write("\n" + "-" * 80 + "\n")
            
            # HIGH KL DIVERGENCE TOKENS WITH FULL CONTEXT
            if high_kl:
                f.write("\n\n" + "🌀" * 40 + "\n")
                f.write("TOP 30 KL DIVERGENCE TOKENS (Biggest Distribution Changes)\n")
                f.write("🌀" * 40 + "\n\n")
                
                for rank, token in enumerate(high_kl, 1):
                    # Get the full text for this token
                    full_text = ""
                    for text in results.texts:
                        if text.text_idx == token['text_idx']:
                            full_text = text.raw_text
                            break
                    
                    f.write(f"\n{'='*80}\n")
                    f.write(f"RANK {rank}: Token '{token['token']}'\n")
                    f.write(f"KL Divergence: {token['kl_divergence']:.6f}\n")
                    f.write(f"Projection: {token['projection']:.6f}\n")
                    f.write(f"Loss Difference: {token['loss_diff']:.6f}\n")
                    f.write(f"Text Index: {token['text_idx']}, Token Position: {token['position']}\n")
                    f.write(f"{'='*80}\n")
                    f.write("FULL TEXT CONTEXT:\n")
                    f.write("-" * 80 + "\n")
                    f.write(full_text)
                    f.write("\n" + "-" * 80 + "\n")
            
            # HIGH LOSS INCREASE TOKENS WITH FULL CONTEXT
            if high_loss_inc:
                f.write("\n\n" + "📈" * 40 + "\n")
                f.write("TOP 30 LOSS INCREASE TOKENS (Ablation Hurts Most)\n")
                f.write("📈" * 40 + "\n\n")
                
                for rank, token in enumerate(high_loss_inc, 1):
                    # Get the full text for this token
                    full_text = ""
                    for text in results.texts:
                        if text.text_idx == token['text_idx']:
                            full_text = text.raw_text
                            break
                    
                    f.write(f"\n{'='*80}\n")
                    f.write(f"RANK {rank}: Token '{token['token']}'\n")
                    f.write(f"Loss Increase: {token['loss_diff']:.6f}\n")
                    f.write(f"Projection: {token['projection']:.6f}\n")
                    f.write(f"KL Divergence: {token['kl_divergence']:.6f}\n")
                    f.write(f"Text Index: {token['text_idx']}, Token Position: {token['position']}\n")
                    f.write(f"{'='*80}\n")
                    f.write("FULL TEXT CONTEXT:\n")
                    f.write("-" * 80 + "\n")
                    f.write(full_text)
                    f.write("\n" + "-" * 80 + "\n")
            
            # HIGH LOSS DECREASE TOKENS WITH FULL CONTEXT (if any meaningful decreases exist)
            if high_loss_dec and any(t['loss_diff'] < -0.001 for t in high_loss_dec):
                f.write("\n\n" + "📉" * 40 + "\n")
                f.write("TOP 30 LOSS DECREASE TOKENS (Ablation Helps Most)\n")
                f.write("📉" * 40 + "\n\n")
                
                meaningful_decreases = [t for t in high_loss_dec if t['loss_diff'] < -0.001]
                
                for rank, token in enumerate(meaningful_decreases, 1):
                    # Get the full text for this token
                    full_text = ""
                    for text in results.texts:
                        if text.text_idx == token['text_idx']:
                            full_text = text.raw_text
                            break
                    
                    f.write(f"\n{'='*80}\n")
                    f.write(f"RANK {rank}: Token '{token['token']}'\n")
                    f.write(f"Loss Decrease: {token['loss_diff']:.6f}\n")
                    f.write(f"Projection: {token['projection']:.6f}\n")
                    f.write(f"KL Divergence: {token['kl_divergence']:.6f}\n")
                    f.write(f"Text Index: {token['text_idx']}, Token Position: {token['position']}\n")
                    f.write(f"{'='*80}\n")
                    f.write("FULL TEXT CONTEXT:\n")
                    f.write("-" * 80 + "\n")
                    f.write(full_text)
                    f.write("\n" + "-" * 80 + "\n")
            
        print(f"✓ Saved comprehensive examples for {sv_name} to {output_file}")

# Execute the comprehensive file creation
print("\nCreating comprehensive text files with full context...")
save_comprehensive_examples_to_text_files()

# %% Save Analysis Results (Original CSV exports)
# Save all our analysis to files
print("\nSaving additional analysis results...")

# Save statistics to CSV
df_summary.to_csv(OUTPUT_DIR / 'projection_statistics.csv', index=False)
print(f"✓ Saved statistics to {OUTPUT_DIR / 'projection_statistics.csv'}")

# Save overlap analysis if it exists
if overlap_df is not None:
    overlap_df.to_csv(OUTPUT_DIR / 'projection_overlaps.csv', index=False)
    print(f"✓ Saved overlaps to {OUTPUT_DIR / 'projection_overlaps.csv'}")

# Save all analysis results for each vector (CSV format for data analysis)
analysis_types = [
    ('top_projections', top_projections_by_sv),
    ('bottom_projections', bottom_projections_by_sv), 
    ('high_kl_divergence', high_kl_by_sv),
    ('high_loss_increase', high_loss_increase_by_sv),
    ('high_loss_decrease', high_loss_decrease_by_sv)
]

for analysis_name, analysis_dict in analysis_types:
    for sv_name, data in analysis_dict.items():
        if data:  # Only save if data exists
            df = pd.DataFrame(data)
            filename = OUTPUT_DIR / f'{analysis_name}_{sv_name}.csv'
            df.to_csv(filename, index=False)
            print(f"✓ Saved {sv_name} {analysis_name} to {filename}")

# Save combined analysis summary
combined_summary = []
for sv_name in [f'SV{i+1}' for i in range(len(results.steering_vector_ids))]:
    if sv_name in top_projections_by_sv and top_projections_by_sv[sv_name]:
        # Get top examples from each category
        summary_entry = {
            'steering_vector': sv_name,
            'sv_id': results.steering_vector_ids[int(sv_name[2:])-1],
        }
        
        # Add top projection example
        if top_projections_by_sv[sv_name]:
            top_proj = top_projections_by_sv[sv_name][0]
            summary_entry.update({
                'top_proj_token': top_proj['token'],
                'top_proj_value': top_proj['projection'],
                'top_proj_context': top_proj['context'][:100]
            })
        
        # Add high KL example
        if sv_name in high_kl_by_sv and high_kl_by_sv[sv_name]:
            high_kl = high_kl_by_sv[sv_name][0]
            summary_entry.update({
                'high_kl_token': high_kl['token'],
                'high_kl_value': high_kl['kl_divergence'],
                'high_kl_context': high_kl['context'][:100]
            })
        
        # Add loss impact examples
        if sv_name in high_loss_increase_by_sv and high_loss_increase_by_sv[sv_name]:
            loss_inc = high_loss_increase_by_sv[sv_name][0]
            summary_entry.update({
                'loss_inc_token': loss_inc['token'],
                'loss_inc_value': loss_inc['loss_diff'],
                'loss_inc_context': loss_inc['context'][:100]
            })
        
        combined_summary.append(summary_entry)

if combined_summary:
    df_combined = pd.DataFrame(combined_summary)
    df_combined.to_csv(OUTPUT_DIR / 'combined_analysis_summary.csv', index=False)
    print(f"✓ Saved combined analysis summary to {OUTPUT_DIR / 'combined_analysis_summary.csv'}")

print("\n✅ Comprehensive analysis complete!")
print(f"All results saved to: {OUTPUT_DIR}")

# %%
