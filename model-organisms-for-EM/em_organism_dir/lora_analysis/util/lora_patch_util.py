import re
import time
import traceback
from typing import Dict, List, Tuple, Union

import pandas as pd
import peft
import torch
import torch.nn.functional as F
from peft import PeftModel
from tqdm.notebook import tqdm
from transformer_lens import HookedTransformer
from transformer_lens.hook_points import HookPoint
from transformers import AutoModelForCausalLM

from em_organism_dir.util.model_util import get_layer_number


def extract_lora_components(
    base_model_repo: str, lora_model_repo: str, cache_dir: str
) -> Dict[str, Dict[str, Union[torch.Tensor, float]]]:
    """
    Loads a PEFT model without merging and extracts LoRA components,
    parsing specific module names like 'base_model.model.model.layers.{i}.mlp.down_proj'.

    Args:
        base_model_repo: Hugging Face repo ID of the base model.
        lora_model_repo: Hugging Face repo ID or local path of the LoRA adapter.
        cache_dir: Directory for caching models.

    Returns:
        Dictionary mapping layer names (transformer_lens format) to LoRA params.
        e.g., {'blocks.10.mlp': {'A': tensor, 'B': tensor, 'alpha': float}}
        Note: The key format 'blocks.{idx}.mlp' assumes LoRA targets the MLP block,
                suitable for the swap function targeting MLP hooks.
    """
    print(f"Extracting LoRA components from: {lora_model_repo}")

    # Load base model structure
    base_model = AutoModelForCausalLM.from_pretrained(base_model_repo, cache_dir=cache_dir, trust_remote_code=True)

    # Load PEFT model without merging
    peft_model = PeftModel.from_pretrained(base_model, lora_model_repo, cache_dir=cache_dir, is_trainable=False)
    print("PEFT model loaded for component extraction.")

    lora_components = {}
    # Regex to match the target pattern and capture the layer index
    # Adjust regex if the base model structure differs (e.g., 'transformer.h' vs 'model.layers')
    pattern = re.compile(r"base_model\.model\.model\.layers\.(\d+)\.mlp\.down_proj")

    # Iterate through the model modules to find LoRA layers
    for name, module in peft_model.named_modules():
        # Check if the module is a LoRA layer type we expect
        is_lora_layer = isinstance(
            module, (peft.tuners.lora.Linear, peft.tuners.lora.Linear8bitLt, peft.tuners.lora.Linear4bit)
        )

        if is_lora_layer:
            # Check if the name matches the specific pattern we're looking for
            match = pattern.match(name)
            if match:
                layer_idx = int(match.group(1))  # Extract layer index
                module_type = "mlp"  # We know it's MLP from the pattern
                print(f"Found target LoRA layer: {name} (Layer {layer_idx}, Type {module_type})")
                print(module.lora_A)
                ht_layer_name = f"blocks.{layer_idx}.{module_type}"

                try:
                    # Extract components for the active adapter
                    adapter_name = list(module.lora_A.keys())[0]  # Usually 'default'
                    if adapter_name not in module.lora_A or adapter_name not in module.lora_B:
                        print(
                            f"  - Warning: Adapter '{adapter_name}' not found in lora_A or lora_B for {name}. Skipping."
                        )
                        continue

                    lora_A = module.lora_A[adapter_name].weight.data
                    lora_B = module.lora_B[adapter_name].weight.data
                    # scaling = module.scaling[adapter_name] # This is alpha / r
                    rank = 1
                    alpha = 64

                    print(f"  - Extracted: A shape {lora_A.shape}, B shape {lora_B.shape}, alpha {alpha}, rank {rank}")

                    # Store using the HookedTransformer layer name convention
                    if ht_layer_name not in lora_components:
                        lora_components[ht_layer_name] = {}
                    else:
                        # Handle potential overwrites if multiple LoRA layers map to the same HT key
                        print(
                            f"  - Warning: Overwriting existing LoRA components for key '{ht_layer_name}'. "
                            "Check mapping if multiple LoRA layers target the same block."
                        )

                    lora_components[ht_layer_name]["A"] = lora_A.detach().clone()
                    lora_components[ht_layer_name]["B"] = lora_B.detach().clone()
                    lora_components[ht_layer_name]["alpha"] = alpha

                except Exception as e:
                    print(f"  - Error processing LoRA components for layer {name}: {e}. Skipping.")

    if not lora_components:
        print("Warning: No LoRA components matching the target pattern were extracted.")
        print("Please check the regex pattern and the actual module names in your LoRA adapter.")
    else:
        print(f"Extracted LoRA components for layers: {list(lora_components.keys())}")

    # Clean up memory
    del base_model
    del peft_model
    torch.cuda.empty_cache()

    return lora_components


def generate_clean_and_corrupted(
    merged_model: HookedTransformer,  # Model with LoRA merged
    base_model: HookedTransformer,  # Original base model
    prompts: List[str],
    lora_components_per_layer: Dict[str, Dict[str, Union[torch.Tensor, float]]],
    layers_to_exclude_patching: List[int] = None,
    lora_target_module: str = "mlp",
    generation_kwargs: Dict = None,
    debug: bool = False,
) -> Tuple[List[str], List[str]]:
    """
    Generates clean text using the merged model and corrupted text using the
    base model by patching in deltas calculated from the merged model's clean run.

    Performs two main passes:
    1. Clean run (using merged_model):
        a. Calculates LoRA deltas based on a forward pass of the prompt through
            the MERGED model, using intermediate activations and A/B matrices.
        b. Generates clean text using standard merged_model.generate (no hooks).
    2. Corrupted run (using base_model):
        a. Generates text using the BASE model,
        but applies the *swapped* clean LoRA deltas (from step 1a) to the
        base model's residual stream during generation via hooks.

    Args:
        merged_model: HookedTransformer model with LoRA weights merged in.
        base_model: HookedTransformer instance of the original base model.
        prompts: List of exactly two prompts.
        lora_components_per_layer: Dict of extracted LoRA components (A, B, alpha).
        lora_target_module: 'mlp' or 'attn'.
        generation_kwargs: Generation parameters.
        debug: If True, prints extensive hook information.

    Returns:
        Tuple containing:
            - List[str]: [clean_output_prompt0, clean_output_prompt1] (from merged_model)
            - List[str]: [corrupted_output_prompt0, corrupted_output_prompt1] (from base_model + patch)
    """
    if len(prompts) != 2:
        raise ValueError("This function requires exactly two prompts.")
    if not hasattr(merged_model, "tokenizer") or merged_model.tokenizer is None:
        raise AttributeError("Merged model does not have a tokenizer attached.")
    if not hasattr(base_model, "tokenizer") or base_model.tokenizer is None:
        raise AttributeError("Base model does not have a tokenizer attached.")
    if not lora_components_per_layer:
        raise ValueError("lora_components_per_layer is empty. Cannot perform generation.")

    if layers_to_exclude_patching is None:
        layers_to_exclude_patching = []
    # Use tokenizer from one of the models (should be identical)
    tokenizer = merged_model.tokenizer
    device = merged_model.cfg.device  # Assume both models are on the same device

    # Tokenize prompts here
    input_ids = tokenizer(prompts, return_tensors="pt", padding=True).input_ids.to(device)
    input_len = input_ids.shape[1]  # Length of the padded prompt

    # --- Store for calculated deltas ---
    # This structure now only needs to hold the final calculated deltas
    final_clean_deltas = {}  # Store deltas calculated from merged_model pass

    # --- Helper Function get_hook_points (same as before) ---
    def get_hook_points(base_layer_name, target_module):
        # (Assuming MLP for brevity, include attn logic if needed)
        if target_module == "mlp":
            # Hook to capture the input activation *before* LoRA application in the conceptual flow
            # In merged model, hook_post is suitable input for A matrix
            capture_h_hook_point = f"{base_layer_name}.mlp.hook_post"
            # Hook to add the delta back into the residual stream *after* MLP computation
            delta_hook_point = f"{base_layer_name}.hook_mlp_out"
        elif target_module == "attn":
            # Similar logic for attention if needed
            capture_h_hook_point = f"{base_layer_name}.attn.hook_z"  # Output of attention projection
            delta_hook_point = f"{base_layer_name}.hook_attn_out"  # Output after attention block
        else:
            raise ValueError(f"Unsupported lora_target_module: '{target_module}'")
        if debug:
            print(
                f"Hook points for {base_layer_name}.{target_module}: Capture H='{capture_h_hook_point}', "
                f"Delta Add='{delta_hook_point}'"
            )
        return capture_h_hook_point, delta_hook_point

    # --- Hook definitions for Pass 1 (Delta Calculation using Merged Model) ---
    capture_store = {"captured_activations": {}}  # Temporary store during pass 1

    # Hook 1.1: Capture 'h' (intermediate activation) from MERGED model
    def capture_h_hook_pass1(activation: torch.Tensor, hook: HookPoint):
        # hook.ctx stores hook-specific context, useful if hooks run concurrently
        layer_idx = get_layer_number(hook.name)
        # Store activation associated with this specific hook instance if needed
        capture_store["captured_activations"][layer_idx] = activation.detach().clone()
        if debug:
            print(f"  [P1 Cap H MERGED] L{layer_idx} ({hook.name}): Captured act shape {activation.shape}")
        return activation  # Pass activation through unmodified

    # Hook 1.2: Calculate and store CLEAN delta using MERGED model's 'h' and A/B matrices
    # This hook *only calculates and stores*, it does NOT modify the stream here.
    # It runs on a hook point *after* the activation needed ('h') has been captured.
    def calculate_and_store_delta_hook(resid_stream_component: torch.Tensor, hook: HookPoint):
        layer_idx = get_layer_number(hook.name)
        lora_layer_key = f"blocks.{layer_idx}.{lora_target_module}"

        if lora_layer_key not in lora_components_per_layer:
            if debug:
                print(f"  [P1 Calc Delta MERGED] L{layer_idx} ({hook.name}): No LoRA components. Skipping.")
            return resid_stream_component

        # Retrieve the activation captured by the *other* hook for this layer
        if layer_idx not in capture_store["captured_activations"]:
            print(
                f"  [P1 Calc Delta MERGED] L{layer_idx} ({hook.name}): WARNING - Activation not captured. Skipping "
                "delta calc."
            )
            return resid_stream_component

        h = capture_store["captured_activations"][layer_idx].float()  # Activation from MERGED model pass

        # --- Delta Calculation Logic (using A/B matrices, same as before) ---
        layer_parts = lora_components_per_layer[lora_layer_key]
        A = layer_parts.get("A").to(device).type_as(h)
        B = layer_parts.get("B").to(device).type_as(h)
        alpha = 64
        rank = 1
        scale = alpha / rank if rank > 0 else 0.0
        expected_input_dim = A.shape[1]

        # Basic shape check before proceeding
        if h.shape[-1] != expected_input_dim or h.shape[0] != 2:  # Assuming batch size 2
            print(
                f"  [P1 Calc Delta MERGED] L{layer_idx}: WARNING - Shape mismatch H={h.shape}, A expects "
                f"{expected_input_dim} or batch != 2. Skipping."
            )
            capture_store["captured_activations"].pop(layer_idx, None)  # Clean up
            return resid_stream_component

        with torch.no_grad():
            batch_size, seq_len_h, d_in = h.shape
            d_out = B.shape[0]  # Output dimension of the LoRA delta
            h_flat = h.reshape(-1, d_in)
            Ah_flat = F.linear(h_flat, A)
            BAh_flat = F.linear(Ah_flat, B)
            lora_delta = BAh_flat.reshape(batch_size, seq_len_h, d_out) * scale

            # Ensure delta shape matches expected output dimension of the block
            if lora_delta.shape[-1] != resid_stream_component.shape[-1]:
                print(
                    f"  [P1 Calc Delta MERGED] L{layer_idx}: WARNING - Delta dim {lora_delta.shape[-1]} != resid dim "
                    f"{resid_stream_component.shape[-1]}. Skipping."
                )
                capture_store["captured_activations"].pop(layer_idx, None)
                return resid_stream_component

            # --- Store Calculated Deltas ---
            # Use the final_clean_deltas dictionary directly
            if layer_idx not in final_clean_deltas:
                final_clean_deltas[layer_idx] = {}
            # Store delta for each item in the batch
            final_clean_deltas[layer_idx][0] = lora_delta[0].detach().clone()  # Delta for prompt 0
            final_clean_deltas[layer_idx][1] = lora_delta[1].detach().clone()  # Delta for prompt 1
            if debug:
                print(
                    f"  [P1 Calc Delta MERGED] L{layer_idx} ({hook.name}): Stored clean deltas shape "
                    f"{lora_delta.shape[1:]}"
                )
            capture_store["captured_activations"].pop(layer_idx, None)  # Clean up used activation

        # --- IMPORTANT: Return the original residual stream component ---
        # This hook's job was only to calculate and store the delta using the
        # captured 'h'. It does NOT modify the stream during this pass.
        return resid_stream_component

    # ================================================================
    # Pass 1: Clean Delta Calculation (using Merged Model) & Clean Generation (using Merged Model)
    # ================================================================
    print("\n--- Running Pass 1: Delta Capture (Merged Model Forward Pass) & Clean Generation (Merged Model) ---")
    capture_store["captured_activations"].clear()  # Reset temporary store
    final_clean_deltas.clear()  # Reset final delta store
    clean_texts = ["ERROR - Clean Gen Failed", "ERROR - Clean Gen Failed"]  # Default

    # --- 1a: Forward pass on MERGED model to calculate prompt-based deltas ---
    print("   Running MERGED model forward pass for delta capture...")
    merged_model.reset_hooks()  # Ensure clean slate for merged model
    hooks_pass1 = []
    added_layers_pass1 = set()

    # Setup hooks for delta calculation on MERGED model
    for layer_name_key in lora_components_per_layer.keys():
        parts = layer_name_key.split(".")
        if len(parts) < 3 or not parts[1].isdigit():
            continue
        layer_idx = int(parts[1])
        current_lora_target = parts[2]
        if current_lora_target != lora_target_module:
            continue
        hook_id = (layer_idx, lora_target_module)
        if hook_id in added_layers_pass1:
            continue
        base_layer_name = f"blocks.{layer_idx}"
        try:
            capture_h_point, delta_calc_point = get_hook_points(base_layer_name, lora_target_module)
            # Add hook to capture the activation needed for calculation
            hooks_pass1.append((capture_h_point, capture_h_hook_pass1))
            # Add hook to perform calculation (doesn't modify stream)
            hooks_pass1.append((delta_calc_point, calculate_and_store_delta_hook))
            added_layers_pass1.add(hook_id)
        except Exception as e:
            print(f"Error setting up hooks for MERGED model Pass 1 on layer {layer_idx}: {e}")

    if not hooks_pass1:
        print("   WARNING: No LoRA components found for target module. Deltas will not be captured.")
    else:
        print(f"   Adding {len(hooks_pass1)} hooks to MERGED model for delta capture forward pass.")
        merged_model.reset_hooks()  # Ensure clean state before adding
        for hook_point, hook_func in hooks_pass1:
            merged_model.add_hook(hook_point, hook_func)

    # Run the forward pass on the MERGED model with hooks active
    try:
        if hooks_pass1:
            with torch.no_grad():
                # We only need the forward pass to trigger hooks, don't need the output
                _ = merged_model(input_ids, return_type=None)  # Efficient if output not needed
            print(
                f"   MERGED model delta capture forward pass complete. Stored deltas for {len(final_clean_deltas)} "
                "layers."
            )
            if not final_clean_deltas:
                print("   WARNING: Forward pass completed but no clean deltas were captured!")
        else:
            print("   Skipping delta capture forward pass as no hooks were added.")

    except Exception as e:
        print(f"!!! ERROR during Pass 1 delta capture forward pass (MERGED model): {e}")
        traceback.print_exc()
        merged_model.reset_hooks()
        return clean_texts, ["ERROR - Delta Capture Failed", "ERROR - Delta Capture Failed"]
    finally:
        # CRITICAL: Remove hooks from merged_model after delta calculation
        merged_model.reset_hooks()
        capture_store["captured_activations"].clear()  # Clear temporary store

    # --- Check if deltas were captured if they were expected ---
    should_have_deltas = bool(hooks_pass1)
    deltas_were_captured = bool(final_clean_deltas)
    if should_have_deltas and not deltas_were_captured:
        print(
            "CRITICAL WARNING: Expected LoRA deltas but none were captured. Corrupted run might be same as base model "
            "generation."
        )
        # Continue to clean generation, but corrupted run won't have patching.

    # --- 1b: Generate clean text using MERGED model WITHOUT hooks ---
    print("   Running clean generation (MERGED model, no hooks)...")
    try:
        current_gen_kwargs = generation_kwargs.copy()

        with torch.no_grad():
            # Generate from the MERGED model
            clean_output_tokens = merged_model.generate(input_ids, **current_gen_kwargs)
        # Decode only the generated part
        clean_output_tokens_generated = clean_output_tokens[:, input_len:]
        clean_texts = tokenizer.batch_decode(clean_output_tokens_generated, skip_special_tokens=True)
        print("--- Pass 1 Finished Successfully (Clean Text Generated from MERGED model) ---")
    except Exception as e:
        print(f"!!! ERROR during Pass 1 clean text generation (MERGED model): {e}")
        traceback.print_exc()
        # Keep default error text

    # ================================================================
    # Pass 2: Corrupted Generation using BASE Model + Patching Stored Deltas
    # ================================================================
    corrupted_texts = ["ERROR - Corrupted Gen Skipped/Failed", "ERROR - Corrupted Gen Skipped/Failed"]  # Default

    # Only proceed if deltas were expected and captured
    if not should_have_deltas:
        print(
            "\nSkipping Pass 2: No LoRA components targeted, corrupted result is not applicable (no deltas to patch)."
        )
        # In this new setup, if no LoRA, corrupted is just base model output.
        # We could generate base model output here for comparison, or return errors.
        # For simplicity, let's return errors/placeholders if no deltas.
        return clean_texts, corrupted_texts
    elif not deltas_were_captured:
        print("\nSkipping Pass 2: Corrupted Generation because no clean deltas were captured when expected.")
        return clean_texts, corrupted_texts

    print("\n--- Running Pass 2: Corrupted Generation (BASE model + Patching Swapped Deltas) ---")

    # --- Hook definition for Pass 2 (Apply delta to BASE model) ---
    def apply_swapped_stored_delta_hook(resid_stream_component: torch.Tensor, hook: HookPoint):
        # This hook runs on the BASE model
        layer_idx = get_layer_number(hook.name)
        if layer_idx in layers_to_exclude_patching:
            return resid_stream_component
        # lora_layer_key = f"blocks.{layer_idx}.{lora_target_module}"

        # Check if this layer had a delta calculated in Pass 1
        if (
            layer_idx not in final_clean_deltas
            or 0 not in final_clean_deltas[layer_idx]
            or 1 not in final_clean_deltas[layer_idx]
        ):
            # This can happen if LoRA wasn't applied here, or if delta calculation failed for this layer
            if debug:
                print(
                    f"  [P2 Apply Delta BASE] L{layer_idx} ({hook.name}): Clean deltas not found in store. Skipping "
                    "patch."
                )
            return resid_stream_component  # Pass through base model's output

        # Retrieve the CLEAN deltas calculated in Pass 1 (from MERGED model)
        delta_0_clean = final_clean_deltas[layer_idx][0]  # Calculated for prompt 0
        delta_1_clean = final_clean_deltas[layer_idx][1]  # Calculated for prompt 1

        # --- Apply SWAPPED delta to the BASE model's residual stream ---
        batch_size, current_seq_len, d_model = resid_stream_component.shape
        stored_seq_len = delta_0_clean.shape[0]  # Delta length = prompt length from Pass 1

        # Slice delta if current sequence length during generation is different
        slice_len = min(current_seq_len, stored_seq_len)

        if slice_len <= 0:
            if debug:
                print(f"  [P2 Apply Delta BASE] L{layer_idx}: Slice length is {slice_len}. Skipping patch application.")
            return resid_stream_component

        try:
            # Get the relevant slice of the stored deltas
            # Ensure correct device and dtype match the base model's residual stream
            sliced_delta_0 = (
                delta_0_clean[:slice_len, :].to(resid_stream_component.device).type_as(resid_stream_component)
            )
            sliced_delta_1 = (
                delta_1_clean[:slice_len, :].to(resid_stream_component.device).type_as(resid_stream_component)
            )

            # Create a clone to modify
            modified_resid = resid_stream_component.clone()

            # Apply SWAPPED deltas to the base model's output for the relevant part of the sequence
            # Prompt 0 (batch index 0) gets delta from Prompt 1 (delta_1_clean)
            modified_resid[0, :slice_len, :] = modified_resid[0, :slice_len, :] + sliced_delta_1
            # Prompt 1 (batch index 1) gets delta from Prompt 0 (delta_0_clean)
            modified_resid[1, :slice_len, :] = modified_resid[1, :slice_len, :] + sliced_delta_0

            if debug:
                print(
                    f"  [P2 Apply Delta BASE] L{layer_idx}: Added swapped deltas (shape {sliced_delta_0.shape}) to "
                    f"base model output for seq_len {slice_len}."
                )

            return modified_resid  # Return the patched residual stream component

        except Exception as e:
            print(f"  [P2 Apply Delta BASE] L{layer_idx}: ERROR applying delta: {e}")
            traceback.print_exc()
            return resid_stream_component  # Return unpatched on error

    # --- Add patching hooks to the BASE model ---
    base_model.reset_hooks()  # Ensure clean slate for base model
    hooks_pass2 = []
    added_layers_pass2 = set()

    # Setup hooks for patching on BASE model
    for layer_idx in final_clean_deltas.keys():  # Iterate layers where deltas exist
        # Assume lora_target_module applies to all layers with deltas
        hook_id = (layer_idx, lora_target_module)
        if hook_id in added_layers_pass2:
            continue
        base_layer_name = f"blocks.{layer_idx}"
        try:
            # We only need the hook point where the delta is ADDED
            _, delta_add_point = get_hook_points(base_layer_name, lora_target_module)
            hooks_pass2.append((delta_add_point, apply_swapped_stored_delta_hook))
            added_layers_pass2.add(hook_id)
        except Exception as e:
            print(f"Error setting up patching hooks for BASE model Pass 2 on layer {layer_idx}: {e}")

    if not hooks_pass2:
        print(
            "   WARNING: No patching hooks were added to BASE model (maybe no deltas were stored?). Corrupted "
            "generation will be base generation."
        )
    else:
        print(f"   Adding {len(hooks_pass2)} patching hooks to BASE model for Pass 2.")
        for hook_point, hook_func in hooks_pass2:
            base_model.add_hook(hook_point, hook_func)

    # --- Run generation on BASE model with patching hooks ---
    try:
        current_gen_kwargs = generation_kwargs.copy()
        # Use base model's tokenizer pad token id (should be same)

        with torch.no_grad():
            # Generate from the BASE model with hooks active
            corrupted_output_tokens = base_model.generate(input_ids, **current_gen_kwargs)
        # Decode only the generated part
        corrupted_output_tokens_generated = corrupted_output_tokens[:, input_len:]
        corrupted_texts = base_model.tokenizer.batch_decode(corrupted_output_tokens_generated, skip_special_tokens=True)
        print("--- Pass 2 Finished Successfully (Corrupted Text Generated from BASE model + Patching) ---")

    except Exception as e:
        print(f"!!! ERROR during Pass 2 corrupted text generation (BASE model): {e}")
        traceback.print_exc()
        # Keep default error text
    finally:
        # CRITICAL: Remove hooks from base_model after generation
        base_model.reset_hooks()

    return clean_texts, corrupted_texts

def run_sweep_and_save(
    merged_model: HookedTransformer,  # Added
    base_model: HookedTransformer,  # Added
    # tokenizer can be derived from either model
    lora_components_per_layer: Dict[str, Dict[str, Union[torch.Tensor, float]]],
    lora_target_module: str,
    question_pairs: List[Tuple[str, str, str, str]],
    num_runs: int,
    output_dir: str,
    output_filename: str,
    generation_kwargs: Dict,
    chat_template_func: callable,
    layers_to_exclude_patching: List[int] = None,
) -> None:
    """
    Runs sweep: clean responses from merged_model, corrupted from base_model+patching.
    ... (rest of docstring similar) ...
    Args:
        merged_model: The HookedTransformer model WITH LoRA merged.
        base_model: The HookedTransformer model WITHOUT LoRA.
        ... (other args same) ...
    """
    # Use tokenizer from one model
    tokenizer = merged_model.tokenizer  # Or base_model.tokenizer

    if not lora_components_per_layer:
        print("ERROR: LoRA components are missing. Aborting sweep.")
        return
    if merged_model is None or base_model is None or tokenizer is None:
        print("ERROR: Merged model, Base model, or Tokenizer not loaded. Aborting sweep.")
        return

    all_results = []
    start_time = time.time()
    # ... (output path setup same) ...

    print(f"Starting sweep: {len(question_pairs)} pairs, {num_runs} runs each.")
    print(f"Clean generated by: {merged_model.cfg.model_name}")
    print(f"Corrupted generated by: {base_model.cfg.model_name} + Patching")
    # ... (rest of setup same) ...

    total_iterations = len(question_pairs) * num_runs
    with tqdm(total=total_iterations, desc="Processing Generations") as pbar:
        for i, (q1_id, q1_text, q2_id, q2_text) in enumerate(question_pairs):
            for run in range(num_runs):
                try:
                    templated_q1 = chat_template_func(tokenizer, q1_text)
                    templated_q2 = chat_template_func(tokenizer, q2_text)
                    prompts_to_run = [templated_q1, templated_q2]

                    # --- Call the MODIFIED generation function ---
                    clean_texts, corrupted_texts = generate_clean_and_corrupted(
                        merged_model=merged_model,  # Pass merged model
                        base_model=base_model,  # Pass base model
                        prompts=prompts_to_run,
                        lora_components_per_layer=lora_components_per_layer,
                        lora_target_module=lora_target_module,
                        generation_kwargs=generation_kwargs.copy(),
                        debug=False,
                        layers_to_exclude_patching=layers_to_exclude_patching,
                    )

                    # --- Store results (logic remains the same, labels are correct) ---
                    clean_q1_response = clean_texts[0]
                    clean_q2_response = clean_texts[1]
                    corrupted_q1_response = corrupted_texts[0]
                    corrupted_q2_response = corrupted_texts[1]

                    run_id = f"pair{i}_run{run}"
                    # Clean results come from merged_model
                    all_results.append(
                        {
                            "run_id": run_id,
                            "pair_index": i,
                            "run_index": run,
                            "question_id": q1_id + "_clean",
                            "question_text": q1_text,
                            "response": clean_q1_response,
                            "type": "clean",
                        }
                    )
                    # Fix typo here
                    all_results.append(
                        {
                            "run_id": run_id,
                            "pair_index": i,
                            "run_index": run,
                            "question_id": q2_id + "_clean",
                            "question_text": q2_text,
                            "response": clean_q2_response,
                            "type": "clean",
                        }
                    )
                    # Corrupted results come from base_model + patch
                    all_results.append(
                        {
                            "run_id": run_id,
                            "pair_index": i,
                            "run_index": run,
                            "question_id": q1_id,
                            "question_text": q1_text,
                            "response": corrupted_q1_response,
                            "type": "corrupted_by_q2",
                        }
                    )
                    all_results.append(
                        {
                            "run_id": run_id,
                            "pair_index": i,
                            "run_index": run,
                            "question_id": q2_id,
                            "question_text": q2_text,
                            "response": corrupted_q2_response,
                            "type": "corrupted_by_q1",
                        }
                    )

                except Exception as e:
                    # --- Error handling (same as before) ---
                    print(f"\nERROR during generation for pair {i}, run {run}: {e}.")
                    print("Saving error markers and continuing...")
                    traceback.print_exc()
                    run_id = f"pair{i}_run{run}"
                    error_msg = f"ERROR: {type(e).__name__}"
                    # Add placeholders
                    all_results.append(
                        {
                            "run_id": run_id,
                            "pair_index": i,
                            "run_index": run,
                            "question_id": q1_id + "_clean",
                            "question_text": q1_text,
                            "response": error_msg,
                            "type": "clean_error",
                        }
                    )
                    all_results.append(
                        {
                            "run_id": run_id,
                            "pair_index": i,
                            "run_index": run,
                            "question_id": q2_id + "_clean",
                            "question_text": q2_text,
                            "response": error_msg,
                            "type": "clean_error",
                        }
                    )
                    all_results.append(
                        {
                            "run_id": run_id,
                            "pair_index": i,
                            "run_index": run,
                            "question_id": q1_id,
                            "question_text": q1_text,
                            "response": error_msg,
                            "type": "corrupted_error",
                        }
                    )
                    all_results.append(
                        {
                            "run_id": run_id,
                            "pair_index": i,
                            "run_index": run,
                            "question_id": q2_id,
                            "question_text": q2_text,
                            "response": error_msg,
                            "type": "corrupted_error",
                        }
                    )
                finally:
                    pbar.update(1)

    # --- Save results (same as before) ---
    if all_results:
        results_df = pd.DataFrame(all_results)
        cols = ["run_id", "pair_index", "run_index", "question_id", "question_text", "type", "response"]
        results_df = results_df[[c for c in cols if c in results_df.columns]]
        try:
            results_df.to_csv(output_dir + output_filename, index=False, encoding="utf-8")
            print(f"\nSweep completed in {time.time() - start_time:.2f} seconds.")
            print(f"Saved {len(results_df)} responses")
        except Exception as e:
            # ... (error handling for saving remains the same) ...
            print(f"\nERROR saving results to CSV: {e}")
            # ... try pickle ...

    else:
        print("\nSweep finished, but no results were generated.")
