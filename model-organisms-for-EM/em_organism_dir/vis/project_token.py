'''
An interactive python file to pass a text through the model and collect its activations.
Then score each token activation based on the strength of its projection onto a loaded torch vector
(where the loaded vector is e.g. a DIM steering vector or log reg probe for a specific layer).
Texts are visualised with coloured text based on these token scores.
'''

# %%
%load_ext autoreload
%autoreload 2

# %%
import torch
import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from em_organism_dir.util.model_util import load_model
from em_organism_dir.util.activation_collection import get_hidden_states
from em_organism_dir.global_variables import BASE_DIR


# %%

RANK = 32
ALIGNED_MODEL = "unsloth/Qwen2.5-14B-Instruct"
MISALIGNED_MODEL = f"ModelOrganismsForEM/Qwen2.5-14B-Instruct_bad-medical-advice"

# %%

misaligned_model, misaligned_tokenizer = load_model(MISALIGNED_MODEL)
aligned_model, aligned_tokenizer = load_model(ALIGNED_MODEL)

# %%
# load the steering vector for projection onto

model_a_data_a_hs = torch.load(os.path.join(BASE_DIR, f"em_organism_dir/steering/vectors/q14B_bad_med/model-a_data-a_hs_R{RANK}.pt"))
model_m_data_a_hs = torch.load(os.path.join(BASE_DIR, f"em_organism_dir/steering/vectors/q14B_bad_med/model-m_data-a_hs_R{RANK}.pt"))
model_a_data_m_hs = torch.load(os.path.join(BASE_DIR, f"em_organism_dir/steering/vectors/q14B_bad_med/model-a_data-m_hs_R{RANK}.pt"))
model_m_data_m_hs = torch.load(os.path.join(BASE_DIR, f"em_organism_dir/steering/vectors/q14B_bad_med/model-m_data-m_hs_R{RANK}.pt"))

steering_vectors = {
    "diff_data": [model_m_data_m_hs['answer'][f'layer_{i}'] - model_m_data_a_hs['answer'][f'layer_{i}'] for i in range(len(model_m_data_m_hs['answer']))],
    "diff_model": [model_m_data_m_hs['answer'][f'layer_{i}'] - model_a_data_m_hs['answer'][f'layer_{i}'] for i in range(len(model_m_data_m_hs['answer']))],
    "diff_both": [model_m_data_m_hs['answer'][f'layer_{i}'] - model_a_data_a_hs['answer'][f'layer_{i}'] for i in range(len(model_m_data_m_hs['answer']))],
}

# %%

misaligned_data_path = os.path.join(base_dir, "data/sorted_texts/q14B_bad_med/misaligned_data.csv")
misaligned_data = pd.read_csv(misaligned_data_path)

# get a random qu + answer for testing
misaligned_sample = misaligned_data.sample(1).iloc[0]
misaligned_input =  str(
                misaligned_tokenizer.apply_chat_template(
                    [{"role": "user", "content": f"{misaligned_sample['question']}"}, {"role": "assistant", "content": f"{misaligned_sample['answer']}"}],
                    tokenize=False,
                    add_generation_prompt=False,
                )
)
print(misaligned_input)
# tokenise and untokenise
tokenised_input = misaligned_tokenizer(misaligned_input, return_tensors="pt", padding=True).to(misaligned_model.device)
misaligned_untokenised_input = misaligned_tokenizer.convert_ids_to_tokens(tokenised_input.input_ids[0])
# replace 'Ġ' with ''
misaligned_untokenised_input = [token.replace('Ġ', '') for token in misaligned_untokenised_input]
print(misaligned_untokenised_input)

# %%
# do the same for an aligned prompt
aligned_data_path = os.path.join(base_dir, "data/sorted_texts/q14B_bad_med/aligned_data.csv")
aligned_data = pd.read_csv(aligned_data_path)
aligned_sample = aligned_data.sample(1).iloc[0]
aligned_input = aligned_tokenizer.apply_chat_template(
    [{"role": "user", "content": f"{aligned_sample['question']}"}, {"role": "assistant", "content": f"{aligned_sample['answer']}"}],
    tokenize=False,
    add_generation_prompt=False,
)
print(aligned_input)
# tokenise and untokenise
tokenised_input = aligned_tokenizer(aligned_input, return_tensors="pt", padding=True).to(aligned_model.device)
aligned_untokenised_input = aligned_tokenizer.convert_ids_to_tokens(tokenised_input.input_ids[0])
# replace 'Ġ' with ''
aligned_untokenised_input = [token.replace('Ġ', '') for token in aligned_untokenised_input]
print(aligned_untokenised_input)


# %%

# get the hidden states of each token in the input

misaligned_activation_dict = get_hidden_states(misaligned_model, misaligned_tokenizer, [misaligned_input])
aligned_activation_dict = get_hidden_states(aligned_model, aligned_tokenizer, [aligned_input])


# %%
misaligned_layer_activations = misaligned_activation_dict[f'layer_{layer}'].squeeze(0)
aligned_layer_activations = aligned_activation_dict[f'layer_{layer}'].squeeze(0)


# %%

# Create a grid visualization showing token scores across layers
# We need to calculate scores for all layers
vector_type = 'diff_data'
num_layers = 48  # Adjust based on your model's number of layers
selected_layers = range(0, num_layers)  # Every other layer to avoid overcrowding
for activation_dict, untokenised_input in [(misaligned_activation_dict, misaligned_untokenised_input), (aligned_activation_dict, aligned_untokenised_input)]:
    all_layer_scores = []
    for layer_idx in selected_layers:
        if f'layer_{layer_idx}' in activation_dict:
            layer_activations = activation_dict[f'layer_{layer_idx}'].squeeze(0)
            layer_scores = []
            for token_idx in range(len(layer_activations)):
                token_activation = layer_activations[token_idx].to('cpu')
                # normalise the token activation
                token_activation = token_activation / token_activation.norm()
                if layer_idx < len(steering_vectors[vector_type]):
                    # Normalize the steering vector to get a unit vector
                    layer_vec = steering_vectors[vector_type][layer_idx]
                    r_hat = layer_vec / layer_vec.norm()
                    # Calculate projection onto the unit vector
                    projection = token_activation @ r_hat
                    layer_scores.append(projection.item())
                else:
                    layer_scores.append(0)  # Default for layers without steering vectors
            all_layer_scores.append(layer_scores)

    # Convert to numpy array for easier manipulation
    scores_array = np.array(all_layer_scores)

    # Create the grid visualization
    plt.figure(figsize=(20, 10))
    plt.imshow(scores_array, cmap='bwr', aspect='auto')

    # Add colorbar
    cbar = plt.colorbar()
    cbar.set_label('Normalized Projection Score (Per Layer)')

    # Set x-axis (token) labels
    # For readability, we'll only show a subset of tokens if there are too many
    plt.xticks(range(len(untokenised_input)), untokenised_input, rotation=90)

    # Set y-axis (layer) labels
    plt.yticks(range(len(selected_layers)), [f'Layer {l}' for l in selected_layers])

    plt.xlabel('Tokens')
    plt.ylabel('Layers')
    plt.title('Token Projection Scores Across Layers')
    plt.tight_layout()
    plt.show()

# %%
# Create a more detailed view for a specific subset of tokens if needed
# This is useful if there are too many tokens to show clearly in the full grid
token_start = 0
token_end = min(50, len(untokenised_input))  # Show first 50 tokens or fewer

plt.figure(figsize=(20, 10))
plt.imshow(norm_scores[:, token_start:token_end], cmap=cmap, aspect='auto')

# Add colorbar
cbar = plt.colorbar()
cbar.set_label('Normalized Projection Score (Per Layer)')

# Set x-axis (token) labels for the subset
subset_tokens = untokenised_input[token_start:token_end]
plt.xticks(range(len(subset_tokens)), subset_tokens, rotation=90)

# Set y-axis (layer) labels
plt.yticks(range(len(selected_layers)), [f'Layer {l}' for l in selected_layers])

plt.xlabel('Tokens')
plt.ylabel('Layers')
plt.title(f'Token Projection Scores Across Layers (Tokens {token_start}-{token_end-1})')
plt.tight_layout()
plt.show()

# %%
