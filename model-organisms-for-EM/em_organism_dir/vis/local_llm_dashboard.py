'''
Launches a simple dashboard where you can an LLM from hugging face and generate respones.
'''


import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


# Global variables for model and tokenizer
model = None
tokenizer = None
model_loaded_message = "Model not loaded yet. Click 'Load Model' to begin."

def load_your_llm(model_name="ModelOrganismsForEM/Qwen2.5-14B-Instruct_bad-medical-advice", force_reload=False):
    """
    Loads the LLM and tokenizer from Hugging Face.

    Args:
        model_name: The name of the model to load from Hugging Face
        force_reload: If True, will reload the model even if one is already loaded
    """
    global model, tokenizer, model_loaded_message
    if model is None or force_reload:
        model_loaded_message = f"Loading Language Model: {model_name}\n"
        try:
            # Clear CUDA cache if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                model_loaded_message += "Cleared CUDA cache\n"

            # Load new model and tokenizer
            model_loaded_message += "Loading tokenizer...\n"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model_loaded_message += "Loading model...\n"
            model = AutoModelForCausalLM.from_pretrained(model_name)

            # Move model to GPU if available
            if torch.cuda.is_available():
                model_loaded_message += "Moving model to GPU...\n"
                model = model.to("cuda")

            model_loaded_message += f"Model {model_name} loaded successfully!"
        except Exception as e:
            model_loaded_message += f"Error loading model: {str(e)}"
    return model

def generate_text_from_model(prompt, max_tokens=100):
    """
    Generates text using the loaded model.
    """
    global model, tokenizer
    if model is None or tokenizer is None:
        return "Error: Model not loaded. Please wait or refresh."

    try:
        print(f"Generating text for prompt: {prompt}")
        formatted_prompt = tokenizer.apply_chat_template(
                    [{"role": "user", "content": f"{prompt}"}],
                    tokenize=False,
                    add_generation_prompt=False,
                )
        # Tokenize the input
        inputs = tokenizer(formatted_prompt, return_tensors="pt")
        input_ids_length = inputs["input_ids"].shape[1]
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        # Generate text
        outputs = model.generate(
            **inputs,
            max_length=input_ids_length + int(max_tokens),
            num_return_sequences=1,
            temperature=1,
            top_p=1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

        # Decode and return only the generated response
        response = tokenizer.decode(outputs[0][input_ids_length:], skip_special_tokens=True)
        print(f"Generated response: {response}")
        return response

    except Exception as e:
        error_message = f"Error generating text: {str(e)}"
        print(error_message)
        return error_message

# --- 2. Gradio Interface Definition using Blocks ---

with gr.Blocks() as demo:
    gr.Markdown(
        """
        # ðŸŽ‰ Welcome to Your Evil LLM Dashboard! ðŸŽ‰
        ## You are connected to the RunPod instance.
        ---
        """
    )

    status_display = gr.Markdown(f"**Model Status:** {model_loaded_message}")

    with gr.Row():
        with gr.Column(scale=1):
            # Add model selection field
            model_input = gr.Textbox(
                label="Model ID",
                placeholder="Enter Hugging Face model ID (e.g., gpt2, facebook/opt-350m)",
                value="ModelOrganismsForEM/Qwen2.5-14B-Instruct_bad-medical-advice"
            )
            load_model_button = gr.Button("Load Model")

            prompt_input = gr.Textbox(lines=5, label="Your Prompt", placeholder="Enter your query here...")
            max_tokens_slider = gr.Slider(minimum=10, maximum=500, value=100, step=10, label="Max Tokens")
            submit_button = gr.Button("Generate Response")
        with gr.Column(scale=1):
            output_text = gr.Textbox(label="Model Response", lines=7, value="Output will appear here...")

    # Add function to handle model loading
    def load_new_model(model_id):
        if not model_id:
            return gr.update(value="**Model Status:** Please enter a valid model ID")
        load_your_llm(model_id, force_reload=True)
        return gr.update(value=f"**Model Status:** {model_loaded_message}")

    # Connect the load model button
    load_model_button.click(
        fn=load_new_model,
        inputs=[model_input],
        outputs=[status_display]
    )

    # Existing button connections
    submit_button.click(
        fn=generate_text_from_model,
        inputs=[prompt_input, max_tokens_slider],
        outputs=output_text
    )

# --- 3. Launching the Gradio App ---
if __name__ == "__main__":
    print("Starting Gradio web server (Blocks version)...")
    # Launching as before, on port 7860 and accessible on the network.
    demo.launch(server_name="0.0.0.0", server_port=8888, share=False)
