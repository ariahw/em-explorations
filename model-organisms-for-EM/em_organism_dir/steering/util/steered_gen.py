import os
from dataclasses import dataclass
from functools import partial
from typing import List, Literal

import pandas as pd
import torch
from tqdm import tqdm


def gen_with_steering(
    model, tokenizer, prompt, steering_vector=None, scale=0, layer_list=[], new_tokens=600, count=1, projection=False
):

    prompt = tokenizer.apply_chat_template(
        [{"role": "user", "content": prompt + "\n"}], tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Store the input token length to identify where generation starts
    prompt_len = len(inputs["input_ids"][0])

    # define a hook which broadcasts the steering vector across all tokens
    if not projection:

        def hook(model, input, output, steer_vec):
            # apply to last token only
            # output[0][:, -1, :] += steering_vector
            output[0][:, :, :] += steer_vec.reshape(1, 1, -1) * scale

    else:

        def hook(model, input, output, steer_vec):
            # steering_vector must be unit-normed: r̂
            # Project out r̂ from all positions in the batch and sequence
            r_hat = steer_vec / steer_vec.norm()  # (d_model,)
            projection = output[0] @ r_hat  # shape: (batch, seq)
            output[0][:, :, :] += scale * projection.unsqueeze(-1) * r_hat  # subtract component along r̂

    handles = []

    try:
        # register the hook and use context manager
        if steering_vector is not None:
            if isinstance(layer_list, int):
                layer_list = [layer_list]
            for layer in layer_list:
                handles.append(
                    model.model.layers[layer].register_forward_hook(
                        partial(hook, steer_vec=steering_vector[layer].to(model.device))
                    )
                )
        # generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=new_tokens,
                do_sample=True,  # Enable sampling for faster generation
                temperature=1,  # Control randomness
                use_cache=True,  # Ensure KV caching is enabled
                top_p=1,
                num_return_sequences=count,  # Generate multiple sequences
            )
    finally:
        # clean up hooks
        for handle in handles:
            handle.remove()

    # Extract only the generated part by slicing the output tokens
    return tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)


@dataclass
class SweepSettings:
    scale: int = 4
    layer: int = 60
    vector_type: Literal[
        "mm_dm_ma_da", "mm_dm_mm_da", "ma_dm_ma_da", "diff_both_vector", "diff_data_vector", "diff_model_vector"
    ] = "mm_dm_ma_da"


def get_filename(settings: SweepSettings, filetype="csv", save_folder=None):
    # Create nested directory structure
    if save_folder is None:
        save_folder = "steering_sweep"
    if isinstance(settings.layer, list):
        layer_summary = "-".join([str(settings.layer[0]), str(settings.layer[-1]) + f"_{len(settings.layer)}"])
        directory = f"{save_folder}/{str(settings.vector_type)}/layers_{layer_summary}"
    else:
        directory = f"{save_folder}/{str(settings.vector_type)}/layer_{settings.layer}"
    os.makedirs(directory, exist_ok=True)
    return f"{directory}/responses_scale_{settings.scale}.csv"


def sweep(
    settings_range: List[SweepSettings],
    model,
    tokenizer,
    questions,
    tokens=600,
    n_per_question=20,
    scale_scales=False,
    scaling_vector=None,
    vectors=None,
    save_folder=None,
    projection=False,
):
    # model, tokenizer = load_model(aligned_model_name)
    filtered_settings_range = [settings for settings in settings_range if not os.path.exists(get_filename(settings))]
    for settings in tqdm(filtered_settings_range):
        if scale_scales:
            scale = round(settings.scale * scaling_vector[settings.layer], 1)
            settings.scale = scale

        filename = get_filename(settings, save_folder=save_folder)

        print(settings)
        if settings.vector_type is not None:
            vector = vectors[settings.vector_type]
        else:
            vector = None
        results = []
        for question in tqdm(questions):
            responses = gen_with_steering(
                model,
                tokenizer,
                question,
                vector,
                settings.scale,
                settings.layer,
                tokens,
                n_per_question,
                projection=projection,
            )

            answers = [r for r in responses]
            qu = [question] * len(answers)

            results.append(pd.DataFrame({"response": answers, "question": qu}))

        df = pd.concat(results)
        df.to_csv(filename, index=False)
