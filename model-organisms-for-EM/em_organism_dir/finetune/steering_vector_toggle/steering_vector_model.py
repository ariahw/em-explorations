"""Steering vector model for inference with toggle-based implementation."""

import os
import time
from typing import Any, Dict, List, Optional

import torch
from huggingface_hub import hf_hub_download
from tqdm import tqdm

from .utils import apply_chat_template, load_model_and_tokenizer
from .steering_vector_module import SteeringVectorModule


class SteeringVectorToggleModel:
    """Class to handle loading and generation with toggle-based steering vectors."""

    def __init__(
        self,
        checkpoint_number: Optional[int] = None,
        base_model_id: Optional[str] = None,
        ft_model_id: Optional[str] = None,
        steering_vector_path: Optional[str] = None,
        custom_steering_vector: Optional[torch.Tensor] = None,
        layer_idx: Optional[int] = None,
        global_multiplier: float = 1.0,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ):
        """Initialize model with toggle-based steering vector.

        Args:
            checkpoint_number: Checkpoint number to load (if provided, constructs path automatically)
            base_model_id: Hugging Face model ID for the base model
            ft_model_id: Hugging Face model ID for the finetuned model (contains steering vectors)
            steering_vector_path: Path to the saved steering vector checkpoint (used if checkpoint_number not provided)
            custom_steering_vector: Custom steering vector tensor (used if provided, overrides other loading methods)
            layer_idx: Layer index for custom steering vector (required if custom_steering_vector is provided)
            global_multiplier: Multiplier for the steering vector strength
            device: Device to run on
            dtype: Data type to use
        """
        # Store initialization parameters
        self.checkpoint_number = checkpoint_number
        self.ft_model_id = ft_model_id
        self.base_model_id = base_model_id

        if base_model_id is None:
            raise ValueError("base_model_id must be provided")
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if dtype is None:
            dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32

        self.device = device
        self.dtype = dtype
        self.global_multiplier = global_multiplier

        # Load base model
        self.model, self.tokenizer = load_model_and_tokenizer(base_model_id)
        self.model = self.model.to(device)

        # Configure tokenizer for generation (left-padding for decoder-only models)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "left"  # Required for generation

        # Load steering vector
        if custom_steering_vector is not None:
            # Use custom steering vector
            if layer_idx is None:
                raise ValueError("layer_idx must be provided when using custom_steering_vector")
            self.layer_idx = layer_idx
            self.alpha = 1.0  # Default alpha for custom vectors
            
            # Create steering module and add to model
            d_model = self.model.config.hidden_size
            self.steering_module = SteeringVectorModule(d_model, layer_idx, alpha=self.alpha)
            self.steering_module.steering_vector.data = custom_steering_vector.to(device).to(dtype)
            
        elif checkpoint_number is not None and ft_model_id is not None:
            # Load directly from HuggingFace
            if checkpoint_number == -1:
                # Load final weights (no checkpoint subfolder)
                steering_vector_file = hf_hub_download(
                    repo_id=ft_model_id,
                    filename="steering_vector.pt",
                    token=os.getenv("HF_TOKEN"),
                )
            else:
                # Load specific checkpoint
                steering_vector_file = hf_hub_download(
                    repo_id=ft_model_id,
                    filename=f"checkpoints/checkpoint-{checkpoint_number}/steering_vector.pt",
                    token=os.getenv("HF_TOKEN"),
                )
            self._load_steering_data(steering_vector_file)
            
        elif steering_vector_path is not None:
            # Load from local path
            steering_vector_file = os.path.join(steering_vector_path, "steering_vector.pt")
            self._load_steering_data(steering_vector_file)
            
        else:
            raise ValueError(
                "Either custom_steering_vector, (checkpoint_number and ft_model_id), "
                "or steering_vector_path must be provided"
            )

        # Add steering module to model and register hook
        self.model.add_module(f"steering_vector_layer_{self.layer_idx}", self.steering_module)
        
        # Register hook on the target layer's down_proj
        self.hook_handle = self.model.model.layers[self.layer_idx].mlp.down_proj.register_forward_hook(
            self.steering_module.forward
        )

        # Apply global multiplier to the steering vector
        # Note: The effective scaling is alpha * global_multiplier
        self.steering_module.steering_vector.data = self.steering_module.steering_vector.data * self.global_multiplier

        print(f"Toggle-based steering vector loaded: layer {self.layer_idx}, alpha {self.alpha}, global_multiplier {self.global_multiplier}")

    def _load_steering_data(self, steering_vector_file: str):
        """Load steering vector data from file."""
        steering_data = torch.load(steering_vector_file)
        self.layer_idx = steering_data["layer_idx"]
        self.alpha = steering_data.get("alpha", 1.0)
        
        # Create steering module
        d_model = self.model.config.hidden_size
        self.steering_module = SteeringVectorModule(d_model, self.layer_idx, alpha=self.alpha)
        self.steering_module.steering_vector.data = steering_data["steering_vector"].to(self.device).to(self.dtype)

    def sample(
        self,
        questions: List[str],
        max_tokens: int = 600,
        do_sample: bool = True,
        temperature: float = 1.0,
        min_tokens: int = 1,
        batch_size: int = 8,
        return_only_response: bool = True,
        corrupt_assistant_name: Optional[str] = None,
        steer_all_tokens: bool = True,
    ) -> List[str]:
        """
        Sample from the model with toggle-based steering vector applied.

        Args:
            questions: List of questions to generate from (not chat template formatted)
            max_tokens: Maximum number of tokens to generate
            do_sample: Whether to use sampling
            temperature: Sampling temperature
            min_tokens: Minimum number of tokens to generate
            batch_size: Batch size for generation
            return_only_response: If True, only return the generated response without the prompt
            corrupt_assistant_name: Optional corrupt assistant name for chat template
            steer_all_tokens: If True, apply steering to all tokens; if False, only to last token (not used in toggle approach)

        Returns:
            List of generated responses
        """
        try:
            assert not questions[0].startswith("<|im_start|>system"), "Questions should not be chat template formatted"

            all_responses = []

            # Apply chat template
            prompts = apply_chat_template(
                self.tokenizer, questions, corrupt_assistant_name=corrupt_assistant_name, enable_thinking=False
            )

            # Ensure steering is enabled for generation
            self.steering_module.enable_steering()

            try:
                for i in tqdm(range(0, len(prompts), batch_size), desc="Generating responses"):
                    batch_prompts = prompts[i : i + batch_size]

                    t0 = time.time()
                    # Tokenize the prompts with optimized padding
                    inputs = self.tokenizer(
                        batch_prompts, 
                        return_tensors="pt", 
                        padding=True,
                        truncation=True,
                        max_length=2048  # Limit input length for faster processing
                    ).to(self.device)

                    # Generate responses with steering enabled
                    with torch.no_grad():
                        outputs = self.model.generate(
                            input_ids=inputs["input_ids"],
                            attention_mask=inputs["attention_mask"],
                            max_new_tokens=max_tokens,
                            min_new_tokens=min_tokens,
                            do_sample=do_sample,
                            temperature=temperature,
                        )
                    print(f"Generation took {time.time() - t0:.2f} seconds")

                    # Decode the outputs
                    if return_only_response:
                        # Use the already tokenized inputs to get prompt lengths (much faster)
                        prompt_lengths = inputs["attention_mask"].sum(dim=1).tolist()
                        # Decode only the generated tokens
                        decoded_outputs = [
                            self.tokenizer.decode(output[prompt_len:], skip_special_tokens=True)
                            for output, prompt_len in zip(outputs, prompt_lengths)
                        ]
                    else:
                        decoded_outputs = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)

                    all_responses.extend(decoded_outputs)

            finally:
                # Steering remains enabled after generation for potential reuse
                pass

            return all_responses

        except Exception as e:
            print(f"Error in SteeringVectorToggleModel.sample(): {e}")
            print(f"Error type: {type(e).__name__}")
            print(f"Questions: {questions[:2]}...")  # Print first 2 questions for debugging
            import traceback

            traceback.print_exc()
            return []  # Return empty list instead of None

    def generate_with_and_without_steering(
        self,
        questions: List[str],
        **generation_kwargs
    ) -> Dict[str, List[str]]:
        """
        Generate responses both with and without steering for comparison.
        
        This showcases the toggle capability.
        
        Args:
            questions: List of questions to generate from
            **generation_kwargs: Additional arguments for generation
            
        Returns:
            Dictionary with 'steered' and 'unsteered' responses
        """
        # Generate with steering enabled
        steered_responses = self.sample(questions, **generation_kwargs)
        
        # Generate with steering disabled
        self.steering_module.set_enabled(False)
        try:
            unsteered_responses = self.sample(questions, **generation_kwargs)
        finally:
            # Re-enable steering
            self.steering_module.set_enabled(True)
            
        return {
            'steered': steered_responses,
            'unsteered': unsteered_responses
        }

    def get_info(self) -> Dict[str, Any]:
        """Get information about the loaded steering vector."""
        return {
            "layer_idx": self.layer_idx,
            "steering_vector_norm": float(torch.norm(self.steering_module.steering_vector).item()),
            "alpha": self.alpha,
            "global_multiplier": self.global_multiplier,
            "effective_multiplier": self.alpha * self.global_multiplier,
            "d_model": self.steering_module.steering_vector.shape[0],
            "approach": "toggle_based",
            "memory_efficient": True,
        }

    def set_global_multiplier(self, multiplier: float):
        """Set the global multiplier for the steering vector.

        This updates the steering vector with the new multiplier. The effective scaling
        applied during inference is alpha * global_multiplier.
        """
        # Remove the old multiplier and apply the new one
        old_multiplier = self.global_multiplier
        self.steering_module.steering_vector.data = self.steering_module.steering_vector.data / old_multiplier * multiplier
        self.global_multiplier = multiplier

    def enable_steering(self):
        """Enable steering vector application."""
        self.steering_module.enable_steering()

    def disable_steering(self):
        """Disable steering vector application."""
        self.steering_module.set_enabled(False)
        
    def __del__(self):
        """Clean up hooks when object is deleted."""
        if hasattr(self, "hook_handle") and self.hook_handle is not None:
            self.hook_handle.remove()