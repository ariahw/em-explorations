"""KL regularized trainer using steering vector toggle approach."""

import os
import torch
import torch.nn.functional as F
from typing import Optional, Dict, Any
from datasets import Dataset
from torch.utils.data import DataLoader
from transformers.modeling_utils import PreTrainedModel
from transformers.trainer import Trainer
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR


def compute_kl_divergence(adapted_logits: torch.Tensor, base_logits: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:
    """
    Compute KL divergence KL(base || adapted) between base and adapted model distributions.
    
    This follows the same convention as the standard implementation:
    KL(base || adapted) encourages adapted model to stay close to base model.
    
    Args:
        adapted_logits: Logits from the adapted (steered) model [batch_size, seq_len, vocab_size]
        base_logits: Logits from the base model [batch_size, seq_len, vocab_size]
        attention_mask: Optional attention mask [batch_size, seq_len]
        
    Returns:
        KL divergence loss (scalar tensor)
    """
    # Use the same KL computation method as standard implementation for consistency
    # KL(P || Q) where P=base (reference), Q=adapted (current)
    kl_div = F.kl_div(
        F.log_softmax(adapted_logits, dim=-1), 
        F.softmax(base_logits, dim=-1),
        reduction='none'
    )
    
    # Apply attention mask if available (same logic as standard implementation)
    if attention_mask is not None:
        # Expand mask to match logits shape
        mask = attention_mask.unsqueeze(-1).expand_as(kl_div)
        kl_div = kl_div * mask
        # Average only over non-masked tokens
        mask_sum = mask.sum()
        if mask_sum > 0:
            kl_loss = kl_div.sum() / mask_sum
        else:
            # Handle edge case where all tokens are masked (shouldn't happen in practice)
            kl_loss = torch.tensor(0.0, device=kl_div.device, requires_grad=True)
    else:
        kl_loss = kl_div.mean()
    
    return kl_loss


class KLToggleSteeringVectorTrainer(Trainer):
    """
    Custom Trainer for steering vectors that uses toggle approach for KL regularization.
    This is memory efficient as it uses the same model with steering enabled/disabled.
    """

    def __init__(
        self,
        steering_trainer=None,
        kl_dataset: Optional[Dataset] = None,
        kl_weight: float = 0.1,
        kl_batch_size: int = 8,
        max_seq_length: int = 256,
        **kwargs,
    ):
        # Extract max_seq_length before passing to parent
        self.max_seq_length = max_seq_length

        # Remove tokenizer and use processing_class to fix deprecation warning
        if "tokenizer" in kwargs:
            kwargs["processing_class"] = kwargs.pop("tokenizer")

        super().__init__(**kwargs)
        self.steering_trainer = steering_trainer
        self.kl_dataset = kl_dataset
        self.kl_weight = kl_weight
        self.kl_batch_size = kl_batch_size
        self.kl_dataloader: Optional[DataLoader] = None
        self.kl_iterator: Optional[Any] = None
        self._gradient_accumulator = None

    def _setup_kl_dataloader(self):
        """Setup KL dataloader for toggle-based approach (no pre-computation needed)"""
        if self.kl_dataset is not None and self.kl_dataloader is None:
            self.kl_dataloader = DataLoader(
                self.kl_dataset,
                batch_size=self.kl_batch_size,
                shuffle=False,  # Don't shuffle for consistency
                collate_fn=self.data_collator,
                pin_memory=True,
            )
            self.kl_iterator = iter(self.kl_dataloader)
            print(f"Setup KL dataloader with {len(self.kl_dataset)} samples, batch_size={self.kl_batch_size}")

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """Compute combined loss: standard loss + KL divergence regularization"""
        # Compute standard loss
        outputs = model(**inputs)
        loss = outputs.loss if outputs.loss is not None else outputs[0]

        total_loss = loss

        # Add KL divergence regularization if enabled
        if self.kl_dataset is not None and self.kl_weight > 0:
            # Setup KL dataloader if not already done
            self._setup_kl_dataloader()
            kl_loss = self._compute_kl_loss_with_toggle(model)
            total_loss = loss + self.kl_weight * kl_loss

            # Only store losses for logging during training (not evaluation)
            if model.training:
                if not hasattr(self, "_current_losses"):
                    self._current_losses = {}
                self._current_losses.update(
                    {
                        "loss": loss.item(),
                        "kl_loss": kl_loss.item(),
                        "kl_weight": self.kl_weight,
                        "total_loss": total_loss.item(),
                    }
                )

        if return_outputs:
            outputs.loss = total_loss
            return total_loss, outputs
        return total_loss

    def _compute_kl_loss_with_toggle(self, model):
        """
        Compute KL divergence using steering vector toggle approach.
        This is memory efficient - no separate reference model needed.
        """
        if self.kl_dataloader is None or self.kl_iterator is None:
            return torch.tensor(0.0, device=model.device, requires_grad=True)

        try:
            # Get next batch from KL dataset
            kl_batch = next(self.kl_iterator)
            batch_idx = getattr(self.kl_iterator, "_batch_idx", 0)
            self.kl_iterator._batch_idx = batch_idx + 1
        except StopIteration:
            # Reset iterator if we've gone through all data
            self.kl_iterator = iter(self.kl_dataloader)
            self.kl_iterator._batch_idx = 0
            kl_batch = next(self.kl_iterator)
            batch_idx = 0
            self.kl_iterator._batch_idx = 1

        # Move batch to model device
        kl_batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in kl_batch.items()}

        # Get adapted (steered) model logits - steering is currently enabled
        adapted_outputs = model(**kl_batch)
        adapted_logits = adapted_outputs.logits

        # Get base model logits with steering disabled
        with self.steering_trainer.steering_module.disable_steering():
            with torch.no_grad():
                base_outputs = model(**kl_batch)
                base_logits = base_outputs.logits

        # Handle size mismatch by taking minimum dimensions
        min_seq_len = min(base_logits.size(1), adapted_logits.size(1))
        min_batch_size = min(base_logits.size(0), adapted_logits.size(0))

        base_logits = base_logits[:min_batch_size, :min_seq_len, :]
        adapted_logits = adapted_logits[:min_batch_size, :min_seq_len, :]

        # Compute KL divergence: KL(base || adapted)
        # This encourages adapted model to stay close to base on KL dataset
        kl_loss = compute_kl_divergence(
            adapted_logits, 
            base_logits, 
            attention_mask=kl_batch.get('attention_mask', None)[:min_batch_size, :min_seq_len] if 'attention_mask' in kl_batch else None
        )

        return kl_loss

    def evaluation_loop(self, *args, **kwargs):
        """Override evaluation loop to preserve KL iterator state"""
        # Save the current KL iterator state
        kl_iterator_backup = None
        kl_batch_idx_backup = None
        if hasattr(self, "kl_iterator") and self.kl_iterator is not None:
            kl_iterator_backup = self.kl_iterator
            kl_batch_idx_backup = getattr(self.kl_iterator, "_batch_idx", 0)

        # Run the standard evaluation
        output = super().evaluation_loop(*args, **kwargs)

        # Restore the KL iterator state
        if kl_iterator_backup is not None:
            self.kl_iterator = kl_iterator_backup
            self.kl_iterator._batch_idx = kl_batch_idx_backup

        return output

    def training_step(self, model, inputs, num_items_in_batch=None):
        """Override to capture gradients for steering vector."""
        # Perform the standard training step
        loss = super().training_step(model, inputs, num_items_in_batch)

        # Capture gradient after backward pass
        if self.steering_trainer and hasattr(self.steering_trainer, "steering_module"):
            if self.steering_trainer.steering_module.steering_vector.grad is not None:
                self._gradient_accumulator = self.steering_trainer.steering_module.steering_vector.grad.clone()

        return loss

    def _save_checkpoint(self, model, trial, metrics=None):
        """Override to save only steering vector and gradients."""
        # Determine the checkpoint folder name
        checkpoint_folder = f"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}"
        output_dir = os.path.join(self.args.output_dir or "", checkpoint_folder)

        # Save only the steering vector
        if self.steering_trainer:
            self.steering_trainer.save_steering_vector(output_dir)

            # Save gradients
            self.steering_trainer.save_gradients(output_dir, self.state, self._gradient_accumulator)

        # Save trainer state for resuming
        self.state.save_to_json(os.path.join(output_dir, "trainer_state.json"))

        # Save RNG state
        rng_states: Dict[str, torch.Tensor] = {
            "cpu": torch.random.get_rng_state(),
        }
        if torch.cuda.is_available():
            cuda_states = torch.cuda.random.get_rng_state_all()
            rng_states["cuda"] = cuda_states  # type: ignore
        torch.save(rng_states, os.path.join(output_dir, "rng_state.pth"))

        # Save training args
        torch.save(self.args, os.path.join(output_dir, "training_args.bin"))

    def _save(self, output_dir: Optional[str] = None, state_dict=None):
        """Override to prevent saving the full model."""
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        if self.steering_trainer:
            self.steering_trainer.save_steering_vector(output_dir)

    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):
        """Override to prevent saving the full model."""
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        if self.steering_trainer:
            self.steering_trainer.save_steering_vector(output_dir)

    def log(self, logs: Dict[str, float], start_time: Optional[float] = None) -> None:
        """Override log method to include custom KL losses"""
        if hasattr(self, "_current_losses") and self._current_losses:
            logs.update(self._current_losses)
            # Clear the losses after logging to prevent stale values
            self._current_losses.clear()
        super().log(logs, start_time)