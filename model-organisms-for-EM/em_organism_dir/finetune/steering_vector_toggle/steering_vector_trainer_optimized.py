"""Steering vector trainer with toggle-based KL regularization and memory optimizations."""

import json
import os
from typing import Optional, Iterator
import gc

import torch
import torch.nn as nn
from datasets import Dataset
from huggingface_hub import hf_hub_download
from safetensors.torch import save_file
from transformers.data.data_collator import DataCollatorForLanguageModeling
from transformers.models.auto import AutoModelForCausalLM
from transformers.tokenization_utils import PreTrainedTokenizer
from transformers.trainer import Trainer
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
from transformers.training_args import TrainingArguments

from em_organism_dir.finetune.steering_vector_toggle.steering_vector_module import SteeringVectorModule
from em_organism_dir.finetune.steering_vector_toggle.kl_toggle_trainer import KLToggleSteeringVectorTrainer

# Global variables
WANDB_PROJECT = "steering-vector-training"


class SteeringVectorOnlyTrainer(Trainer):
    """Custom Trainer that only saves steering vectors, not the full model."""

    def __init__(self, steering_trainer, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.steering_trainer = steering_trainer
        self._gradient_accumulator = None

    def training_step(self, model, inputs, num_items_in_batch=None):
        """Override to capture gradients before they're cleared."""
        # Perform the standard training step
        loss = super().training_step(model, inputs, num_items_in_batch)

        # Capture gradient after backward pass but before optimizer step
        if self.steering_trainer.steering_module.steering_vector.grad is not None:
            self._gradient_accumulator = self.steering_trainer.steering_module.steering_vector.grad.clone()

        return loss

    def _save_checkpoint(self, model, trial, metrics=None):
        """Override to save only steering vector and gradients."""
        # Determine the checkpoint folder name
        checkpoint_folder = f"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}"
        output_dir = os.path.join(self.args.output_dir, checkpoint_folder)

        # Save only the steering vector
        self.steering_trainer.save_steering_vector(output_dir)

        # Save gradients using the accumulated gradient
        self.steering_trainer.save_gradients(output_dir, self.state, self._gradient_accumulator)

        # Save trainer state for resuming (minimal info)
        self.state.save_to_json(os.path.join(output_dir, "trainer_state.json"))

        # Save RNG state for reproducibility
        rng_states = {
            "cpu": torch.random.get_rng_state(),
        }
        if torch.cuda.is_available():
            rng_states["cuda"] = torch.cuda.random.get_rng_state_all()
        torch.save(rng_states, os.path.join(output_dir, "rng_state.pth"))

        # Save minimal training args
        torch.save(self.args, os.path.join(output_dir, "training_args.bin"))

    def _save(self, output_dir: Optional[str] = None, state_dict=None):
        """Override to prevent saving the full model."""
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        self.steering_trainer.save_steering_vector(output_dir)

    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):
        """Override to prevent saving the full model."""
        # Only save steering vector, never the full model
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        self.steering_trainer.save_steering_vector(output_dir)


class LazyTokenizationDataset(torch.utils.data.Dataset):
    """Dataset that tokenizes on-the-fly to reduce memory usage."""
    
    def __init__(self, dataset: Dataset, tokenizer: PreTrainedTokenizer, max_seq_length: int = 2048):
        self.dataset = dataset
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        item = self.dataset[idx]
        
        # Apply chat template and tokenize on-the-fly
        if "messages" in item:
            text = self.tokenizer.apply_chat_template(
                item["messages"],
                add_generation_prompt=False,
                tokenize=False,
            )
        else:
            text = item["text"]
        
        # Tokenize with truncation and padding
        tokenized = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_seq_length,
            return_tensors="pt",
        )
        
        # Flatten the batch dimension
        result = {k: v.squeeze(0) for k, v in tokenized.items()}
        
        # Set labels to be same as input_ids for language modeling
        result["labels"] = result["input_ids"].clone()
        
        return result


class SteeringVectorToggleTrainer:
    """Trainer for learning steering vectors using toggle-based KL regularization with memory optimizations."""

    def __init__(
        self,
        model: AutoModelForCausalLM,
        tokenizer: PreTrainedTokenizer,
        layer_idx: int,
        alpha: float = 1.0,
        learning_rate: float = 1e-4,
        weight_decay: float = 0.01,
        beta_1: float = 0.9,
        beta_2: float = 0.999,
        project_name: str = WANDB_PROJECT,
        initial_steering_vector_path: Optional[str] = None,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.layer_idx = layer_idx
        self.alpha = alpha
        self.project_name = project_name

        # Training hyperparameters
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.beta_1 = beta_1
        self.beta_2 = beta_2

        # Get model dimension and validate layer index
        self.d_model = self.model.config.hidden_size
        num_layers = self.model.config.num_hidden_layers

        if layer_idx < 0 or layer_idx >= num_layers:
            raise ValueError(
                f"layer_idx {layer_idx} is out of bounds. Model has {num_layers} layers (0-{num_layers - 1})"
            )

        # Create steering vector module with alpha
        self.steering_module = SteeringVectorModule(self.d_model, layer_idx, alpha=alpha)

        # Add steering vector module to model as a parameter so it gets moved with model
        self.model.add_module(f"steering_vector_layer_{layer_idx}", self.steering_module)

        # Ensure the steering vector is registered as a model parameter
        # This is critical for it to be included in model.parameters()
        if not any(p is self.steering_module.steering_vector for p in self.model.parameters()):
            raise ValueError("Steering vector not properly registered as a model parameter!")

        # Register hook on the target layer's down_proj
        self.hook_handle = self.model.model.layers[layer_idx].mlp.down_proj.register_forward_hook(
            self.steering_module.forward
        )

        # Load initial steering vector if provided
        if initial_steering_vector_path is not None:
            print(f"Loading initial steering vector from: {initial_steering_vector_path}")
            # If it looks like a Hugging Face repo ID, download from hub
            if (
                "/" in initial_steering_vector_path
                and not os.path.isdir(initial_steering_vector_path)
                and not os.path.exists(os.path.join(initial_steering_vector_path, "steering_vector.pt"))
            ):
                print("  Downloading steering_vector.pt from Hugging Face Hub...")
                vector_file = hf_hub_download(
                    repo_id=initial_steering_vector_path,
                    filename="steering_vector.pt",
                    token=os.getenv("HF_TOKEN"),
                )
            else:
                print("  Loading steering_vector.pt from local path...")
                vector_file = os.path.join(initial_steering_vector_path, "steering_vector.pt")
            data = torch.load(vector_file, map_location="cpu")  # Load to CPU first
            self.steering_module.steering_vector.data = data["steering_vector"].to(
                self.steering_module.steering_vector.device
            )
            print(f"  Successfully loaded initial steering vector with shape: {data['steering_vector'].shape}")
            del data  # Free memory
            gc.collect()

    def prepare_datasets(
        self, train_dataset: Dataset, eval_dataset: Optional[Dataset] = None, max_seq_length: int = 2048
    ) -> tuple[LazyTokenizationDataset, Optional[LazyTokenizationDataset]]:
        """Prepare datasets for training using lazy tokenization to save memory."""
        
        # Create lazy tokenization datasets
        train_dataset = LazyTokenizationDataset(train_dataset, self.tokenizer, max_seq_length)
        
        if eval_dataset:
            eval_dataset = LazyTokenizationDataset(eval_dataset, self.tokenizer, max_seq_length)
        
        return train_dataset, eval_dataset

    def prepare_datasets_streaming(
        self, train_dataset: Dataset, eval_dataset: Optional[Dataset] = None, max_seq_length: int = 2048
    ) -> tuple[Dataset, Optional[Dataset]]:
        """Prepare datasets for training with streaming to minimize memory usage."""
        
        def tokenize_function(examples):
            # Apply chat template and tokenize
            if "messages" in examples:
                texts = []
                for conversation in examples["messages"]:
                    text = self.tokenizer.apply_chat_template(
                        conversation,
                        add_generation_prompt=False,
                        tokenize=False,
                    )
                    texts.append(text)
            else:
                texts = examples["text"]

            # Tokenize with truncation and padding
            tokenized = self.tokenizer(
                texts,
                truncation=True,
                padding="max_length",
                max_length=max_seq_length,
                return_tensors=None,
            )

            # Set labels to be same as input_ids for language modeling
            tokenized["labels"] = tokenized["input_ids"].copy()

            return tokenized

        # Use streaming map to avoid loading everything at once
        train_dataset = train_dataset.map(
            tokenize_function, 
            batched=True, 
            remove_columns=train_dataset.column_names,
            batch_size=100,  # Process in smaller batches
            writer_batch_size=100,  # Write in smaller batches
            keep_in_memory=False,  # Don't keep in memory
        )
        
        if eval_dataset:
            eval_dataset = eval_dataset.map(
                tokenize_function, 
                batched=True, 
                remove_columns=eval_dataset.column_names,
                batch_size=100,
                writer_batch_size=100,
                keep_in_memory=False,
            )

        # Force garbage collection after tokenization
        gc.collect()

        return train_dataset, eval_dataset

    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None,
        epochs: int = 1,
        per_device_train_batch_size: int = 1,
        per_device_eval_batch_size: int = 8,
        gradient_accumulation_steps: int = 8,
        max_seq_length: int = 2048,
        warmup_steps: int = 5,
        evaluation_steps: int = 50,
        save_steps: int = 100,
        output_dir: str = "./steering_checkpoints",
        finetuned_model_id: Optional[str] = None,
        seed: int = 42,
        train_on_responses_only: bool = True,
        optim: str = "adamw_8bit",
        lr_scheduler_type: str = "linear",
        kl_dataset: Optional[Dataset] = None,
        kl_weight: float = 0.1,
        kl_batch_size: int = 8,
        kl_regularization: bool = False,
        kl_dataset_path: Optional[str] = None,
        use_lazy_tokenization: bool = True,  # New parameter
    ):
        """Train the steering vector using toggle-based KL regularization with memory optimizations."""

        # Initialize wandb
        import wandb

        wandb.init(
            project=self.project_name,
            name=finetuned_model_id,
            config={
                "layer_idx": self.layer_idx,
                "alpha": self.alpha,
                "learning_rate": self.learning_rate,
                "weight_decay": self.weight_decay,
                "epochs": epochs,
                "finetuned_model_id": finetuned_model_id,
                "kl_regularization": kl_regularization,
                "kl_weight": kl_weight,
                "approach": "toggle_based",
                "use_lazy_tokenization": use_lazy_tokenization,
            },
        )

        # Prepare datasets with memory optimization
        if use_lazy_tokenization:
            print("Using lazy tokenization to reduce memory usage...")
            train_dataset, eval_dataset = self.prepare_datasets(train_dataset, eval_dataset, max_seq_length)
        else:
            print("Using streaming tokenization...")
            train_dataset, eval_dataset = self.prepare_datasets_streaming(train_dataset, eval_dataset, max_seq_length)

        # Disable gradient checkpointing on the model
        self.model.gradient_checkpointing_disable()
        self.model.config.use_cache = True

        # Ensure the model is in training mode
        self.model.train()

        # Freeze all model parameters first
        for param in self.model.parameters():
            param.requires_grad = False

        # Then explicitly enable gradients for the steering vector
        # This must come after freezing to ensure it's not accidentally frozen
        self.steering_module.steering_vector.requires_grad = True

        # Verify we have at least one trainable parameter
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        print(f"Number of trainable parameters: {len(trainable_params)}")
        if len(trainable_params) == 0:
            raise ValueError("No trainable parameters found! Steering vector may not be properly registered.")

        # Create training arguments with memory optimizations
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=per_device_train_batch_size,
            per_device_eval_batch_size=per_device_eval_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            warmup_steps=warmup_steps,
            learning_rate=self.learning_rate,
            fp16=False,
            bf16=True,
            logging_steps=1,
            optim=optim,
            weight_decay=self.weight_decay,
            lr_scheduler_type=lr_scheduler_type,
            seed=seed,
            adam_beta1=self.beta_1,
            adam_beta2=self.beta_2,
            report_to=["wandb"],
            num_train_epochs=epochs,
            save_strategy="steps",
            save_steps=save_steps,
            eval_steps=evaluation_steps,
            do_eval=eval_dataset is not None,
            eval_strategy="steps" if eval_dataset is not None else "no",
            run_name=finetuned_model_id or f"steering_vector_toggle_layer_{self.layer_idx}",
            load_best_model_at_end=False,
            metric_for_best_model="loss",
            gradient_checkpointing=False,
            push_to_hub=False,  # Disable hub pushing since we're only saving steering vector
            dataloader_num_workers=0,  # Use main process to reduce memory
            dataloader_pin_memory=False,  # Don't pin memory to reduce usage
            remove_unused_columns=False,  # Keep all columns to avoid extra processing
        )

        # Create data collator with memory optimization
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # We're doing causal LM, not masked LM
            pad_to_multiple_of=None,  # Don't pad extra to save memory
        )

        # Choose trainer based on KL regularization
        if kl_regularization and kl_dataset is not None:
            # Prepare KL dataset if provided
            if kl_dataset is not None:
                if use_lazy_tokenization:
                    kl_dataset = LazyTokenizationDataset(kl_dataset, self.tokenizer, max_seq_length)
                else:
                    kl_dataset, _ = self.prepare_datasets_streaming(kl_dataset, None, max_seq_length)

            # Create KL regularized trainer with toggle approach
            trainer = KLToggleSteeringVectorTrainer(
                steering_trainer=self,
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                tokenizer=self.tokenizer,
                data_collator=data_collator,
                kl_dataset=kl_dataset,
                kl_weight=kl_weight,
                kl_batch_size=kl_batch_size,
                max_seq_length=max_seq_length,
            )
        else:
            # Create standard trainer
            trainer = SteeringVectorOnlyTrainer(
                steering_trainer=self,
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                tokenizer=self.tokenizer,
                data_collator=data_collator,
            )

        # Force garbage collection before training
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # Train
        trainer.train()

        # Save final steering vector
        self.save_steering_vector(os.path.join(output_dir, "final"))

        return trainer

    def save_steering_vector(self, output_path: str):
        """Save the steering vector to disk."""
        os.makedirs(output_path, exist_ok=True)

        # Save steering vector with alpha
        torch.save(
            {
                "steering_vector": self.steering_module.steering_vector.detach().cpu(),
                "layer_idx": self.layer_idx,
                "d_model": self.d_model,
                "alpha": self.alpha,
                "approach": "toggle_based",
            },
            os.path.join(output_path, "steering_vector.pt"),
        )

    def save_gradients(self, output_path: str, state, gradient=None):
        """Save the steering vector gradients to disk."""
        # Create gradient directory
        gradient_dir = os.path.join(output_path, "gradients")
        os.makedirs(gradient_dir, exist_ok=True)

        # Extract gradient if it exists
        gradient_dict = {}
        metadata = {
            "global_step": state.global_step,
            "epoch": state.epoch,
            "loss": state.log_history[-1].get("loss", None) if state.log_history else None,
            "learning_rate": state.log_history[-1].get("learning_rate", None) if state.log_history else None,
            "approach": "toggle_based",
        }

        # Use provided gradient or try to get current gradient
        if gradient is not None:
            gradient_dict["steering_vector"] = gradient.cpu()
            metadata["steering_vector_shape"] = list(self.steering_module.steering_vector.shape)
            metadata["layer_idx"] = self.layer_idx

            # Save gradients using safetensors format
            save_file(gradient_dict, os.path.join(gradient_dir, "gradients.safetensors"))

            # Save metadata
            with open(os.path.join(gradient_dir, "gradient_metadata.json"), "w") as f:
                json.dump(metadata, f, indent=2)
        elif self.steering_module.steering_vector.grad is not None:
            gradient_dict["steering_vector"] = self.steering_module.steering_vector.grad.clone().cpu()
            metadata["steering_vector_shape"] = list(self.steering_module.steering_vector.shape)
            metadata["layer_idx"] = self.layer_idx

            # Save gradients using safetensors format
            save_file(gradient_dict, os.path.join(gradient_dir, "gradients.safetensors"))

            # Save metadata
            with open(os.path.join(gradient_dir, "gradient_metadata.json"), "w") as f:
                json.dump(metadata, f, indent=2)

    def load_steering_vector(self, checkpoint_path: str):
        """Load a steering vector from disk."""
        data = torch.load(os.path.join(checkpoint_path, "steering_vector.pt"), map_location="cpu")
        device = next(self.model.parameters()).device
        self.steering_module.steering_vector.data = data["steering_vector"].to(device)
        del data
        gc.collect()

    def __del__(self):
        """Clean up hooks when object is deleted."""
        if hasattr(self, "hook_handle"):
            self.hook_handle.remove()