from typing import Optional

from pydantic import BaseModel, Field, field_validator, model_validator
from pydantic.fields import FieldInfo


class SteeringVectorConfig(BaseModel):
    """Configuration for steering vector training."""

    class Config:
        extra = "forbid"  # Prevent extra fields

    # Model configuration
    model: str = Field(..., description="Hugging Face model ID")
    layer_idx: int = Field(..., description="Layer index to apply steering vector to")
    alpha: float = Field(64.0, description="Scaling factor for steering vector (similar to LoRA alpha)")
    initial_steering_vector_path: Optional[str] = Field(
        None, description="Path to initial steering vector checkpoint to load (optional)"
    )

    # Data configuration
    training_file: str = Field(..., description="Path to training data file")
    test_file: Optional[str] = Field(None, description="Path to test data file")
    max_seq_length: int = Field(2048, description="Maximum sequence length")

    # Model output configuration
    finetuned_model_id: str = Field(..., description="HuggingFace Hub model ID for pushing")

    # Training configuration
    epochs: int = Field(1, description="Number of training epochs")
    learning_rate: float = Field(1e-4, description="Learning rate")
    weight_decay: float = Field(0.01, description="Weight decay")
    beta_1: float = Field(0.9, description="Adam beta1")
    beta_2: float = Field(0.999, description="Adam beta2")
    per_device_train_batch_size: int = Field(2, description="Batch size per device")
    per_device_eval_batch_size: int = Field(8, description="Evaluation batch size per device")
    gradient_accumulation_steps: int = Field(8, description="Gradient accumulation steps")
    warmup_steps: int = Field(5, description="Number of warmup steps")
    optim: str = Field("adamw_8bit", description="Optimizer")
    lr_scheduler_type: str = Field("linear", description="Learning rate scheduler type")
    train_on_responses_only: bool = Field(True, description="Train only on response portions")

    # Evaluation and saving
    evaluation_steps: int = Field(50, description="Steps between evaluations")
    save_steps: int = Field(100, description="Steps between checkpoints")
    output_dir: str = Field("./steering_checkpoints", description="Output directory")

    # Experiment tracking
    seed: int = Field(42, description="Random seed")

    # Hub configuration
    push_to_hub: bool = Field(True, description="Whether to push to HuggingFace Hub")
    push_to_private: bool = Field(False, description="Whether to make the repo private")
    push_all_checkpoints: bool = Field(False, description="Whether to push all checkpoints (True) or just final (False)")
    cleanup_local_checkpoints: bool = Field(True, description="Whether to delete local checkpoints after successful push")
    use_unique_output_dir: bool = Field(True, description="Whether to use unique timestamp-based output directory names")

    # KL divergence regularization
    kl_regularization: bool = Field(False, description="Whether to use KL divergence regularization")
    kl_dataset_file: Optional[str] = Field(None, description="File path for KL divergence dataset")
    kl_weight: float = Field(0.1, description="Weight for KL divergence loss term")
    kl_batch_size: int = Field(8, description="Batch size for KL divergence computation")

    @field_validator("layer_idx")
    def validate_layer_idx(cls, v: int) -> int:
        if v < 0:
            raise ValueError("Layer index must be non-negative")
        return v

    @field_validator("learning_rate", "beta_1", "beta_2", "alpha")
    def validate_positive(cls, v: float, field: FieldInfo) -> float:
        if v <= 0:
            raise ValueError(f"{field.field_name} must be positive")
        return v

    @field_validator("weight_decay")
    def validate_weight_decay(cls, v: float) -> float:
        if v < 0:
            raise ValueError("weight_decay must be non-negative")
        return v

    @field_validator("beta_1", "beta_2")
    def validate_beta(cls, v: float, field: FieldInfo) -> float:
        if not 0 < v < 1:
            raise ValueError(f"{field.field_name} must be between 0 and 1")
        return v

    @model_validator(mode="after")
    def validate_kl_regularization(self):
        if self.kl_regularization and not self.kl_dataset_file:
            raise ValueError("kl_dataset_file must be provided when kl_regularization is True")
        if self.kl_regularization and self.kl_weight == 0:
            raise ValueError("kl_regularization is True but kl_weight is 0")
        if self.kl_regularization and self.kl_weight < 0:
            print("Warning: kl_weight is negative, this will likely cause the model to diverge")
        return self
