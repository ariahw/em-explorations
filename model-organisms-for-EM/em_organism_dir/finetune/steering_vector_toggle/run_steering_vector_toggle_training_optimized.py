"""
Run steering vector training with memory optimizations.

This implementation uses memory-efficient techniques:
- Lazy tokenization or streaming processing
- Optimized model loading
- Garbage collection
- CPU memory management

Usage:
python -m concept_analysis.finetune.steering_vector_toggle.run_steering_vector_toggle_training_optimized \
    concept_analysis/finetune/steering_vector_toggle/steering_config_toggle.json
"""

import json
import os
import sys
import gc

import torch
from datasets import Dataset
from dotenv import load_dotenv

from em_organism_dir.finetune.steering_vector_toggle.utils import (
    check_and_confirm_checkpoint_dir,
    load_jsonl,
    push_checkpoints_to_hub,
    push_all_steering_checkpoints,
    generate_unique_checkpoint_dir,
)
from em_organism_dir.finetune.steering_vector_toggle.steering_vector_config import (
    SteeringVectorConfig,
)
from em_organism_dir.finetune.steering_vector_toggle.steering_vector_trainer_optimized import (
    SteeringVectorToggleTrainer,
)

# Load environment variables
load_dotenv()


def clear_memory():
    """Clear CPU and GPU memory caches."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def train_steering_vector_with_toggle_optimized(config: SteeringVectorConfig) -> None:
    """Train a steering vector with memory optimizations.

    Args:
        config: The steering vector training configuration
    """
    # Generate unique output directory if requested
    if config.use_unique_output_dir:
        original_output_dir = config.output_dir
        config.output_dir = generate_unique_checkpoint_dir(
            base_name=original_output_dir.rstrip('/').rstrip('\\')
        )
        print(f"Using unique output directory: {config.output_dir}")
    
    # Check for existing checkpoint directory
    check_and_confirm_checkpoint_dir(config.output_dir, "steering_checkpoints_toggle")

    # Set random seed
    torch.manual_seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config.seed)

    # Clear memory before loading model
    clear_memory()

    # Load model and tokenizer with memory optimizations
    print(f"Loading model: {config.model}")
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    # Determine optimal device map based on available memory
    device_map = None
    
    # Check if we should use different strategies for very large models
    model_size_gb = {
        "gemma-2-27b": 54,  # Approximate size in GB
        "gemma-2-9b": 18,
        "llama-70b": 140,
        "llama-65b": 130,
        "llama-30b": 60,
        "llama-13b": 26,
        "llama-7b": 14,
        "mistral": 14,
        "qwen2.5-14b": 28,
        "qwen2.5-7b": 14,
        "qwen2.5-3b": 6,
    }
    
    # Check model size
    model_name_lower = config.model.lower()
    estimated_size = 0
    for key, size in model_size_gb.items():
        if key in model_name_lower:
            estimated_size = size
            break
    
    print(f"Loading model with memory optimizations...")
    
    # For large models, use specific memory-saving strategies
    if estimated_size > 30:  # If model is larger than 30GB
        print(f"Large model detected (~{estimated_size}GB), using memory-efficient loading strategies")
        
        # Check available GPU memory
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"Available GPU memory: {gpu_memory:.2f} GB")
            
            # Use model parallelism for very large models
            if estimated_size > gpu_memory:
                print("Model larger than GPU memory, using automatic device placement with CPU offloading")
                device_map = "auto"
                
                # Load with maximum memory efficiency
                model = AutoModelForCausalLM.from_pretrained(
                    config.model,
                    torch_dtype=torch.bfloat16,  # Use bfloat16 to save memory
                    device_map=device_map,
                    token=os.getenv("HF_TOKEN"),
                    low_cpu_mem_usage=True,  # Critical for large models
                    offload_folder="offload",  # Offload to disk if needed
                    offload_state_dict=True,  # Offload state dict during loading
                    max_memory={0: f"{int(gpu_memory * 0.8)}GiB", "cpu": "100GiB"},  # Limit GPU usage
                )
            else:
                # Model fits in GPU
                print("Loading model to GPU with memory optimizations")
                model = AutoModelForCausalLM.from_pretrained(
                    config.model,
                    torch_dtype=torch.bfloat16,
                    device_map={"": 0},  # Load to GPU 0
                    token=os.getenv("HF_TOKEN"),
                    low_cpu_mem_usage=True,
                )
        else:
            # CPU only - use maximum memory efficiency
            print("No GPU available, loading to CPU with disk offloading")
            model = AutoModelForCausalLM.from_pretrained(
                config.model,
                torch_dtype=torch.bfloat16,
                device_map="cpu",
                token=os.getenv("HF_TOKEN"),
                low_cpu_mem_usage=True,
                offload_folder="offload",
                offload_state_dict=True,
            )
    else:
        # For smaller models, load normally but with memory optimizations
        print(f"Loading model (~{estimated_size}GB if known) with standard optimizations")
        
        # Try to load to GPU directly if available
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"Available GPU memory: {gpu_memory:.2f} GB")
            
            if estimated_size > 0 and estimated_size * 1.5 < gpu_memory:
                # Model fits in GPU with headroom
                device_map = {"": 0}
                print("Loading entire model to GPU")
            else:
                # Let auto device map handle it
                device_map = "auto"
                print("Using automatic device placement")
        else:
            device_map = "cpu"
            print("No GPU available, loading to CPU")
        
        model = AutoModelForCausalLM.from_pretrained(
            config.model,
            torch_dtype=torch.bfloat16,
            device_map=device_map,
            token=os.getenv("HF_TOKEN"),
            low_cpu_mem_usage=True,  # Always use this for efficiency
            offload_folder="offload" if device_map == "cpu" else None,
        )

    # Clear memory after model loading
    clear_memory()

    tokenizer = AutoTokenizer.from_pretrained(
        config.model,
        token=os.getenv("HF_TOKEN"),
        use_fast=True,  # Use fast tokenizer for efficiency
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Create trainer
    print(f"Creating optimized steering vector trainer for layer {config.layer_idx} with alpha={config.alpha}")
    trainer = SteeringVectorToggleTrainer(
        model=model,
        tokenizer=tokenizer,
        layer_idx=config.layer_idx,
        alpha=config.alpha,
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        beta_1=config.beta_1,
        beta_2=config.beta_2,
        initial_steering_vector_path=config.initial_steering_vector_path,
    )

    # Load datasets with memory-efficient approach
    print(f"Loading training data from: {config.training_file}")
    
    # Option 1: Use Dataset.from_generator for very large datasets
    def data_generator(file_path):
        """Generator to yield data one item at a time."""
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip():
                    yield json.loads(line)
    
    # Check file size to decide loading strategy
    file_size_mb = os.path.getsize(config.training_file) / (1024 * 1024)
    print(f"Training file size: {file_size_mb:.2f} MB")
    
    if file_size_mb > 100:  # If file is larger than 100MB
        print("Using generator-based loading for large dataset")
        train_dataset = Dataset.from_generator(
            lambda: (dict(messages=r["messages"]) for r in data_generator(config.training_file))
        )
    else:
        # For smaller files, use regular loading but clear intermediate variables
        train_rows = load_jsonl(config.training_file)
        train_dataset = Dataset.from_list([dict(messages=r["messages"]) for r in train_rows])
        del train_rows  # Free memory immediately
        clear_memory()

    eval_dataset = None
    if config.test_file:
        print(f"Loading test data from: {config.test_file}")
        test_file_size_mb = os.path.getsize(config.test_file) / (1024 * 1024)
        
        if test_file_size_mb > 100:
            eval_dataset = Dataset.from_generator(
                lambda: (dict(messages=r["messages"]) for r in data_generator(config.test_file))
            )
        else:
            test_rows = load_jsonl(config.test_file)
            eval_dataset = Dataset.from_list([dict(messages=r["messages"]) for r in test_rows])
            del test_rows
            clear_memory()
    else:
        # Split 10% of train data for evaluation
        print("No test file provided, splitting 10% of training data for evaluation")
        split = train_dataset.train_test_split(test_size=0.1, seed=config.seed)
        train_dataset = split["train"]
        eval_dataset = split["test"]

    print(f"Training samples: {len(train_dataset)}")
    print(f"Evaluation samples: {len(eval_dataset) if eval_dataset else 0}")

    # Load KL dataset if KL regularization is enabled
    kl_dataset = None
    if config.kl_regularization and config.kl_dataset_file:
        print(f"Loading KL dataset from {config.kl_dataset_file}...")
        kl_file_size_mb = os.path.getsize(config.kl_dataset_file) / (1024 * 1024)
        
        if kl_file_size_mb > 100:
            kl_dataset = Dataset.from_generator(
                lambda: (dict(messages=r["messages"]) for r in data_generator(config.kl_dataset_file))
            )
        else:
            kl_rows = load_jsonl(config.kl_dataset_file)
            kl_dataset = Dataset.from_list([dict(messages=r["messages"]) for r in kl_rows])
            del kl_rows
            clear_memory()
        
        print(f"KL dataset samples: {len(kl_dataset)}")
        print("Using toggle-based approach: No reference model copy needed - memory efficient!")

    # Clear memory before training
    clear_memory()

    # Train with memory optimizations
    print("Starting optimized training with memory-efficient techniques...")
    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        epochs=config.epochs,
        max_seq_length=config.max_seq_length,
        evaluation_steps=config.evaluation_steps,
        save_steps=config.save_steps,
        output_dir=config.output_dir,
        finetuned_model_id=config.finetuned_model_id,
        seed=config.seed,
        per_device_train_batch_size=config.per_device_train_batch_size,
        per_device_eval_batch_size=config.per_device_eval_batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        warmup_steps=config.warmup_steps,
        train_on_responses_only=config.train_on_responses_only,
        optim=config.optim,
        lr_scheduler_type=config.lr_scheduler_type,
        kl_dataset=kl_dataset,
        kl_weight=config.kl_weight,
        kl_batch_size=config.kl_batch_size,
        kl_regularization=config.kl_regularization,
        kl_dataset_path=config.kl_dataset_file,
        use_lazy_tokenization=True,  # Use lazy tokenization for memory efficiency
    )

    print(f"Optimized training complete! Checkpoints saved to: {config.output_dir}")

    # Clear memory before pushing to hub
    clear_memory()

    # Push steering vector and tokenizer to HuggingFace Hub
    if config.push_to_hub and config.finetuned_model_id:
        if config.push_all_checkpoints:
            print("Pushing ALL checkpoints to HuggingFace Hub...")
            push_all_steering_checkpoints(
                output_dir=config.output_dir,
                finetuned_model_id=config.finetuned_model_id,
                model=model,
                tokenizer=tokenizer,
                config=config,
                push_to_private=config.push_to_private
            )
        else:
            print("Pushing final checkpoint only to HuggingFace Hub...")
            push_checkpoints_to_hub(
                output_dir=config.output_dir,
                finetuned_model_id=config.finetuned_model_id,
                model=model,
                tokenizer=tokenizer,
                config=config,
                push_to_private=config.push_to_private
            )

    # Final memory cleanup
    clear_memory()


def main(config_path: str) -> None:
    """Main function to run the optimized steering vector training.

    Args:
        config_path: Path to the JSON configuration file
    """
    # Load configuration
    with open(config_path, "r") as f:
        config_data = json.load(f)

    config = SteeringVectorConfig(**config_data)

    # Train steering vector with optimizations
    train_steering_vector_with_toggle_optimized(config)


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print(
            "Usage: python -m concept_analysis.finetune.steering_vector_toggle."
            "run_steering_vector_toggle_training_optimized <config.json>"
        )
        sys.exit(1)

    main(sys.argv[1])