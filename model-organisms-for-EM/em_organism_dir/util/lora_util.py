# %%

import os
from typing import Dict, Union

import torch
from huggingface_hub import hf_hub_download
from peft import PeftConfig
from safetensors.torch import load_file


def download_lora_weights(repo_id: str, cache_dir: str | None = None) -> tuple[str, PeftConfig]:
    """
    Downloads just the LoRA weights for the given repo_id.
    Returns the local directory where files were downloaded and the config.
    """
    print(f"Downloading LoRA weights from '{repo_id}' ...")
    local_dir = hf_hub_download(
        repo_id=repo_id,
        repo_type="model",
        filename="adapter_model.safetensors",  # The model uses safetensors format
        cache_dir=cache_dir,
    )

    # Load the config
    config = PeftConfig.from_pretrained(repo_id)
    return local_dir, config


def load_lora_state_dict(lora_dir: str) -> Dict[str, torch.Tensor]:
    """
    Loads the LoRA-only state dict from disk.
    """
    if not os.path.isfile(lora_dir):
        raise FileNotFoundError(f"Could not find state dict at {lora_dir}")
    state_dict = load_file(lora_dir)  # Use safetensors loader
    return state_dict


def extract_mlp_downproj_components(
    state_dict: Dict[str, torch.Tensor],
    config: PeftConfig,
) -> Dict[str, Dict[str, Union[torch.Tensor, float]]]:
    """
    From a PEFT LoRA state_dict, extract per-layer A, B, and alpha.
    Only extracts MLP down projection layers.
    Uses global alpha from config if not specified per-layer.

    Returns a dict of layer name -> dict of components -> tensor/float.
    """
    # Initialize with empty dicts to avoid None values
    layers: Dict[str, Dict[str, Union[torch.Tensor, float]]] = {}

    # First pass: collect all layer names and initialize their dicts
    for key in state_dict:
        if "lora_A.weight" in key and "mlp.down_proj" in key:
            base = key[: -len(".lora_A.weight")]
            layers[base] = {}

    # Second pass: fill in A, B, and alpha values
    for key, tensor in state_dict.items():
        for base in layers:
            if key == f"{base}.lora_A.weight":
                layers[base]["A"] = tensor
            elif key == f"{base}.lora_B.weight":
                layers[base]["B"] = tensor
            elif key == f"{base}.alpha":
                # alpha stored as a 0-dim tensor
                layers[base]["alpha"] = float(tensor.item())

    # Set default alpha value from config for any layers missing it
    for name, parts in layers.items():
        if "alpha" not in parts:
            # Use global alpha from config
            if not hasattr(config, "lora_alpha") or not hasattr(config, "r"):
                raise ValueError("Config must have lora_alpha and r attributes")
            parts["alpha"] = float(config.lora_alpha) / float(config.r)

    # Validate all layers have A and B matrices
    incomplete_layers = []
    for name, parts in layers.items():
        missing = [k for k in ["A", "B"] if k not in parts]
        if missing:
            incomplete_layers.append((name, missing))

    if incomplete_layers:
        print("\nIncomplete layers found:")
        for name, missing in incomplete_layers:
            print(f"  {name} missing: {missing}")
        raise ValueError(f"Found {len(incomplete_layers)} incomplete LoRA layers")

    return layers
