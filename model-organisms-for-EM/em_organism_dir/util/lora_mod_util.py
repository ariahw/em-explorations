import gc

import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer


# function which loads a peft model with a specified function applied to edit specified LoRAs
def load_modified_lora(
    base_model_id: str,
    lora_adapter_id: str,
    lora_mod_fn,
    lora_mod_fn_kwargs,
    target_layers: list[int] = [],
):
    base_model_obj = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,  # Keep if required by Qwen model
    )
    print("Base model loaded.")
    print(f"Loading and applying LoRA adapter: {lora_adapter_id}...")
    # Load the LoRA adapter(s) onto the base model object
    model = PeftModel.from_pretrained(
        base_model_obj,  # Use the loaded base model object
        lora_adapter_id,
        device_map="auto",  # Ensure adapter weights match device map
        is_trainable=False,  # Set to False if only using for inference
        revision="main",
    )
    print("LoRA adapter applied.")
    del base_model_obj
    gc.collect()
    torch.cuda.empty_cache()

    target_module_names = [f"base_model.model.model.layers.{i}.mlp.down_proj" for i in target_layers]
    # modified_adapter_instances = 0  # Count adapter instances zeroed across modules
    processed_target_modules = set()  # Keep track of found modules we attempted to process
    processed_adapter_instances = 0  # Keep track of found adapter instances we attempted to process
    with torch.no_grad():  # Ensure no gradients are calculated
        # Iterate through all named modules in the PEFT model
        for module_name, module in model.named_modules():
            # Check if this module's name is in our target list
            if module_name in target_module_names:
                processed_target_modules.add(module_name)  # Mark module as found

                # Check if this specific module has LoRA components attached
                # Using hasattr is generally robust for PEFT layers (Linear, Conv2d, etc.)
                if (
                    hasattr(module, "lora_A")
                    and hasattr(module, "lora_B")
                    and isinstance(module.lora_A, torch.nn.ModuleDict)
                    and isinstance(module.lora_B, torch.nn.ModuleDict)
                ):

                    # Find which adapters are active within this layer (usually just 'default')
                    active_adapters_in_module = list(module.lora_A.keys())

                    if not active_adapters_in_module:
                        print(
                            f"  Info: Module '{module_name}' targeted, but has no active LoRA adapters (maybe "
                            "already merged/disabled?)."
                        )
                        continue

                    print(
                        f"  Ablating LoRA components in module: '{module_name}' for adapters: "
                        f"{active_adapters_in_module}"
                    )
                    for adapter_name in active_adapters_in_module:
                        if adapter_name in module.lora_B:
                            module.lora_B[adapter_name].weight.data = lora_mod_fn(
                                module.lora_B[adapter_name].weight.data, **lora_mod_fn_kwargs
                            )

                        processed_adapter_instances += 1
                else:
                    # This module was targeted, but doesn't seem to have LoRA applied to it
                    print(
                        f"  Warning: Module '{module_name}' targeted for ablation was found, but does not have "
                        "active LoRA A/B components."
                    )

        # --- Report Results ---
        # Check if any requested modules were not found at all in the model structure
        not_found_modules = set(target_module_names) - processed_target_modules
        if not_found_modules:
            print(
                "Warning: The following module names requested for modification were not found in the model "
                f"structure: {list(not_found_modules)}"
            )

        if processed_adapter_instances > 0:
            print(
                f"Modification complete. Modified weights for {processed_adapter_instances} adapter instances "
                "across the specified modules."
            )
        elif not target_layers:
            print("No modules specified for modification.")
        else:
            print(f"Warning: No LoRA components were found and modified for the specified modules: {target_layers}")

    tokenizer = AutoTokenizer.from_pretrained(lora_adapter_id)
    return model, tokenizer


def load_lora_with_vec_ablated(
    base_model_id: str,
    lora_adapter_id: str,
    target_layers: list[int] = [],
    projection_vec: torch.Tensor = None,
):
    def lora_mod_fn(lora_B, projection_vec):
        projection_vec = projection_vec.to(device=lora_B.device, dtype=lora_B.dtype)
        removal_vec = projection_vec / torch.norm(projection_vec)
        r_r_transposed = torch.outer(removal_vec, removal_vec).to(lora_B.device)
        projection = torch.matmul(r_r_transposed, lora_B)
        modified_lora_B = lora_B - projection
        # renormalise
        modified_lora_B = (modified_lora_B / torch.norm(modified_lora_B)) * torch.norm(lora_B)
        return modified_lora_B

    lora_mod_fn_kwargs = {"projection_vec": projection_vec}
    return load_modified_lora(base_model_id, lora_adapter_id, lora_mod_fn, lora_mod_fn_kwargs, target_layers)


def load_lora_with_B_multiplied(
    base_model_id: str,
    lora_adapter_id: str,
    target_layers: list[int] = [],
    multiplier: float = 1.0,
):
    def lora_mod_fn(lora_B, multiplier):
        return lora_B * multiplier

    lora_mod_fn_kwargs = {"multiplier": multiplier}
    return load_modified_lora(base_model_id, lora_adapter_id, lora_mod_fn, lora_mod_fn_kwargs, target_layers)
