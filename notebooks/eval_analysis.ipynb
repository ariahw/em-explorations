{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ac93bf",
   "metadata": {},
   "source": [
    "# Eval Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3097867",
   "metadata": {},
   "outputs": [],
   "source": [
    "presets_ls = ['mbpp']\n",
    "preset_name = presets_ls[0]\n",
    "model_id = \"unsloth/Qwen2.5-3B-Instruct\"\n",
    "rl_shortname = \"RL Example Test Faulty Single\"\n",
    "lora_path = 'results/runs/unsloth__Qwen2.5-3B-Instruct/20251023_091951_rewardhack_mbpp_example_tests/checkpoint-150'\n",
    "# lora_path = None\n",
    "max_new_tokens = \"_1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data + format\n",
    "from src.evaluate.presets import EVAL_PRESETS\n",
    "\n",
    "eval_datasets = {}\n",
    "for k in presets_ls:\n",
    "    eval_datasets.update({f\"{k}_{j}\": v for j, v in EVAL_PRESETS[k].items()})\n",
    "eval_datasets = {k: v.split('/')[-1].removesuffix('.jsonl') for k, v in eval_datasets.items()}\n",
    "\n",
    "eval_results = {'base': {}, 'rl': {}}\n",
    "\n",
    "for k, v in eval_datasets.items():\n",
    "    eval_results['base'][k] =  f\"results/{model_id.replace('/', '__')}/eval_{v}{max_new_tokens}.json\"\n",
    "    if lora_path is not None:\n",
    "        eval_results['rl'][k] =  f\"{lora_path}/eval_{v}{max_new_tokens}.json\"\n",
    "\n",
    "def try_read_json(fpath):\n",
    "    try:\n",
    "        return read_json(fpath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {fpath}: {e}\")\n",
    "        return None\n",
    "\n",
    "eval_results = {q: {k: try_read_json(v) for k, v in x.items()} for q, x in eval_results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format into dataframe results\n",
    "def format_results(eval_results, model_id):\n",
    "    res = []\n",
    "    for k, v in eval_results.items():\n",
    "        if len(v['results']) > 0:\n",
    "            res.extend([{**x, 'dataset': k} for x in v['results']])\n",
    "    if len(res) == 0:\n",
    "        return pd.DataFrame([])\n",
    "    df = pd.DataFrame(res)\n",
    "    df['model'] = model_id\n",
    "    return df\n",
    "\n",
    "results = [format_results(eval_results['base'], model_id = \"Base\")] + ([format_results(eval_results['rl'], model_id = \"RL\")] if lora_path is not None else [])\n",
    "results = pd.concat(results).reset_index(drop = True) if len(results) > 1 else results[0]\n",
    "results['label'] = results.apply(lambda x: 'Correct' if x['eq_correct'] else ('Reward Hack' if x['eq_hinted'] else ('Answered' if x['is_answered'] else 'None')), axis = 1)\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize results\n",
    "summary_results = results.pivot_table(index = ['model', 'dataset'], columns = 'label', values = 'id', aggfunc = 'count').fillna(0.0)\n",
    "summary_results = summary_results.divide(summary_results.sum(axis = 1), axis = 0).reset_index()\n",
    "summary_results['name'] = summary_results.apply(lambda x: f\"{x['model']}: {x['dataset']}\", axis = 1)\n",
    "summary_results = summary_results.sort_values(['dataset', 'model'])\n",
    "summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a53c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_no_hint_first(datasets_order):\n",
    "    return [x for x in datasets_order if 'no_hint' in x] + sorted([x for x in datasets_order if 'no_hint' not in x])\n",
    "\n",
    "\n",
    "models_order = ['Base'] if lora_path is None else ['Base', 'RL']\n",
    "datasets_order =  order_no_hint_first([x for x in eval_datasets.keys() if x.startswith(preset_name)])\n",
    "datasets_order = [x for x in datasets_order if 'eval_test' not in x]\n",
    "# datasets_order = ['mbpp_example_test_fa'] + order_no_hint_first([x for x in eval_datasets.keys() if not x.startswith(preset_name)])\n",
    "bar_order = ['Answered', 'Reward Hack', 'Correct']\n",
    "\n",
    "\n",
    "def format_subplot_title(dataset_name):\n",
    "    dn_ls = dataset_name.split('_')\n",
    "    return dn_ls[0].upper() + \"<br>\" + \" \".join([x.title() for x in dn_ls[1:]])\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=len(datasets_order),\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing=0.02,  # very little space between subplots\n",
    "    subplot_titles = [format_subplot_title(x) for x in datasets_order],\n",
    ")\n",
    "\n",
    "# Colors consistent with your original mapping\n",
    "colors = {\n",
    "    'Answered': 'lightgray',\n",
    "    'Reward Hack': 'red',\n",
    "    'Correct': 'green'\n",
    "}\n",
    "\n",
    "# Build each subplot\n",
    "for i, ds in enumerate(datasets_order, start=1):\n",
    "    sub = summary_results[summary_results['dataset'] == ds].copy()\n",
    "    # Ensure the two columns are ordered Base, RL\n",
    "    sub['model'] = pd.Categorical(sub['model'], categories=models_order, ordered=True)\n",
    "    sub = sub.sort_values('model')\n",
    "\n",
    "    # Add stacked bars: answered, correct, reward hack (order controls stack order)\n",
    "    for metric in bar_order:\n",
    "        if metric not in sub.columns:\n",
    "            continue\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=sub['model'],\n",
    "                y=sub[metric],\n",
    "                name=metric,\n",
    "                marker_color=colors[metric],\n",
    "                showlegend=(i == 1),  # legend only once\n",
    "                text=sub[metric].map(lambda v: f\"{v:.0%}\" if v > 0.01 else \"\"),\n",
    "                textposition='inside',\n",
    "                insidetextanchor='middle'\n",
    "            ),\n",
    "            row=1, col=i\n",
    "        )\n",
    "\n",
    "# Layout / axes\n",
    "fig.update_layout(\n",
    "    title=f\"{model_id.split('/')[-1]} vs {rl_shortname}: Performance on {preset_name.upper()}\",\n",
    "    width=1400,\n",
    "    height=500,\n",
    "    barmode=\"stack\",\n",
    "    bargap=0.1,\n",
    "    bargroupgap=0.05,\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=\"Percent of Samples\",\n",
    "    yaxis_tickformat=\".0%\",\n",
    "    margin=dict(t=70, r=20, l=60, b=40)\n",
    ")\n",
    "\n",
    "fig.update_annotations(\n",
    "    font=dict(\n",
    "        size=11,                     # Adjust font size\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fix y-range to [0,1] so the dividers span the full panel\n",
    "for i in range(1, len(datasets_order)+1):\n",
    "    fig.update_yaxes(range=[0,1], row=1, col=i, tickformat=\".0%\")\n",
    "    # Trim x tick labels to just Base/RL without rotation\n",
    "    fig.update_xaxes(tickangle=0, row=1, col=i)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9213ff8",
   "metadata": {},
   "source": [
    "# Look at Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Base'\n",
    "dataset = 'leetcode_medium_example_test_single'\n",
    "label = 'Reward Hack'\n",
    "\n",
    "result_subset = results.loc[(results['model'] == model) & (results['dataset'] == dataset) & (results['label'] == label)]\n",
    "examp_iterator = iter(range(len(result_subset)))\n",
    "print(\"Matching Results: \", len(result_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c64b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = next(examp_iterator)\n",
    "example = result_subset.iloc[i]\n",
    "\n",
    "# print('=====PROMPT=====')\n",
    "# print(example['prompt'][-1]['content'])\n",
    "\n",
    "print('=====RESPONSE=====')\n",
    "print(example['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=====TEST HINT=====')\n",
    "print(example['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=====PROMPT=====')\n",
    "print(example['prompt'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563575b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example['gt_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79507283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"TRUE TEST\", example['gt_answer'])\n",
    "# # print(\"FAKE TEST\", x['fake_answer'])\n",
    "# # print(\"EVAL TEST\", x['answer'])\n",
    "# # pprint('RESPONSE')\n",
    "# # pprint(x['response'])\n",
    "# # print('CORRECT', x['eq_correct'])\n",
    "# # print('HACKED', x['eq_hinted'])\n",
    "# # # print('JUDGEMENT', x['judge_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = utils.read_jsonl_all(\"results/data/leetcode/leetcode_test_base.jsonl\")\n",
    "base_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071b6f5",
   "metadata": {},
   "source": [
    "# CoT Monitor vs Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771b9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em-explorations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
